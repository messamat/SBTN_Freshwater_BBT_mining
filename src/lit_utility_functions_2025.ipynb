{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b8f253-74c5-4a91-bfc1-b7f0b71633be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import collections\n",
    "from inspect import getsourcefile\n",
    "import itertools\n",
    "import litstudy  # Use pip install git+https://github.com/NLeSC/litstudy to download dev version\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pyzotero import zotero\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652c7330-518c-4905-8e02-1ebeb02583dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and compile references from the WoS search into a single document set (lit_study format)\n",
    "def rpickle_bibdocset(in_dirpath, in_pattern, out_pickle):\n",
    "    if not out_pickle.exists():\n",
    "        # Get list of every bib file\n",
    "        bib_initlist = [p for p in list(in_dirpath.glob('*')) if re.compile(in_pattern).match(str(p))]\n",
    "        # Read bib files from first scoping and join them (takes ~15-20 sec/1000 refs)\n",
    "        reflist = []\n",
    "        for bib in bib_initlist:\n",
    "            reflist += litstudy.load_bibtex(bib)\n",
    "\n",
    "        # Pickle them (save the full document set as a binary file on disk that can be easily retrieved)\n",
    "        with open(out_pickle, 'wb') as f:\n",
    "            pickle.dump(reflist, f)\n",
    "    else:\n",
    "        # Read pre-saved document set\n",
    "        with open(out_pickle, 'rb') as f:\n",
    "            reflist = pickle.load(f)\n",
    "    return reflist\n",
    "\n",
    "# Get titles and DOIs from Zotero test list\n",
    "def get_testlist(library_id, api_key_path):\n",
    "    api_key = api_key_path.read_text().strip()\n",
    "    zot = zotero.Zotero(library_id=library_id, library_type='group', api_key=api_key)\n",
    "    testlist_colID = str([col['key'] for col in zot.collections_top() if col['data']['name'] == 'test list'][0])\n",
    "    testlist_items = zot.everything(zot.collection_items_top(testlist_colID))\n",
    "\n",
    "    testlist_title_dois = collections.defaultdict(list)\n",
    "    for ref in testlist_items:\n",
    "        testlist_title_dois[ref['key']].append(ref['data']['title'])\n",
    "        if 'DOI' in ref['data']:\n",
    "            testlist_title_dois[ref['key']].append(ref['data']['DOI'])\n",
    "        else:\n",
    "            testlist_title_dois[ref['key']].append(np.nan)\n",
    "    return testlist_title_dois\n",
    "\n",
    "# Get all DOIs and titles in references returned from search\n",
    "def tabulate_searchlist(in_reflist, out_csvpath):\n",
    "    if not out_csvpath.exists():\n",
    "        reflist_dict = {}\n",
    "        for i, ref in enumerate(in_reflist):\n",
    "            reflist_dict[i] = [re.sub(r\"[^a-zA-Z\\d\\s]\", \"\", ref.title.replace('\\n', ' ').lower()),\n",
    "                               ref.publication_source, ref.publication_year, ref.abstract]\n",
    "            if 'doi' in ref.entry:\n",
    "                reflist_dict[i].append(ref.entry['doi'])\n",
    "            else:\n",
    "                reflist_dict[i].append(np.nan)\n",
    "\n",
    "        reflist_pd = pd.DataFrame.from_dict(reflist_dict, orient='index')\n",
    "        reflist_pd.columns = ['title', 'source', 'year', 'abstract', 'doi']\n",
    "        reflist_pd.to_csv(out_csvpath)\n",
    "    else:\n",
    "        reflist_pd = pd.read_csv(out_csvpath)\n",
    "    return reflist_pd\n",
    "\n",
    "# Erite string y to file x\n",
    "def write(x, y):\n",
    "    with open(x, 'a') as f:\n",
    "        f.write(y)\n",
    "        f.write('\\n')\n",
    "    return _\n",
    "\n",
    "def combine_2w_regex(pattern1, pattern2, precede=False):\n",
    "    \"\"\"\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    \"\"\"\n",
    "    regexp = f\"{pattern1}\\\\W{pattern2}\\\\b\"\n",
    "    if precede == False:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W{pattern1}\\\\b)\"\n",
    "    return regexp\n",
    "\n",
    "# Count number a times a simple 2-pattern group occurs in text\n",
    "def find_2w_regex(text, pattern1, pattern2, precede=False):\n",
    "    \"\"\"\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    \"\"\"\n",
    "    regexp = f\"{pattern1}\\\\W{pattern2}\\\\b\"\n",
    "    if precede == False:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W{pattern1}\\\\b)\"\n",
    "    wa = re.findall(regexp, text)\n",
    "    wal = len(wa)\n",
    "    return wal\n",
    "\n",
    "# Count number a times a simple pattern occurs in text\n",
    "def find_regex(text, pattern):\n",
    "    return len(re.findall(f\"{word}\", text))\n",
    "\n",
    "# Join all strings in a list with | signs and parentheses\n",
    "def recomb(in_str, recomb_sep):\n",
    "    if isinstance(in_str, list):\n",
    "        return f\"({recomb_sep.join(f'({w})' for w in in_str)})\"\n",
    "    else:\n",
    "        return in_str\n",
    "\n",
    "# Find patterns in text based on search dictionary\n",
    "def combo_refind(in_searchdict, text):\n",
    "    for regexp_combo in in_searchdict.values():\n",
    "        if regexp_combo[0] == 'with':\n",
    "            k = find_2w_regex(text, regexp_combo[1][0], regexp_combo[1][1], precede=False)\n",
    "        elif regexp_combo[0] == 'pre':\n",
    "            k = find_2w_regex(text, regexp_combo[1][0], regexp_combo[1][1], precede=True)\n",
    "        elif regexp_combo[0] is None:\n",
    "            k = find_regex(text, regexp_combo)\n",
    "        else:\n",
    "            break\n",
    "        return k\n",
    "\n",
    "# Generate n-grams from DOI\n",
    "#CHECK OUT: from pattern.en import ngrams\n",
    "#print(ngrams(\"He goes to hospital\", n=2))\n",
    "def DOI_ngram(A):\n",
    "    count0 = collections.Counter()\n",
    "    s1 = A[0].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count0 = count0 + (collections.Counter(every))\n",
    "    count0 = count0.most_common()\n",
    "\n",
    "    count1 = collections.Counter()\n",
    "    s1 = A[1].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count1 = count1 + (collections.Counter(every))\n",
    "    count1 = count1.most_common()\n",
    "\n",
    "    count2 = collections.Counter()\n",
    "    for idx, i in enumerate(A[2]):\n",
    "        x = collections.Counter([l.lower() for l in i])\n",
    "        count2 += x\n",
    "    count2 = count2.most_common()\n",
    "\n",
    "    count3 = count0 + count1 + count2\n",
    "    return count3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08506a3a-2e56-4b2f-b196-42eec3753afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
