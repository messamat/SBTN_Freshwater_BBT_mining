{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef14fbb9-2ed6-4b60-97d0-cde068a855d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File Sizes (MB):\n",
      "mydata_pickle.pkl: 0.0304 MB\n",
      "mydata_pickle_gz.pkl.gz: 0.0044 MB\n",
      "mydata_pickle_bz2.pkl.bz2: 0.0017 MB\n",
      "mydata_json.json: 0.3546 MB\n",
      "mydata_json_gz.json.gz: 0.0053 MB\n",
      "mydata_json_bz2.json.bz2: 0.0022 MB\n",
      "mydata_h5py.h5: 5.8276 MB\n",
      "mydata_parquet.parquet: 0.0025 MB\n",
      "\n",
      "Loading Times (seconds):\n",
      "mydata_pickle.pkl: 0.0195 seconds\n",
      "Warning: No loading function found for mydata_pickle_gz.pkl.gz\n",
      "mydata_pickle_gz.pkl.gz: 0.0000 seconds\n",
      "Warning: No loading function found for mydata_pickle_bz2.pkl.bz2\n",
      "mydata_pickle_bz2.pkl.bz2: 0.0000 seconds\n",
      "mydata_json.json: 0.0259 seconds\n",
      "Warning: No loading function found for mydata_json_gz.json.gz\n",
      "mydata_json_gz.json.gz: 0.0000 seconds\n",
      "Warning: No loading function found for mydata_json_bz2.json.bz2\n",
      "mydata_json_bz2.json.bz2: 0.0000 seconds\n",
      "mydata_h5py.h5: 2.8556 seconds\n",
      "mydata_parquet.parquet: 0.0599 seconds\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "import bz2\n",
    "# import joblib  <-- Removed joblib\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "# --- Create a larger dictionary (100x) ---\n",
    "\n",
    "# Original dictionary (small, for demonstration):\n",
    "original_dict = {\n",
    "    'record1': {'data': [1, 2, 3], 'info': {'a': 'hello', 'b': 42}},\n",
    "    'record2': {'data': [4, 5, 6], 'info': {'a': 'world', 'b': 99}}\n",
    "}\n",
    "\n",
    "# Create a larger dictionary by replicating the original\n",
    "oalex_records_dict_filtered = {}\n",
    "for i in range(1000):\n",
    "    for key, value in original_dict.items():\n",
    "        new_key = f\"{key}_{i}\"  # Create a unique key\n",
    "        oalex_records_dict_filtered[new_key] = value  # Copy the value\n",
    "\n",
    "\n",
    "# --- 1. Pickle ---\n",
    "\n",
    "def save_with_pickle(data, filename, compress=False, compression_method='gzip'):\n",
    "    \"\"\"Saves data using pickle, optionally with compression.\"\"\"\n",
    "    if compress:\n",
    "        if compression_method == 'gzip':\n",
    "            filepath = filename + \".pkl.gz\"\n",
    "            with gzip.open(filepath, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "        elif compression_method == 'bz2':\n",
    "            filepath = filename + \".pkl.bz2\"\n",
    "            with bz2.open(filepath, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid compression_method. Choose 'gzip' or 'bz2'.\")\n",
    "    else:\n",
    "        filepath = filename + \".pkl\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    return filepath\n",
    "\n",
    "def load_with_pickle(filename):\n",
    "    \"\"\"Loads data saved with pickle, handling compression.\"\"\"\n",
    "    if filename.endswith('.gz'):\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif filename.endswith('.bz2'):\n",
    "        with bz2.open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif filename.endswith('.pkl'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file extension.  Expected .pkl, .pkl.gz, or .pkl.bz2\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. JSON ---\n",
    "\n",
    "def save_with_json(data, filename, compress=False, compression_method='gzip'):\n",
    "    \"\"\"Saves data using JSON, optionally with compression.\"\"\"\n",
    "    if compress:\n",
    "        if compression_method == 'gzip':\n",
    "            filepath = filename + \".json.gz\"\n",
    "            with gzip.open(filepath, 'wt', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "        elif compression_method == 'bz2':\n",
    "            filepath = filename + \".json.bz2\"\n",
    "            with bz2.open(filepath, 'wt', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid compression_method. Choose 'gzip' or 'bz2'.\")\n",
    "    else:\n",
    "        filepath = filename + \".json\"\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "    return filepath\n",
    "\n",
    "def load_with_json(filename):\n",
    "    \"\"\"Loads data saved with JSON, handling compression.\"\"\"\n",
    "    if filename.endswith('.gz'):\n",
    "        with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    elif filename.endswith('.bz2'):\n",
    "        with bz2.open(filename, 'rt', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    elif filename.endswith('.json'):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file extension.  Expected .json, .json.gz, or .json.bz2\")\n",
    "\n",
    "\n",
    "# --- 3. HDF5 ---\n",
    "\n",
    "def save_with_h5py(data, filename):\n",
    "    \"\"\"Saves a nested dictionary to an HDF5 file.\"\"\"\n",
    "    filepath = filename + \".h5\"\n",
    "    with h5py.File(filepath, 'w') as hf:\n",
    "        _save_dict_to_h5py(hf, '/', data)\n",
    "    return filepath\n",
    "\n",
    "def _save_dict_to_h5py(h5file, path, dic):\n",
    "    \"\"\"Recursively saves a dictionary to HDF5 groups and datasets.\"\"\"\n",
    "    for key, value in dic.items():\n",
    "        if isinstance(value, dict):\n",
    "            group = h5file.create_group(path + key)\n",
    "            _save_dict_to_h5py(group, path + key + '/', value)\n",
    "        else:\n",
    "            if isinstance(value, (list, tuple)):\n",
    "                value = np.array(value)\n",
    "            h5file[path + key] = value\n",
    "\n",
    "def load_with_h5py(filename):\n",
    "    \"\"\"Loads a nested dictionary from an HDF5 file.\"\"\"\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        return _load_dict_from_h5py(hf, '/')\n",
    "\n",
    "def _load_dict_from_h5py(h5file, path):\n",
    "    \"\"\"Recursively loads a dictionary from HDF5 groups and datasets.\"\"\"\n",
    "    output = {}\n",
    "    for key in h5file[path].keys():\n",
    "        item = h5file[path + key]\n",
    "        if isinstance(item, h5py.Group):\n",
    "            output[key] = _load_dict_from_h5py(h5file, path + key + '/')\n",
    "        else:\n",
    "            output[key] = item[()]\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Parquet (with PyArrow) ---\n",
    "\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    \"\"\"Flattens a nested dictionary into a single-level dictionary.\"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "             items.append((new_key, str(v))) # Convert lists to strings\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def save_with_parquet(data, filename):\n",
    "    \"\"\"Saves a dictionary to a Parquet file using Pandas and PyArrow.\"\"\"\n",
    "    filepath = filename + \".parquet\"\n",
    "\n",
    "    # Flatten the dictionary\n",
    "    flat_data = [flatten_dict(record) for record in data.values()]\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(flat_data)\n",
    "\n",
    "    # Convert to PyArrow Table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Write to Parquet file\n",
    "    pq.write_table(table, filepath)\n",
    "    return filepath\n",
    "\n",
    "def load_with_parquet(filename):\n",
    "    \"\"\"Loads data from a Parquet file into a dictionary.\"\"\"\n",
    "\n",
    "    # Read Parquet file into PyArrow Table\n",
    "    table = pq.read_table(filename)\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    # Convert DataFrame back to a list of dictionaries (undo flattening)\n",
    "    loaded_data = df.to_dict('records')\n",
    "    \n",
    "    #reconstruct nested dictionary\n",
    "    reconstructed_data = {}\n",
    "    for i, flat_dict in enumerate(loaded_data):\n",
    "      nested_dict = {}\n",
    "      for key, value in flat_dict.items():\n",
    "          parts = key.split('_')\n",
    "          current_level = nested_dict\n",
    "          for part in parts[:-1]:\n",
    "              if part not in current_level:\n",
    "                  current_level[part] = {}\n",
    "              current_level = current_level[part]\n",
    "          \n",
    "          #try to convert back the lists\n",
    "          if isinstance(value,str) and value.startswith(\"[\") and value.endswith(\"]\"):\n",
    "            try:\n",
    "                value = eval(value) #Use of eval for simplicity. In production, ast.literal_eval would be safer\n",
    "            except (SyntaxError, ValueError):\n",
    "                pass\n",
    "          current_level[parts[-1]] = value\n",
    "      reconstructed_data[f\"record{i+1}\"] = nested_dict #use ordered keys as in original dict\n",
    "\n",
    "\n",
    "    return reconstructed_data\n",
    "\n",
    "# --- Usage and Comparison ---\n",
    "filenames = []\n",
    "#pickle\n",
    "filepath_pickle = save_with_pickle(oalex_records_dict_filtered, \"mydata_pickle\")\n",
    "filenames.append(filepath_pickle)\n",
    "filepath_pickle_gz = save_with_pickle(oalex_records_dict_filtered, \"mydata_pickle_gz\", compress=True)\n",
    "filenames.append(filepath_pickle_gz)\n",
    "filepath_pickle_bz2 = save_with_pickle(oalex_records_dict_filtered, \"mydata_pickle_bz2\", compress=True, compression_method='bz2')\n",
    "filenames.append(filepath_pickle_bz2)\n",
    "\n",
    "#json\n",
    "filepath_json = save_with_json(oalex_records_dict_filtered, \"mydata_json\")\n",
    "filenames.append(filepath_json)\n",
    "filepath_json_gz = save_with_json(oalex_records_dict_filtered, \"mydata_json_gz\", compress=True)\n",
    "filenames.append(filepath_json_gz)\n",
    "filepath_json_bz2 = save_with_json(oalex_records_dict_filtered, \"mydata_json_bz2\", compress=True, compression_method='bz2')\n",
    "filenames.append(filepath_json_bz2)\n",
    "\n",
    "#h5py\n",
    "filepath_h5py = save_with_h5py(oalex_records_dict_filtered, \"mydata_h5py\")\n",
    "filenames.append(filepath_h5py)\n",
    "\n",
    "#parquet\n",
    "filepath_parquet = save_with_parquet(oalex_records_dict_filtered, \"mydata_parquet\")\n",
    "filenames.append(filepath_parquet)\n",
    "\n",
    "\n",
    "print(\"\\nFile Sizes (MB):\")\n",
    "for filename in filenames:\n",
    "    size = os.path.getsize(filename) / (1024 * 1024)  # Convert bytes to MB\n",
    "    print(f\"{filename}: {size:.4f} MB\")\n",
    "\n",
    "\n",
    "print(\"\\nLoading Times (seconds):\")\n",
    "load_functions = {  # Use a dictionary to map extensions to loading functions\n",
    "    \".pkl\": load_with_pickle,\n",
    "    \".pkl.gz\": load_with_pickle,\n",
    "    \".pkl.bz2\": load_with_pickle,\n",
    "    \".json\": load_with_json,\n",
    "    \".json.gz\": load_with_json,\n",
    "    \".json.bz2\": load_with_json,\n",
    "    \".h5\": load_with_h5py,\n",
    "    \".parquet\": load_with_parquet\n",
    "}\n",
    "\n",
    "for filename in filenames:\n",
    "    start_time = time.time()\n",
    "    extension = os.path.splitext(filename)[-1] #get the last extension\n",
    "    if extension in load_functions:\n",
    "        loaded_data = load_functions[extension](filename) # Call correct load function\n",
    "        # Basic check to make sure the loading process went ok\n",
    "        if extension == \".parquet\": #Parquet dict has slightly different keys\n",
    "            assert len(loaded_data) == len(oalex_records_dict_filtered)\n",
    "        else:\n",
    "            assert len(loaded_data) == len(oalex_records_dict_filtered) # Check length\n",
    "    else:\n",
    "        print(f\"Warning: No loading function found for {filename}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"{filename}: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f01369-1c47-41fc-8501-e19adaf6f249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
