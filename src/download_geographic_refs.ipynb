{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "407b6564-d71c-4904-b709-f04ccb6f78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run set_up.py \n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import py7zr\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urljoin\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e4d53253-cb10-4436-9e57-6844f30d19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(in_url, out_dir, out_zip_name=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Download and extract zip file\n",
    "    \n",
    "    Parameters:\n",
    "    url (str): URL to download the zip file\n",
    "    out_dir (str): Directory to save and extract the files\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the extracted GDB directory\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(out_dir):\n",
    "        if verbose:\n",
    "            print(f'Creating {out_dir}')\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # Get filename from URL if not provided\n",
    "    if out_zip_name is None:\n",
    "        if verbose:\n",
    "            print('Did not provide output zip file name, extracting from URL')\n",
    "        out_zip_name = os.path.basename(urlparse(in_url).path)\n",
    "        \n",
    "        # Check if file has zip or 7z extension\n",
    "        if not re.match(r'.*\\.(zip|7z)$', out_zip_name):\n",
    "            raise ValueError(\"Could not extract file name with zip or 7z extension from URL\")\n",
    "\n",
    "    out_zip_path = os.path.join(out_dir, out_zip_name)\n",
    "    \n",
    "    # Download the file\n",
    "    if not os.path.exists(out_zip_path):\n",
    "        if verbose:\n",
    "            print(f\"Downloading from {in_url}...\")\n",
    "        response = requests.get(in_url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        \n",
    "        # Save the zip file\n",
    "        with open(out_zip_path, 'wb') as f:\n",
    "            shutil.copyfileobj(response.raw, f)\n",
    "\n",
    "        # Extract the zip file\n",
    "        print(\"Extracting zip file...\")\n",
    "        with ZipFile(out_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(out_dir)\n",
    "            # Get the name of the first directory in the zip file\n",
    "            first_file = zip_ref.namelist()[0]\n",
    "            unzipped_dir = os.path.dirname(first_file)\n",
    "         \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f'{out_zip_path} already exists. skipping....')\n",
    "\n",
    "        with ZipFile(out_zip_path, 'r') as zip_ref:\n",
    "            # Get the name of the first directory in the zip file\n",
    "            first_file = zip_ref.namelist()[0]\n",
    "            unzipped_dir = os.path.dirname(first_file)\n",
    "            \n",
    "    # Return the path to the extracted GDB directory\n",
    "    return(os.path.join(out_dir, unzipped_dir))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cc17d42b-5d1f-40b1-891a-addc983554c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Water Body Dataset\n",
    "wbd_url = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/National/GDB/WBD_National_GDB.zip\"\n",
    "nhd_dir = os.path.join(datdir, \"nhd\")  # Adjust this path as needed\n",
    "\n",
    "try:\n",
    "    extracted_path = download_and_extract_zip(\n",
    "        in_url=wbd_url,\n",
    "        out_dir=nhd_dir,\n",
    "        verbose=False)\n",
    "    #print(f\"Files extracted to: {extracted_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7957366-6aca-4283-bc18-e51ba5ce081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download NHD\n",
    "def download_nhdplus_hr_hu4(hu4, out_dir, verbose=True):\n",
    "    if (not isinstance(hu4, str)) or (len(hu4) != 4) :\n",
    "        raise TypeError(\"hu4 argument must be a 4-digit string\")\n",
    "        \n",
    "    root_url = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/\"\n",
    "    \n",
    "    zip_name = f\"NHDPLUS_H_{hu4}_HU4_GDB.zip\"\n",
    "    full_url = urljoin(root_url, zip_name)\n",
    "\n",
    "    out_path = download_and_extract_zip(\n",
    "        in_url=full_url, \n",
    "        out_dir=out_dir, \n",
    "        out_zip_name=zip_name,\n",
    "        verbose=verbose)\n",
    "\n",
    "    return(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d9b59b3-8c00-43b9-b4cd-d0493176e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download AWS S3 bucket\n",
    "#- For multiple patterns: `\"Contents[?contains(Key, 'vpu-boundaries.gpkg') || contains(Key, 'other-pattern')]\"`\n",
    "#- For exact suffix match: `\"Contents[?ends_with(Key, 'vpu-boundaries.gpkg')]\"`\n",
    "\n",
    "def download_s3_bucket_contents(in_bucket_name, in_prefix, out_dir, key=None,\n",
    "                               verbose=True):\n",
    "    # Create the S3 client with the unsigned configuration\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        region_name='us-west-2',\n",
    "        config=Config(signature_version=UNSIGNED) # Create an unsigned (\"anonymous\") configuration\n",
    "    )\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(out_dir):\n",
    "        print(f'Creating {out_dir}')\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    try:\n",
    "        # List all objects in the bucket\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(Bucket=in_bucket_name, \n",
    "                                           Prefix=in_prefix)\n",
    "\n",
    "        if key:\n",
    "            full_key = os.path.join(in_prefix, key)\n",
    "            fname = os.path.basename(key) #\".\".join(key.split(\".\")[:2]))\n",
    "            outf_path = os.path.join(out_dir, fname)\n",
    "            if not os.path.exists(outf_path):\n",
    "                if verbose: \n",
    "                    print(f\"Downloading: {key}\")\n",
    "                s3.download_file(in_bucket_name, full_key, \n",
    "                                Filename=outf_path)\n",
    "                if verbose: \n",
    "                    print(\"Download completed successfully\")\n",
    "            else:\n",
    "                if verbose: \n",
    "                    print(f'{outf_path} already exists. Skipping...')\n",
    "        else:\n",
    "            # Download each object\n",
    "            for page in page_iterator:\n",
    "                if 'Contents' in page:\n",
    "                    for obj in page['Contents']:\n",
    "                        # Get the object key\n",
    "                        key = obj['Key']\n",
    "                \n",
    "                        # Download the file\n",
    "                        fname = os.path.basename(key) #\".\".join(key.split(\".\")[:2]))\n",
    "                        outf_path = os.path.join(out_dir, fname)\n",
    "                        if not os.path.exists(outf_path):\n",
    "                            if verbose: \n",
    "                                print(f\"Downloading: {key}\")\n",
    "                            s3.download_file(in_bucket_name, key, \n",
    "                                             Filename=outf_path)\n",
    "                            if verbose: \n",
    "                                print(\"Download completed successfully\")\n",
    "                        else:\n",
    "                            if verbose: \n",
    "                                print(f'{outf_path} already exists. Skipping...')\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dcc54d9a-1d4d-4d7d-ab75-4e730c484449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download geoglows data\n",
    "\n",
    "geoglows_dir = os.path.join(datdir, 'geoglows')\n",
    "#Download VPU to boundaries to know which streams/catchments to use\n",
    "#http://geoglows-v2.s3-us-west-2.amazonaws.com/streams-global/vpu-boundaries.gpkg\n",
    "download_s3_bucket_contents(in_bucket_name=\"geoglows-v2\",\n",
    "                            in_prefix='streams-global/', \n",
    "                            out_dir=geoglows_dir,\n",
    "                            key='vpu-boundaries.gpkg',\n",
    "                            verbose=False)\n",
    "\n",
    "#Download streams\n",
    "#\"http://geoglows-v2.s3-website-us-west-2.amazonaws.com/#streams/\"\n",
    "download_s3_bucket_contents(in_bucket_name=\"geoglows-v2\",\n",
    "                            in_prefix='streams/', \n",
    "                            out_dir=os.path.join(geoglows_dir, 'streams'),\n",
    "                            verbose=False)\n",
    "#Download catchments\n",
    "download_s3_bucket_contents(in_bucket_name=\"geoglows-v2\",\n",
    "                            in_prefix='catchments/', \n",
    "                            out_dir=os.path.join(geoglows_dir, 'catchments'),\n",
    "                            verbose=False)\n",
    "\n",
    "#Download tables\n",
    "download_s3_bucket_contents(in_bucket_name=\"geoglows-v2\",\n",
    "                            in_prefix='tables/', \n",
    "                            out_dir=os.path.join(geoglows_dir, 'tables'),\n",
    "                            verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b000aee-d8d8-468d-ad3e-af03fd04b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download GADM\n",
    "#https://gadm.org/download_world.html\n",
    "gadm_url = \"https://geodata.ucdavis.edu/gadm/gadm4.1/gadm_410-gpkg.zip\"\n",
    "gadm_dir = os.path.join(datdir, \"gadm\")  # Adjust this path as needed\n",
    "\n",
    "try:\n",
    "    gadm_path = download_and_extract_zip(in_url=gadm_url, \n",
    "                                        out_dir=gadm_dir,\n",
    "                                        verbose=False)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0f0d1f85-aa20-4955-9f31-c6f59b4c55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download HydroATLAS\n",
    "#https://www.hydrosheds.org/hydroatlas\n",
    "basinatlas_url = \"https://figshare.com/ndownloader/files/20082137\"\n",
    "riveratlas_url = \"https://figshare.com/ndownloader/files/20087321\"\n",
    "hydroatlas_dir = os.path.join(datdir, 'hydroatlas')\n",
    "basinatlas_zip_path = os.path.join(hydroatlas_dir, 'BasinATLAS_Data_v10.gdb.zip')\n",
    "riveratlas_zip_path = os.path.join(hydroatlas_dir, 'RiverATLAS_Data_v10.gdb.zip')\n",
    "\n",
    "basinatlas_path = download_and_extract_zip(\n",
    "    in_url=basinatlas_url,\n",
    "    out_dir=hydroatlas_dir,\n",
    "    out_zip_name=basinatlas_zip_path,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "riveratlas_path = download_and_extract_zip(\n",
    "    in_url=riveratlas_url,\n",
    "    out_dir=hydroatlas_dir,\n",
    "    out_zip_name=riveratlas_zip_path,\n",
    "    verbose=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
