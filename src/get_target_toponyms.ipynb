{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0545b1-2abe-4e10-ad2b-987cc7c6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_geographic_refs.ipynb\n",
    "%run set_up.py \n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd #require pip install pyarrow\n",
    "from shapely.geometry import Point\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad174fb-a2a9-4e12-99b2-6a3ea69c45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pts_path = os.path.join(datdir, 'test_gages', 'test_gages.shp')\n",
    "\n",
    "wbd_path = os.path.join(nhd_dir, 'WBD_National_GDB.gdb')\n",
    "hu12_parquet = os.path.join(nhd_dir, 'wbd_hu12list.parquet')\n",
    "basinatlas_path = os.path.join(hydroatlas_dir,  'BasinATLAS_v10.gdb')\n",
    "basinatlas11_parquet = os.path.join(hydroatlas_dir, 'basinatlas_lev11_idlist.parquet')\n",
    "#geoglows_vpu_path = os.path.join(geoglows_dir, 'vpu-boundaries.gpkg')\n",
    "gadm_path = os.path.join(gadm_dir, 'gadm_410-levels.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c216daa-b88a-49f9-ba50-677d7669e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_basin_idlist(in_id_list,\n",
    "                         in_refids_parquet,\n",
    "                         refids_col,\n",
    "                         out_id_range):\n",
    "    \"\"\"\n",
    "    Expands a list of basin IDs by extracting IDs at different levels from a reference Parquet file.\n",
    "\n",
    "    Args:\n",
    "        in_id_list (list): List of input basin IDs.  Can be integers or strings.\n",
    "        in_refids_parquet (str): Path to the reference Parquet file.\n",
    "        refids_col (str): Name of the column in the Parquet file containing the full basin IDs.\n",
    "        out_id_range (list): List of integer levels to extract.  e.g., [6, 9, 12]\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the expanded basin IDs.  The returned DataFrame\n",
    "                          will have columns named based on `refids_col` and levels in `out_id_range`.\n",
    "                          The data type of the output columns will match the input type of `in_id_list`.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the length of input IDs is inconsistent with the length in reference table.\n",
    "        FileNotFoundError: If the input Parquet file does not exist.\n",
    "        ValueError:  If `out_id_range` contains values greater than the maximum ID length.\n",
    "                   Or if an empty DataFrame is returned by filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    id_all_pd = pd.read_parquet(in_refids_parquet)\n",
    "\n",
    "    # Determine input type and maximum ID length\n",
    "    in_id_type = type(in_id_list[0])\n",
    "    if in_id_type == str:\n",
    "      in_id_len = len(in_id_list[0])\n",
    "    elif in_id_type == int:\n",
    "      in_id_len = len(str(in_id_list[0]))\n",
    "    else:\n",
    "      raise TypeError(\"in_id_list must be a list of strings or integers\")\n",
    "\n",
    "    # Determine the reference ID type and length\n",
    "    first_ref_id = id_all_pd[refids_col].iloc[0] #Get first item in col\n",
    "    refid_type = type(first_ref_id)\n",
    "\n",
    "    if refid_type == str:\n",
    "      refid_len = len(first_ref_id)\n",
    "    elif pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "      refid_len = len(str(first_ref_id))\n",
    "    else:\n",
    "      raise TypeError(f\"{refids_col} must contain strings or integers\")\n",
    "\n",
    "\n",
    "    if in_id_len > refid_len:\n",
    "        raise ValueError(f\"Input ID length ({in_id_len}) is greater than reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Input Validation on out_id_range ---\n",
    "    if any(level > refid_len for level in out_id_range):\n",
    "        raise ValueError(f\"out_id_range values cannot exceed reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Filtering ---\n",
    "    # Convert to string for consistent prefix matching, then convert back later\n",
    "    id_all_pd[refids_col] = id_all_pd[refids_col].astype(str)\n",
    "    in_id_list_str = [str(x) for x in in_id_list]\n",
    "\n",
    "    id_pd = id_all_pd[id_all_pd[refids_col].str.startswith(tuple(in_id_list_str))].copy()\n",
    "\n",
    "    if id_pd.empty:\n",
    "        raise ValueError(\"No matching IDs found. Check in_id_list and refids_col.\")\n",
    "\n",
    "    # --- Column Expansion ---\n",
    "    colroot = re.sub(r'[0-9]+', '', refids_col)\n",
    "    for id_level in out_id_range:\n",
    "        col_name = f'{colroot}{id_level}'\n",
    "        if col_name not in id_pd.columns:\n",
    "            id_pd.loc[:, col_name] = id_pd[refids_col].str[:id_level]\n",
    "            # Convert back to original type, if necessary\n",
    "            if in_id_type == int:\n",
    "                id_pd.loc[:, col_name] = pd.to_numeric(id_pd[col_name],\n",
    "                                                       errors='raise') \n",
    "\n",
    "    # Convert refids_col back to original type\n",
    "    if pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "        id_pd[refids_col] = pd.to_numeric(id_pd[refids_col], \n",
    "                                          errors='raise')\n",
    "\n",
    "    return id_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b629b3-15d0-4616-88d3-dcdd5dd59e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_gdf_tojoin(in_xytab=None, lon_col=None, lat_col=None, \n",
    "                       in_vector=None, hull=True):\n",
    "    #Read xy table\n",
    "    if in_xytab:\n",
    "        points_df = pd.read_table(point_locations_path)\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            points_df,\n",
    "            geometry=[Point(xy) for xy \n",
    "                      in zip(points_df[lon_col], points_df[lat_col])]\n",
    "        )\n",
    "    \n",
    "    #Read vector layer\n",
    "    if in_vector:\n",
    "        gdf_to_join = gpd.read_file(in_vector)\n",
    "\n",
    "    if hull:\n",
    "        # Create convex hull using union_all() (current recommended method)\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            geometry=[gdf_to_join.geometry.union_all().convex_hull], \n",
    "            crs=gdf_to_join.crs\n",
    "        )\n",
    "    return(gdf_to_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f92e4a12-8af2-4549-9500-e7d3131fdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of PFAF_ID for basins level 11\n",
    "def create_basinatlas11_list(basinatlas_path, \n",
    "                             out_basinatlas11_parquet, \n",
    "                             verbose=True):\n",
    "    if not os.path.exists(out_basinatlas11_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of PFAF ID level 11 and saving it to \\\n",
    "            {out_basinatlas11_parquet}')\n",
    "            \n",
    "        basinatlas11_list = gpd.read_file(\n",
    "            filename=basinatlas_path, \n",
    "            layer='BasinATLAS_v10_lev11', \n",
    "            columns=['PFAF_ID'],\n",
    "            rows=1031785,\n",
    "            ignore_geometry=True).\\\n",
    "        astype(pd.Int64Dtype()).\\\n",
    "        rename(columns={\"PFAF_ID\": \"PFAF_ID11\"})\n",
    "        \n",
    "        basinatlas11_list.to_parquet(out_basinatlas11_parquet)\n",
    "    else:\n",
    "        basinatlas11_list = pd.read_parquet(out_basinatlas11_parquet)\n",
    "    return(basinatlas11_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752ba54-3c3c-484a-bdb1-bc440a3096ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_hydrobasin(in_basinatlas_path,\n",
    "                            in_xytab=None, lon_col=None, lat_col=None,\n",
    "                            in_vector=None, \n",
    "                            in_id_list=None, \n",
    "                            in_refids_parquet=None,\n",
    "                            hull=True, sjoin_predicate='intersects'):\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        bas_lev6 = gpd.read_file(filename=in_basinatlas_path, \n",
    "                                 layer='BasinATLAS_v10_lev06',\n",
    "                                 columns=['PFAF_ID']\n",
    "                                ).rename(columns={\"PFAF_ID\": \"PFAF_ID6\"})\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_bas = gpd.sjoin(gdf_to_join.to_crs(crs=bas_lev6.crs), \n",
    "                               bas_lev6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        in_id_list = matched_bas.PFAF_ID6.tolist()\n",
    "\n",
    "    pfaf_pd = _expand_basin_idlist(\n",
    "        in_id_list, \n",
    "        in_refids_parquet, \n",
    "        refids_col='PFAF_ID11', \n",
    "        out_id_range=range(3, 12))\n",
    "\n",
    "    return(pfaf_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e034ba-e56e-413a-8cab-eeb9c3a89e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hydroatlas_data():\n",
    "    print('Getting HydroATLAS data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9edc0a-cc1e-4f28-9aa3-1be09ea46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of NHD HUC12\n",
    "def create_huc12_list(wbd_path, \n",
    "                      out_hu12_parquet,\n",
    "                      verbose=True):\n",
    "    if not os.path.exists(out_hu12_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of HUC 12 and saving it to \\\n",
    "            {out_hu12_parquet}')\n",
    "        wbdhu12_list = gpd.read_file(filename=wbd_path, \n",
    "                                     layer='WBDHU12', \n",
    "                                     rows=105000,\n",
    "                                     columns=['huc12'],\n",
    "                                     ignore_geometry=True)\n",
    "        wbdhu12_list.to_parquet(out_hu12_parquet)\n",
    "    else:\n",
    "        wbdhu12_list = pd.read_parquet(out_hu12_parquet)\n",
    "    return(wbdhu12_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceed7e1-dd52-4480-9e1d-47b0799229f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_NHD_HU(in_wbd_path,\n",
    "                        in_xytab=None, lon_col=None, lat_col=None,\n",
    "                        in_vector=None, \n",
    "                        in_id_list=None, \n",
    "                        in_refids_parquet=None,\n",
    "                        hull=True, sjoin_predicate='intersects'):\n",
    "\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Get NHD WBD\n",
    "        wbdhu6 = gpd.read_file(filename=in_wbd_path, \n",
    "                               layer='WBDHU6',\n",
    "                               columns=['huc6']\n",
    "                              )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_nhd = gpd.sjoin(gdf_to_join.to_crs(crs=wbdhu6.crs), \n",
    "                               wbdhu6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        in_id_list = matched_nhd.huc6.values.tolist()\n",
    "        \n",
    "    huc_pd = _expand_basin_idlist(\n",
    "        in_id_list, \n",
    "        in_refids_parquet, \n",
    "        refids_col='huc12', \n",
    "        out_id_range=range(2, 14, 2))\n",
    "\n",
    "    return(huc_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffc30c-426d-41df-bcd4-2fb3a9ffb4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_nhd_hydronyms(in_hucs, in_wbd_path,  out_dir,\n",
    "                      huc_range=range(2, 14, 2),\n",
    "                      flatten=True,\n",
    "                      verbose=True\n",
    "                    ):\n",
    "    print('Getting NHD basin names')\n",
    "\n",
    "    #If panda dataframe\n",
    "    #Get basin names------------------------------------------------------------\n",
    "    for coln in in_hucs.columns:\n",
    "        huc_len = re.sub(r'[a-zA-Z]+', '', coln)\n",
    "        if int(huc_len) is None:\n",
    "            raise ValueError(f\"HUC level cannot be extracted from {coln}\")\n",
    "        if int(huc_len) in huc_range:\n",
    "            wbd = gpd.read_file(filename=in_wbd_path, \n",
    "                                layer=f'WBDHU{huc_len}',\n",
    "                                columns=[coln, 'name'],\n",
    "                                ignore_geometry=True\n",
    "                               )\n",
    "            in_hucs = in_hucs.merge(wbd, on=coln, how='left').\\\n",
    "            rename(columns={\"name\": f\"{coln}_name\"})\n",
    "\n",
    "    #Download data by HU4 if needed\n",
    "    huc4_list = in_hucs.huc4.unique()\n",
    "    nhd_huc4_pathdict = {}\n",
    "    for huc in huc4_list:\n",
    "        download_nhdplus_hr_hu4(\n",
    "            hu4=huc, \n",
    "            out_dir=out_dir, \n",
    "            verbose=False\n",
    "        )\n",
    "        nhd_huc4_pathdict[huc] = os.path.join(\n",
    "            out_dir,\n",
    "            f'NHDPLUS_H_{huc}_HU4_GDB.gdb')\n",
    "    #print(nhd_huc4_pathdict)\n",
    "    \n",
    "    #Get river names------------------------------------------------------------\n",
    "    #NHD flow line types: FCode attribute to subset\n",
    "    # 46000: Stream/River\n",
    "    # 46003: Stream/River: Hydrographic Category = Intermittent\n",
    "    # 46006: Stream/River: Hydrographic Category = Perennial\n",
    "    # 46007: Stream/River: Hydrographic Category = Ephemeral\n",
    "    # 55800: Artificial path'''\n",
    "    fcode_sel_list = [46000, 46006, 46003, 46007, 55800]\n",
    "    huc4_rivnames_dict = {}\n",
    "\n",
    "    for huc4 in nhd_huc4_pathdict:\n",
    "        if verbose:\n",
    "            print(huc4)\n",
    "        #Read flowlines\n",
    "        #NHD uses geometries with measured (M) values but not supported by pyogrio\n",
    "        #underlying geopandas by default (faster than fiona)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pyogrio.raw\")\n",
    "            \n",
    "            flowlines_gpd = gpd.read_file(\n",
    "                filename=nhd_huc4_pathdict[huc4], \n",
    "                layer='NHDFlowline',\n",
    "                columns=['NHDPlusID', 'ReachCode', 'GNIS_Name', 'FCode'],\n",
    "                ignore_geometry=True\n",
    "            )\n",
    "            \n",
    "        vaa_pd = gpd.read_file(\n",
    "            filename=nhd_huc4_pathdict[huc4], \n",
    "            layer='NHDPlusFlowlineVAA',\n",
    "            columns=['NHDPlusID', 'StreamOrde'],\n",
    "            ignore_geometry=True\n",
    "        )\n",
    "\n",
    "\n",
    "        #reachcode: The first eight digits are the WBD_HUC8.\n",
    "        #The next six digits are randomly assigned, \n",
    "        #sequential numbers that are unique within a HUC8.\n",
    "        flowlines_gpd['huc8'] = flowlines_gpd['ReachCode'].str[:8] \n",
    "        huc8_sel = in_hucs[in_hucs['huc4']==huc4]['huc8'].unique()\n",
    "        flowlines_sub = flowlines_gpd[flowlines_gpd['huc8'].isin(huc8_sel)].\\\n",
    "        merge(vaa_pd, how='inner', on='NHDPlusID')\n",
    "    \n",
    "        rivnames = flowlines_sub[(\n",
    "            (flowlines_sub['FCode'].isin(fcode_sel_list)) \n",
    "            & (flowlines_sub['StreamOrde'] >=6)\n",
    "            & (flowlines_sub['GNIS_Name'].notna())\n",
    "        )].GNIS_Name.unique()\n",
    "\n",
    "        huc4_rivnames_dict[huc4] = rivnames\n",
    "\n",
    "\n",
    "\n",
    "    if flatten:\n",
    "        #Create a set with all unique hydronyms from basins and rivers from NHD\n",
    "        test_nhd_hydronyms_set = set([\n",
    "            *pd.melt(in_hucs, \n",
    "                     value_vars=[f'huc{lev}_name' for lev in huc_range]\n",
    "                    ).value.unique(),\n",
    "            *set({x for v in huc4_rivnames_dict.values() for x in v})\n",
    "        ])\n",
    "        return(test_nhd_hydronyms_set)\n",
    "    else:\n",
    "        #Return dictionary with basin names and river names\n",
    "        out_dict = {}\n",
    "        out_dict['basins_all_pd'] = in_hucs\n",
    "        out_dict['rivers_huc4_dict'] = huc4_rivnames_dict\n",
    "        return(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bbd1c-0951-4875-9c42-1f09ac104c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nhd_data(in_hucs_pd, out_dir, verbose=True):\n",
    "    print('Getting NHD data for all HU4')\n",
    "    \n",
    "    #Download data by HU4 if needed\n",
    "    huc4_list = in_hucs_pd.huc4.unique()\n",
    "    nhd_huc4_pathdict = {}\n",
    "    for huc in huc4_list:\n",
    "        download_nhdplus_hr_hu4(\n",
    "            hu4=huc, \n",
    "            out_dir=out_dir, \n",
    "            verbose=False\n",
    "        )\n",
    "        nhd_huc4_pathdict[huc] = os.path.join(\n",
    "            out_dir,\n",
    "            f'NHDPLUS_H_{huc}_HU4_GDB.gdb')\n",
    "\n",
    "    nhdplus_huc4_dict = {}\n",
    "    for huc4 in nhd_huc4_pathdict:\n",
    "        if verbose:\n",
    "            print(huc4)\n",
    "            \n",
    "        #Read flowlines\n",
    "        #NHD uses geometries with measured (M) values but not supported by pyogrio\n",
    "        #underlying geopandas by default (faster than fiona)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pyogrio.raw\")\n",
    "            \n",
    "            flowlines_gpd = gpd.read_file(\n",
    "                filename=nhd_huc4_pathdict[huc4], \n",
    "                layer='NHDFlowline',\n",
    "                ignore_geometry=False\n",
    "            )\n",
    "        vaa_pd = gpd.read_file(\n",
    "            filename=nhd_huc4_pathdict[huc4], \n",
    "            layer='NHDPlusFlowlineVAA',\n",
    "            ignore_geometry=False\n",
    "        )\n",
    "\n",
    "        flowlines_vaa = flowlines_gpd.merge(\n",
    "            vaa_pd, how='inner', on='NHDPlusID', suffixes=('', '_vaa'))\n",
    "\n",
    "        nhdplus_huc4_dict[huc4] = flowlines_vaa\n",
    "    \n",
    "    return(nhdplus_huc4_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4c209-e270-401d-aa94-c71deb7593d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoglows_vpu(in_geoglows_vpu_path,\n",
    "                     in_xytab=None, lon_col=None, lat_col=None,\n",
    "                     in_vector=None, \n",
    "                     in_id_list=None, \n",
    "                     in_refids_parquet=None,\n",
    "                     hull=True, sjoin_predicate='intersects'):\n",
    "    \n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        vpus = gpd.read_file(filename=in_geoglows_vpu_path, \n",
    "                                 layer='vpu-boundaries',\n",
    "                                 columns=['VPU']\n",
    "                                )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_vpus = gpd.sjoin(gdf_to_join.to_crs(crs=vpus.crs), \n",
    "                                 vpus, \n",
    "                                 how='left', \n",
    "                                 predicate=sjoin_predicate)\n",
    "        vpu_list = matched_vpus.VPU.tolist()\n",
    "\n",
    "    return(vpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42179e20-7d0f-41eb-96f3-a164a431ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gadm_lev1_dict(in_gadm_path,\n",
    "                  in_xytab=None, lon_col=None, lat_col=None,\n",
    "                  in_vector=None, \n",
    "                  in_id_list=None, \n",
    "                  in_refids_parquet=None,\n",
    "                  hull=True, sjoin_predicate='intersects'):\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Get GADM data\n",
    "        gadm_gpd = gpd.read_file(filename=in_gadm_path, layer='ADM_1')\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_adm_units = gpd.sjoin(\n",
    "            gadm_gpd,\n",
    "            gdf_to_join.to_crs(crs=gadm_gpd.crs), \n",
    "            how='inner', \n",
    "            predicate=sjoin_predicate)\n",
    "        #print(matched_adm_units)\n",
    "\n",
    "    return(matched_adm_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755649e-dd8e-4dfa-89db-4e0bf4091f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Run functions\n",
    "# hu12_list = create_huc12_list(wbd_path, hu12_parquet)\n",
    "\n",
    "# basinatlas11_list = create_basinatlas11_list(\n",
    "#     basinatlas_path, \n",
    "#     basinatlas11_parquet)\n",
    "\n",
    "# test_huc_pd = get_matching_NHD_HU(\n",
    "#     in_wbd_path=wbd_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     in_refids_parquet=hu12_parquet,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )\n",
    "# #print(test_huc_pd)\n",
    "# #in_id_list = in_umrb_huc4s = [f'07{str(i).zfill(2)}' for i in range(2,15)]\n",
    "\n",
    "# test_nhd_hydronyms = get_nhd_hydronyms(\n",
    "#     in_hucs=test_huc_pd,\n",
    "#     in_wbd_path=wbd_path,\n",
    "#     out_dir = os.path.join(nhd_dir, 'nhdplus_hr'),\n",
    "#     huc_range=[2, 4, 6],\n",
    "#     verbose=False\n",
    "# )\n",
    "# len(test_nhd_hydronyms)\n",
    "\n",
    "# test_pfaf_pd = get_matching_hydrobasin(\n",
    "#     in_basinatlas_path=basinatlas_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     #in_id_list=None, \n",
    "#     in_refids_parquet=basinatlas11_parquet,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )\n",
    "# #print(test_pfaf_pd)\n",
    "\n",
    "# test_pfaf_pd_idlist = get_matching_hydrobasin(\n",
    "#     in_basinatlas_path=basinatlas_path,\n",
    "#     in_id_list=[742873, 742875, 742876], \n",
    "#     in_refids_parquet=basinatlas11_parquet\n",
    "# )\n",
    "# #print(test_pfaf_pd_idlist)\n",
    "\n",
    "\n",
    "# test_vpu_list = get_geoglows_vpu(\n",
    "#     in_geoglows_vpu_path=geoglows_vpu_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )\n",
    "# print(test_vpu_list)\n",
    "\n",
    "# test_gadm_lev1 = get_gadm_lev1_dict(\n",
    "#     in_gadm_path=gadm_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16249f97-182e-45d5-84ec-a8955c1c4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "################IN DEVELOPMENT ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1fdcd-b9ba-4942-9af3-904dbfeed05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoglows_hydronyms(in_geoglows_path, verbose=True):\n",
    "\n",
    "    country_tab_path = os.path.join(in_geoglows_path, \n",
    "                                    'tables', 'v2-countries-table.parquet')\n",
    "    country_pd = pd.read_parquet(country_tab_path)\n",
    "\n",
    "    meta_tab_path = os.path.join(in_geoglows_path, \n",
    "                                    'tables', 'package-metadata-table.parquet')\n",
    "    meta_pd = pd.read_parquet(meta_tab_path)\n",
    "\n",
    "    model_tab_path = os.path.join(in_geoglows_path, \n",
    "                                    'tables', 'v2-model-table.parquet')\n",
    "    model_pd = pd.read_parquet(model_tab_path)\n",
    "\n",
    "    print('Getting geoglows river names')\n",
    "    for vpu in test_vpu_list[0]:\n",
    "        streams_gpd= gpd.read_file(\n",
    "                filename=nhd_huc4_pathdict[huc4], \n",
    "                layer='NHDFlowline',\n",
    "                ignore_geometry=False\n",
    "        )\n",
    "          \n",
    "# geoglows_path = os.path.join(datdir, 'geoglows')\n",
    "# get_geoglows_hydronyms(in_geoglows_path=geoglows_path, verbose=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
