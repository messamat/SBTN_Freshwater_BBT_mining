{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7c0545b1-2abe-4e10-ad2b-987cc7c6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_geographic_refs.ipynb\n",
    "%run set_up.py \n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0ad174fb-a2a9-4e12-99b2-6a3ea69c45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbd_path = os.path.join(nhd_dir, 'WBD_National_GDB.gdb')\n",
    "hu12_parquet = os.path.join(nhd_dir, 'wbd_hu12list.parquet')\n",
    "basinatlas_path = os.path.join(hydroatlas_dir,  'BasinATLAS_v10.gdb')\n",
    "basinatlas11_parquet = os.path.join(hydroatlas_dir, 'basinatlas_lev11_idlist.parquet')\n",
    "geoglows_vpu_path = os.path.join(geoglows_dir, 'vpu-boundaries.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9b9edc0a-cc1e-4f28-9aa3-1be09ea46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of NHD HUC12\n",
    "def create_huc12_list(wbd_path, \n",
    "                      out_hu12_parquet,\n",
    "                      verbose=True):\n",
    "    if not os.path.exists(out_hu12_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of HUC 12 and saving it to \\\n",
    "            {out_hu12_parquet}')\n",
    "        wbdhu12_list = gpd.read_file(filename=wbd_path, \n",
    "                                     layer='WBDHU12', \n",
    "                                     rows=105000,\n",
    "                                     columns=['huc12'],\n",
    "                                     ignore_geometry=True)\n",
    "        wbdhu12_list.to_parquet(out_hu12_parquet)\n",
    "    else:\n",
    "        wbdhu12_list = pd.read_parquet(out_hu12_parquet)\n",
    "    return(wbdhu12_list)\n",
    "\n",
    "hu12_list = create_huc12_list(wbd_path, hu12_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f92e4a12-8af2-4549-9500-e7d3131fdfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a list of PFAF ID level 11 and saving it to             D:\\WWF_SBTN\\BTT_analysis\\data\\hydroatlas\\basinatlas_lev11_idlist.parquet\n"
     ]
    }
   ],
   "source": [
    "#Create a list of PFAF_ID for basins level 11\n",
    "def create_basinatlas11_list(basinatlas_path, \n",
    "                             out_basinatlas11_parquet, \n",
    "                             verbose=True):\n",
    "    if not os.path.exists(out_basinatlas11_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of PFAF ID level 11 and saving it to \\\n",
    "            {out_basinatlas11_parquet}')\n",
    "            \n",
    "        basinatlas11_list = gpd.read_file(\n",
    "            filename=basinatlas_path, \n",
    "            layer='BasinATLAS_v10_lev11', \n",
    "            columns=['PFAF_ID'],\n",
    "            rows=1031785,\n",
    "            ignore_geometry=True).\\\n",
    "        astype(pd.Int64Dtype()).\\\n",
    "        rename(columns={\"PFAF_ID\": \"PFAF_ID11\"})\n",
    "        \n",
    "        basinatlas11_list.to_parquet(out_basinatlas11_parquet)\n",
    "    else:\n",
    "        basinatlas11_list = pd.read_parquet(out_basinatlas11_parquet)\n",
    "    return(basinatlas11_list)\n",
    "\n",
    "basinatlas11_list = create_basinatlas11_list(\n",
    "    basinatlas_path, \n",
    "    basinatlas11_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "7c216daa-b88a-49f9-ba50-677d7669e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_basin_idlist(in_id_list,\n",
    "                         in_refids_parquet,\n",
    "                         refids_col,\n",
    "                         out_id_range):\n",
    "    \"\"\"\n",
    "    Expands a list of basin IDs by extracting IDs at different levels from a reference Parquet file.\n",
    "\n",
    "    Args:\n",
    "        in_id_list (list): List of input basin IDs.  Can be integers or strings.\n",
    "        in_refids_parquet (str): Path to the reference Parquet file.\n",
    "        refids_col (str): Name of the column in the Parquet file containing the full basin IDs.\n",
    "        out_id_range (list): List of integer levels to extract.  e.g., [6, 9, 12]\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the expanded basin IDs.  The returned DataFrame\n",
    "                          will have columns named based on `refids_col` and levels in `out_id_range`.\n",
    "                          The data type of the output columns will match the input type of `in_id_list`.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the length of input IDs is inconsistent with the length in reference table.\n",
    "        FileNotFoundError: If the input Parquet file does not exist.\n",
    "        ValueError:  If `out_id_range` contains values greater than the maximum ID length.\n",
    "                   Or if an empty DataFrame is returned by filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    id_all_pd = pd.read_parquet(in_refids_parquet)\n",
    "\n",
    "    # Determine input type and maximum ID length\n",
    "    in_id_type = type(in_id_list[0])\n",
    "    if in_id_type == str:\n",
    "      in_id_len = len(in_id_list[0])\n",
    "    elif in_id_type == int:\n",
    "      in_id_len = len(str(in_id_list[0]))\n",
    "    else:\n",
    "      raise TypeError(\"in_id_list must be a list of strings or integers\")\n",
    "\n",
    "    # Determine the reference ID type and length\n",
    "    first_ref_id = id_all_pd[refids_col].iloc[0] #Get first item in col\n",
    "    refid_type = type(first_ref_id)\n",
    "\n",
    "    if refid_type == str:\n",
    "      refid_len = len(first_ref_id)\n",
    "    elif pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "      refid_len = len(str(first_ref_id))\n",
    "    else:\n",
    "      raise TypeError(f\"{refids_col} must contain strings or integers\")\n",
    "\n",
    "\n",
    "    if in_id_len > refid_len:\n",
    "        raise ValueError(f\"Input ID length ({in_id_len}) is greater than reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Input Validation on out_id_range ---\n",
    "    if any(level > refid_len for level in out_id_range):\n",
    "        raise ValueError(f\"out_id_range values cannot exceed reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Filtering ---\n",
    "    # Convert to string for consistent prefix matching, then convert back later\n",
    "    id_all_pd[refids_col] = id_all_pd[refids_col].astype(str)\n",
    "    in_id_list_str = [str(x) for x in in_id_list]\n",
    "\n",
    "    id_pd = id_all_pd[id_all_pd[refids_col].str.startswith(tuple(in_id_list_str))].copy()\n",
    "\n",
    "    if id_pd.empty:\n",
    "        raise ValueError(\"No matching IDs found. Check in_id_list and refids_col.\")\n",
    "\n",
    "    # --- Column Expansion ---\n",
    "    colroot = re.sub(r'[0-9]+', '', refids_col)\n",
    "    for id_level in out_id_range:\n",
    "        col_name = f'{colroot}{id_level}'\n",
    "        if col_name not in id_pd.columns:\n",
    "            id_pd.loc[:, col_name] = id_pd[refids_col].str[:id_level]\n",
    "            # Convert back to original type, if necessary\n",
    "            if in_id_type == int:\n",
    "                id_pd.loc[:, col_name] = pd.to_numeric(id_pd[col_name],\n",
    "                                                       errors='raise') \n",
    "\n",
    "    # Convert refids_col back to original type\n",
    "    if refid_type == int:\n",
    "        id_pd[refids_col] = pd.to_numeric(id_pd[refids_col], \n",
    "                                          errors='raise')\n",
    "\n",
    "    return id_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e8b629b3-15d0-4616-88d3-dcdd5dd59e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_gdf_tojoin(in_xytab=None, lon_col=None, lat_col=None, \n",
    "                       in_vector=None, hull=True):\n",
    "    #Read xy table\n",
    "    if in_xytab:\n",
    "        points_df = pd.read_table(point_locations_path)\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            points_df,\n",
    "            geometry=[Point(xy) for xy \n",
    "                      in zip(points_df[lon_col], points_df[lat_col])]\n",
    "        )\n",
    "    \n",
    "    #Read vector layer\n",
    "    if in_vector:\n",
    "        gdf_to_join = gpd.read_file(in_vector)\n",
    "\n",
    "    if hull:\n",
    "        # Create convex hull using union_all() (current recommended method)\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            geometry=[gdf_to_join.geometry.union_all().convex_hull], \n",
    "            crs=gdf_to_join.crs\n",
    "        )\n",
    "    return(gdf_to_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceed7e1-dd52-4480-9e1d-47b0799229f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_NHD_HU(in_wbd_path,\n",
    "                        in_xytab=None, lon_col=None, lat_col=None,\n",
    "                        in_vector=None, \n",
    "                        in_id_list=None, \n",
    "                        in_refids_parquet=None,\n",
    "                        hull=True, sjoin_predicate='intersects'):\n",
    "\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        wbdhu6 = gpd.read_file(filename=in_wbd_path, \n",
    "                               layer='WBDHU6',\n",
    "                               columns=['huc6']\n",
    "                              )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_nhd = gpd.sjoin(gdf_to_join.to_crs(crs=wbdhu6.crs), \n",
    "                               wbdhu6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        in_id_list = matched_nhd.huc6.values.tolist()\n",
    "        \n",
    "    huc_pd = _expand_basin_idlist(\n",
    "        in_id_list, \n",
    "        in_refids_parquet, \n",
    "        refids_col='huc12', \n",
    "        out_id_range=range(2, 14, 2))\n",
    "\n",
    "    return(huc_pd)\n",
    "\n",
    "test_pts_path = os.path.join(datdir, 'test_gages', 'test_gages.shp')\n",
    "test_huc_pd = get_matching_NHD_HU(\n",
    "    in_wbd_path=wbd_path,\n",
    "    in_vector=test_pts_path,\n",
    "    in_refids_parquet=hu12_parquet,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_huc_pd)\n",
    "\n",
    "#in_id_list = in_umrb_huc4s = [f'07{str(i).zfill(2)}' for i in range(2,15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752ba54-3c3c-484a-bdb1-bc440a3096ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_hydrobasin(in_basinatlas_path,\n",
    "                            in_xytab=None, lon_col=None, lat_col=None,\n",
    "                            in_vector=None, \n",
    "                            in_id_list=None, \n",
    "                            in_refids_parquet=None,\n",
    "                            hull=True, sjoin_predicate='intersects'):\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        bas_lev6 = gpd.read_file(filename=in_basinatlas_path, \n",
    "                                 layer='BasinATLAS_v10_lev06',\n",
    "                                 columns=['PFAF_ID']\n",
    "                                ).rename(columns={\"PFAF_ID\": \"PFAF_ID6\"})\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_bas = gpd.sjoin(gdf_to_join.to_crs(crs=bas_lev6.crs), \n",
    "                               bas_lev6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        in_id_list = matched_bas.PFAF_ID6.tolist()\n",
    "\n",
    "    pfaf_pd = _expand_basin_idlist(\n",
    "        in_id_list, \n",
    "        in_refids_parquet, \n",
    "        refids_col='PFAF_ID11', \n",
    "        out_id_range=range(3, 12))\n",
    "\n",
    "    return(pfaf_pd)\n",
    "\n",
    "test_pfaf_pd = get_matching_hydrobasin(\n",
    "    in_basinatlas_path=basinatlas_path,\n",
    "    in_vector=test_pts_path,\n",
    "    #in_id_list=None, \n",
    "    in_refids_parquet=basinatlas11_parquet,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_pfaf_pd)\n",
    "\n",
    "test_pfaf_pd_idlist = get_matching_hydrobasin(\n",
    "    in_basinatlas_path=basinatlas_path,\n",
    "    in_id_list=[742873, 742875, 742876], \n",
    "    in_refids_parquet=basinatlas11_parquet\n",
    ")\n",
    "print(test_pfaf_pd_idlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "63b4c209-e270-401d-aa94-c71deb7593d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['714', '709']\n"
     ]
    }
   ],
   "source": [
    "def get_geoglows_vpu(in_geoglows_vpu_path,\n",
    "                     in_xytab=None, lon_col=None, lat_col=None,\n",
    "                     in_vector=None, \n",
    "                     in_id_list=None, \n",
    "                     in_refids_parquet=None,\n",
    "                     hull=True, sjoin_predicate='intersects'):\n",
    "    \n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        vpus = gpd.read_file(filename=in_geoglows_vpu_path, \n",
    "                                 layer='vpu-boundaries',\n",
    "                                 columns=['VPU']\n",
    "                                )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_vpus = gpd.sjoin(gdf_to_join.to_crs(crs=vpus.crs), \n",
    "                                 vpus, \n",
    "                                 how='left', \n",
    "                                 predicate=sjoin_predicate)\n",
    "        vpu_list = matched_vpus.VPU.tolist()\n",
    "\n",
    "    return(vpu_list)\n",
    "\n",
    "test_vpu_list = get_geoglows_vpu(\n",
    "    in_geoglows_vpu_path=geoglows_vpu_path,\n",
    "    in_vector=test_pts_path,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_vpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a995b-fef7-4aaf-8984-203a0d31a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spatial_data(point_locations_path, nhd_path, hydrobasins_path, admin_units_path):\n",
    "    # Convert your points to GeoDataFrame\n",
    "\n",
    "    \n",
    "    # Load boundary files\n",
    "    nhd_gdf = gpd.read_file(nhd_path)\n",
    "    hydrobasins_gdf = gpd.read_file(hydrobasins_path)\n",
    "    admin_gdf = gpd.read_file(admin_units_path)\n",
    "    \n",
    "    return points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f511f18-ad44-486c-8248-8e90ff4146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform spatial joins to get catchment information\n",
    "def get_catchment_info(point_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf):\n",
    "    # Spatial joins\n",
    "    points_nhd = gpd.sjoin(point_gdf, nhd_gdf, how='left', op='within')\n",
    "    points_hydrobasins = gpd.sjoin(point_gdf, hydrobasins_gdf, how='left', op='within')\n",
    "    points_admin = gpd.sjoin(point_gdf, admin_gdf, how='left', op='within')\n",
    "    \n",
    "    # Collect all relevant names\n",
    "    location_names = set()\n",
    "    \n",
    "    # Add names from each source (adjust column names as needed)\n",
    "    name_columns = {\n",
    "        'nhd': ['BASIN_NAME', 'RIVER_NAME'],\n",
    "        'hydrobasins': ['HYBAS_NAME'],\n",
    "        'admin': ['COUNTY_NAME', 'STATE_NAME', 'COUNTRY_NAME']\n",
    "    }\n",
    "    \n",
    "    for df, cols in zip([points_nhd, points_hydrobasins, points_admin], \n",
    "                       name_columns.values()):\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                location_names.update(df[col].dropna().unique())\n",
    "    \n",
    "    return location_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67f9c7-0ab0-450b-b060-0d50462acbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process OpenAlex records\n",
    "def process_openalex_records(openalex_df, location_names):\n",
    "    # Function to generate n-grams\n",
    "    def get_ngrams(text, n_range=(1, 3)):\n",
    "        if pd.isna(text):\n",
    "            return set()\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        all_ngrams = set()\n",
    "        \n",
    "        for n in range(n_range[0], n_range[1] + 1):\n",
    "            text_ngrams = set(' '.join(gram) for gram in ngrams(tokens, n))\n",
    "            all_ngrams.update(text_ngrams)\n",
    "            \n",
    "        return all_ngrams\n",
    "\n",
    "    # Convert location names to lowercase for matching\n",
    "    location_names = set(name.lower() for name in location_names)\n",
    "    \n",
    "    # Process each record\n",
    "    matched_records = []\n",
    "    \n",
    "    for _, record in openalex_df.iterrows():\n",
    "        # Combine all text fields\n",
    "        text_fields = [\n",
    "            str(record.get('title', '')),\n",
    "            str(record.get('abstract', '')),\n",
    "            str(record.get('keywords', ''))\n",
    "        ]\n",
    "        \n",
    "        combined_text = ' '.join(text_fields).lower()\n",
    "        \n",
    "        # Generate n-grams from the combined text\n",
    "        record_ngrams = get_ngrams(' '.join(text_fields))\n",
    "        \n",
    "        # Check for matches\n",
    "        if any(location in combined_text for location in location_names) or \\\n",
    "           any(location in record_ngrams for location in location_names):\n",
    "            matched_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(matched_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41e85c-cc5c-46e8-8a3e-28a3034fd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Load your data (replace with actual file paths)\n",
    "    points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf = load_spatial_data(\n",
    "        'points.csv',\n",
    "        'nhd.shp',\n",
    "        'hydrobasins.shp',\n",
    "        'admin_units.shp'\n",
    "    )\n",
    "    \n",
    "    # Get catchment information\n",
    "    location_names = get_catchment_info(points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf)\n",
    "    \n",
    "    # Load and process OpenAlex records\n",
    "    openalex_df = pd.read_csv('openalex_records.csv')  # Replace with actual file path\n",
    "    \n",
    "    # Filter records based on location names\n",
    "    matched_records = process_openalex_records(openalex_df, location_names)\n",
    "    \n",
    "    # Save results\n",
    "    matched_records.to_csv('matched_records.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
