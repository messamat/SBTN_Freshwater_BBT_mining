{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c0545b1-2abe-4e10-ad2b-987cc7c6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_geographic_refs.ipynb\n",
    "%run set_up.py \n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9edc0a-cc1e-4f28-9aa3-1be09ea46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of NHD HUC12\n",
    "in_wbd_path = os.path.join(nhd_dir, 'WBD_National_GDB.gdb')\n",
    "out_hu12_parquet = os.path.join(nhd_dir, 'wbd_hu12list.parquet')\n",
    "if not os.path.exists(out_hu12_parquet):\n",
    "    if verbose:\n",
    "        print(f'Generating a list of HUC 12 and saving it to {out_hu12_parquet}')\n",
    "    wbdhu12_list = gpd.read_file(filename=in_wbd_path, layer='WBDHU12', \n",
    "                          rows=105000,\n",
    "                          columns=['huc12'], ignore_geometry=True)\n",
    "    wbdhu12_list.to_parquet(out_hu12_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517ef609-07c4-4b50-b632-33960e215a53",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 7 (3476286365.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    def get_matching_NHD_HU(in_wbd_path,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 7\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your spatial data\n",
    "\n",
    "#point locations\n",
    "#geographic coordinate table\n",
    "#huc number\n",
    "\n",
    "def create_HUC_db():\n",
    "\n",
    "def get_matching_NHD_HU(in_wbd_path,\n",
    "                        in_points=None, lon_col=None, lat_col=None,\n",
    "                        in_polygons=None, \n",
    "                        in_id=None, \n",
    "                        hull=True):\n",
    "\n",
    "    if in_id:\n",
    "        if (not isinstance(in_id, 'str')) or (len(in_id) in range(2, 14, 2)) :\n",
    "            raise TypeError(\"in_id argument must be a string of even number \\\n",
    "            of digits between 2 and 12\")\n",
    "        huc_len = len(in_id)\n",
    "        wbdhu = gpd.read_file(filename=in_wbd_path, \n",
    "                              layer=f'WBD/WBDHU12')\n",
    "\n",
    " \n",
    "            \n",
    "  \n",
    "    root_url = \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHD/HU4/GPKG/\"\n",
    "    zip_name = f\"NHD_H_{hu4}_HU4_GPKG.zip\"\n",
    "    full_url = urllib.parse.urljoin(root_url, zip_name)\n",
    "        \n",
    "    #Read points\n",
    "    if in_points:\n",
    "        points_ext = os.path.splitext(in_points)\n",
    "        if points_ext =='.csv':\n",
    "            points_df = pd.read_csv(point_locations_path)\n",
    "            points_gdf = gpd.GeoDataFrame(\n",
    "                points_df,\n",
    "                geometry=[Point(xy) for xy \n",
    "                          in zip(points_df[lon_col], points_df[lat_col])]\n",
    "            )\n",
    "        if else points_ext in ['.gpkg', '.shp', '.gdb']:\n",
    "            points_gdf = gpd.read_file(in_points)\n",
    "            \n",
    "    #Read polygons\n",
    "    if in_polygons:\n",
    "        poly_gdf = gpd.read_file(in_polygons)\n",
    "\n",
    "    #Reach NHD WBD\n",
    "    wbdhu4 = gpd.read_file(filename=in_wbd_path, layer='WBD/WBDHU4')\n",
    "    \n",
    "    if hull:\n",
    "        points_gdf.union_all().convex_hull\n",
    "    \n",
    "    #Spatially join to hydrologic units\n",
    "    points_nhd = gpd.sjoin(point_gdf, nhd_gdf, how='left', op='within')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d425e95-9897-4715-bd88-bcd855bbb871",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'verbose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m out_hu12_parquet \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(nhd_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwbd_hu12list.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(out_hu12_parquet):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating a list of HUC 12 and saving it to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_hu12_parquet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     wbdhu12_list \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(filename\u001b[38;5;241m=\u001b[39min_wbd_path, layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWBDHU12\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      7\u001b[0m                           rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m105000\u001b[39m,\n\u001b[0;32m      8\u001b[0m                           columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuc12\u001b[39m\u001b[38;5;124m'\u001b[39m], ignore_geometry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'verbose' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752ba54-3c3c-484a-bdb1-bc440a3096ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_hydrobasin(in_points=None, in_polygon=None, \n",
    "                            in_table=None, lon_col=None, lat_col=None,\n",
    "                            in_id=None, hull=True):\n",
    "\n",
    "\n",
    "def get_matching_geoglows_vpu(in_points=None, in_polygon=None, \n",
    "                              in_table=None, lon_col=None, lat_col=None,\n",
    "                              in_id=None, hull=True):\n",
    "    \n",
    "\n",
    "def load_spatial_data(point_locations_path, nhd_path, hydrobasins_path, admin_units_path):\n",
    "    # Convert your points to GeoDataFrame\n",
    "\n",
    "    \n",
    "    # Load boundary files\n",
    "    nhd_gdf = gpd.read_file(nhd_path)\n",
    "    hydrobasins_gdf = gpd.read_file(hydrobasins_path)\n",
    "    admin_gdf = gpd.read_file(admin_units_path)\n",
    "    \n",
    "    return points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f511f18-ad44-486c-8248-8e90ff4146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform spatial joins to get catchment information\n",
    "def get_catchment_info(point_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf):\n",
    "    # Spatial joins\n",
    "    points_nhd = gpd.sjoin(point_gdf, nhd_gdf, how='left', op='within')\n",
    "    points_hydrobasins = gpd.sjoin(point_gdf, hydrobasins_gdf, how='left', op='within')\n",
    "    points_admin = gpd.sjoin(point_gdf, admin_gdf, how='left', op='within')\n",
    "    \n",
    "    # Collect all relevant names\n",
    "    location_names = set()\n",
    "    \n",
    "    # Add names from each source (adjust column names as needed)\n",
    "    name_columns = {\n",
    "        'nhd': ['BASIN_NAME', 'RIVER_NAME'],\n",
    "        'hydrobasins': ['HYBAS_NAME'],\n",
    "        'admin': ['COUNTY_NAME', 'STATE_NAME', 'COUNTRY_NAME']\n",
    "    }\n",
    "    \n",
    "    for df, cols in zip([points_nhd, points_hydrobasins, points_admin], \n",
    "                       name_columns.values()):\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                location_names.update(df[col].dropna().unique())\n",
    "    \n",
    "    return location_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67f9c7-0ab0-450b-b060-0d50462acbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process OpenAlex records\n",
    "def process_openalex_records(openalex_df, location_names):\n",
    "    # Function to generate n-grams\n",
    "    def get_ngrams(text, n_range=(1, 3)):\n",
    "        if pd.isna(text):\n",
    "            return set()\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        all_ngrams = set()\n",
    "        \n",
    "        for n in range(n_range[0], n_range[1] + 1):\n",
    "            text_ngrams = set(' '.join(gram) for gram in ngrams(tokens, n))\n",
    "            all_ngrams.update(text_ngrams)\n",
    "            \n",
    "        return all_ngrams\n",
    "\n",
    "    # Convert location names to lowercase for matching\n",
    "    location_names = set(name.lower() for name in location_names)\n",
    "    \n",
    "    # Process each record\n",
    "    matched_records = []\n",
    "    \n",
    "    for _, record in openalex_df.iterrows():\n",
    "        # Combine all text fields\n",
    "        text_fields = [\n",
    "            str(record.get('title', '')),\n",
    "            str(record.get('abstract', '')),\n",
    "            str(record.get('keywords', ''))\n",
    "        ]\n",
    "        \n",
    "        combined_text = ' '.join(text_fields).lower()\n",
    "        \n",
    "        # Generate n-grams from the combined text\n",
    "        record_ngrams = get_ngrams(' '.join(text_fields))\n",
    "        \n",
    "        # Check for matches\n",
    "        if any(location in combined_text for location in location_names) or \\\n",
    "           any(location in record_ngrams for location in location_names):\n",
    "            matched_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(matched_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41e85c-cc5c-46e8-8a3e-28a3034fd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Load your data (replace with actual file paths)\n",
    "    points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf = load_spatial_data(\n",
    "        'points.csv',\n",
    "        'nhd.shp',\n",
    "        'hydrobasins.shp',\n",
    "        'admin_units.shp'\n",
    "    )\n",
    "    \n",
    "    # Get catchment information\n",
    "    location_names = get_catchment_info(points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf)\n",
    "    \n",
    "    # Load and process OpenAlex records\n",
    "    openalex_df = pd.read_csv('openalex_records.csv')  # Replace with actual file path\n",
    "    \n",
    "    # Filter records based on location names\n",
    "    matched_records = process_openalex_records(openalex_df, location_names)\n",
    "    \n",
    "    # Save results\n",
    "    matched_records.to_csv('matched_records.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
