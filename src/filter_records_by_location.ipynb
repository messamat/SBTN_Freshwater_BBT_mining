{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0545b1-2abe-4e10-ad2b-987cc7c6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_geographic_refs.ipynb\n",
    "%run set_up.py \n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad174fb-a2a9-4e12-99b2-6a3ea69c45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wbd_path = os.path.join(nhd_dir, 'WBD_National_GDB.gdb')\n",
    "hu12_parquet = os.path.join(nhd_dir, 'wbd_hu12list.parquet')\n",
    "basinatlas_path = os.path.join(hydroatlas_dir,  'BasinATLAS_v10.gdb')\n",
    "basinatlas11_parquet = os.path.join(hydroatlas_dir, 'basinatlas_lev11_idlist.parquet')\n",
    "geoglows_vpu_path = os.path.join(geoglows_dir, 'vpu-boundaries.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9edc0a-cc1e-4f28-9aa3-1be09ea46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of NHD HUC12\n",
    "def create_huc12_list(wbd_path, \n",
    "                      out_hu12_parquet,\n",
    "                      verbose=True):\n",
    "    if not os.path.exists(out_hu12_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of HUC 12 and saving it to \\\n",
    "            {out_hu12_parquet}')\n",
    "        wbdhu12_list = gpd.read_file(filename=wbd_path, \n",
    "                                     layer='WBDHU12', \n",
    "                                     rows=105000,\n",
    "                                     columns=['huc12'],\n",
    "                                     ignore_geometry=True)\n",
    "        wbdhu12_list.to_parquet(out_hu12_parquet)\n",
    "    else:\n",
    "        wbdhu12_list = pd.read_parquet(out_hu12_parquet)\n",
    "    return(wbdhu12_list)\n",
    "\n",
    "hu12_list = create_huc12_list(wbd_path, hu12_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92e4a12-8af2-4549-9500-e7d3131fdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of PFAF_ID for basins level 11\n",
    "def create_basinatlas11_list(basinatlas_path, \n",
    "                             out_basinatlas11_parquet, \n",
    "                             verbose=True):\n",
    "    if not os.path.exists(out_basinatlas11_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of PFAF ID level 11 and saving it to \\\n",
    "            {out_basinatlas11_parquet}')\n",
    "            \n",
    "        basinatlas11_list = gpd.read_file(\n",
    "            filename=basinatlas_path, \n",
    "            layer='BasinATLAS_v10_lev11', \n",
    "            columns=['PFAF_ID'],\n",
    "            rows=1031785,\n",
    "            ignore_geometry=True).\\\n",
    "        astype(pd.Int64Dtype()).\\\n",
    "        rename(columns={\"PFAF_ID\": \"PFAF_ID11\"})\n",
    "        \n",
    "        basinatlas11_list.to_parquet(out_basinatlas11_parquet)\n",
    "    else:\n",
    "        basinatlas11_list = pd.read_parquet(out_basinatlas11_parquet)\n",
    "    return(basinatlas11_list)\n",
    "\n",
    "basinatlas11_list = create_basinatlas11_list(\n",
    "    basinatlas_path, \n",
    "    basinatlas11_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c216daa-b88a-49f9-ba50-677d7669e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _expand_basin_idlist(in_id_list,\n",
    "                         in_refids_parquet,\n",
    "                         refids_col,\n",
    "                         out_id_range):\n",
    "    \"\"\"\n",
    "    Expands a list of basin IDs by extracting IDs at different levels from a reference Parquet file.\n",
    "\n",
    "    Args:\n",
    "        in_id_list (list): List of input basin IDs.  Can be integers or strings.\n",
    "        in_refids_parquet (str): Path to the reference Parquet file.\n",
    "        refids_col (str): Name of the column in the Parquet file containing the full basin IDs.\n",
    "        out_id_range (list): List of integer levels to extract.  e.g., [6, 9, 12]\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the expanded basin IDs.  The returned DataFrame\n",
    "                          will have columns named based on `refids_col` and levels in `out_id_range`.\n",
    "                          The data type of the output columns will match the input type of `in_id_list`.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the length of input IDs is inconsistent with the length in reference table.\n",
    "        FileNotFoundError: If the input Parquet file does not exist.\n",
    "        ValueError:  If `out_id_range` contains values greater than the maximum ID length.\n",
    "                   Or if an empty DataFrame is returned by filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    id_all_pd = pd.read_parquet(in_refids_parquet)\n",
    "\n",
    "    # Determine input type and maximum ID length\n",
    "    in_id_type = type(in_id_list[0])\n",
    "    if in_id_type == str:\n",
    "      in_id_len = len(in_id_list[0])\n",
    "    elif in_id_type == int:\n",
    "      in_id_len = len(str(in_id_list[0]))\n",
    "    else:\n",
    "      raise TypeError(\"in_id_list must be a list of strings or integers\")\n",
    "\n",
    "    # Determine the reference ID type and length\n",
    "    first_ref_id = id_all_pd[refids_col].iloc[0] #Get first item in col\n",
    "    refid_type = type(first_ref_id)\n",
    "\n",
    "    if refid_type == str:\n",
    "      refid_len = len(first_ref_id)\n",
    "    elif pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "      refid_len = len(str(first_ref_id))\n",
    "    else:\n",
    "      raise TypeError(f\"{refids_col} must contain strings or integers\")\n",
    "\n",
    "\n",
    "    if in_id_len > refid_len:\n",
    "        raise ValueError(f\"Input ID length ({in_id_len}) is greater than reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Input Validation on out_id_range ---\n",
    "    if any(level > refid_len for level in out_id_range):\n",
    "        raise ValueError(f\"out_id_range values cannot exceed reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Filtering ---\n",
    "    # Convert to string for consistent prefix matching, then convert back later\n",
    "    id_all_pd[refids_col] = id_all_pd[refids_col].astype(str)\n",
    "    in_id_list_str = [str(x) for x in in_id_list]\n",
    "\n",
    "    id_pd = id_all_pd[id_all_pd[refids_col].str.startswith(tuple(in_id_list_str))].copy()\n",
    "\n",
    "    if id_pd.empty:\n",
    "        raise ValueError(\"No matching IDs found. Check in_id_list and refids_col.\")\n",
    "\n",
    "    # --- Column Expansion ---\n",
    "    colroot = re.sub(r'[0-9]+', '', refids_col)\n",
    "    for id_level in out_id_range:\n",
    "        col_name = f'{colroot}{id_level}'\n",
    "        if col_name not in id_pd.columns:\n",
    "            id_pd.loc[:, col_name] = id_pd[refids_col].str[:id_level]\n",
    "            # Convert back to original type, if necessary\n",
    "            if in_id_type == int:\n",
    "                id_pd.loc[:, col_name] = pd.to_numeric(id_pd[col_name],\n",
    "                                                       errors='raise') \n",
    "\n",
    "    # Convert refids_col back to original type\n",
    "    if pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "        id_pd[refids_col] = pd.to_numeric(id_pd[refids_col], \n",
    "                                          errors='raise')\n",
    "\n",
    "    return id_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b629b3-15d0-4616-88d3-dcdd5dd59e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_gdf_tojoin(in_xytab=None, lon_col=None, lat_col=None, \n",
    "                       in_vector=None, hull=True):\n",
    "    #Read xy table\n",
    "    if in_xytab:\n",
    "        points_df = pd.read_table(point_locations_path)\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            points_df,\n",
    "            geometry=[Point(xy) for xy \n",
    "                      in zip(points_df[lon_col], points_df[lat_col])]\n",
    "        )\n",
    "    \n",
    "    #Read vector layer\n",
    "    if in_vector:\n",
    "        gdf_to_join = gpd.read_file(in_vector)\n",
    "\n",
    "    if hull:\n",
    "        # Create convex hull using union_all() (current recommended method)\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            geometry=[gdf_to_join.geometry.union_all().convex_hull], \n",
    "            crs=gdf_to_join.crs\n",
    "        )\n",
    "    return(gdf_to_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ceed7e1-dd52-4480-9e1d-47b0799229f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               huc12 huc2  huc4    huc6      huc8       huc10\n",
      "0       070200090402   07  0702  070200  07020009  0702000904\n",
      "1       070200030503   07  0702  070200  07020003  0702000305\n",
      "2       070200030602   07  0702  070200  07020003  0702000306\n",
      "6       070200030701   07  0702  070200  07020003  0702000307\n",
      "7       070300040804   07  0703  070300  07030004  0703000408\n",
      "...              ...  ...   ...     ...       ...         ...\n",
      "101242  071200010601   07  0712  071200  07120001  0712000106\n",
      "101243  071200011301   07  0712  071200  07120001  0712000113\n",
      "101244  071200010501   07  0712  071200  07120001  0712000105\n",
      "101246  071200011002   07  0712  071200  07120001  0712000110\n",
      "101248  071200030306   07  0712  071200  07120003  0712000303\n",
      "\n",
      "[11145 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_matching_NHD_HU(in_wbd_path,\n",
    "                        in_xytab=None, lon_col=None, lat_col=None,\n",
    "                        in_vector=None, \n",
    "                        in_id_list=None, \n",
    "                        in_refids_parquet=None,\n",
    "                        hull=True, sjoin_predicate='intersects'):\n",
    "\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Get NHD WBD\n",
    "        wbdhu6 = gpd.read_file(filename=in_wbd_path, \n",
    "                               layer='WBDHU6',\n",
    "                               columns=['huc6']\n",
    "                              )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_nhd = gpd.sjoin(gdf_to_join.to_crs(crs=wbdhu6.crs), \n",
    "                               wbdhu6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        in_id_list = matched_nhd.huc6.values.tolist()\n",
    "        \n",
    "    huc_pd = _expand_basin_idlist(\n",
    "        in_id_list, \n",
    "        in_refids_parquet, \n",
    "        refids_col='huc12', \n",
    "        out_id_range=range(2, 14, 2))\n",
    "\n",
    "    return(huc_pd)\n",
    "\n",
    "test_pts_path = os.path.join(datdir, 'test_gages', 'test_gages.shp')\n",
    "test_huc_pd = get_matching_NHD_HU(\n",
    "    in_wbd_path=wbd_path,\n",
    "    in_vector=test_pts_path,\n",
    "    in_refids_parquet=hu12_parquet,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_huc_pd)\n",
    "\n",
    "#in_id_list = in_umrb_huc4s = [f'07{str(i).zfill(2)}' for i in range(2,15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1752ba54-3c3c-484a-bdb1-bc440a3096ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mamessager\\anaconda3\\Lib\\site-packages\\pyogrio\\raw.py:198: RuntimeWarning: organizePolygons() received a polygon with more than 100 parts. The processing may be really slow.  You can skip the processing by setting METHOD=SKIP, or only make it analyze counter-clock wise parts by setting METHOD=ONLY_CCW if you can assume that the outline of holes is counter-clock wise defined\n",
      "  return ogr_read(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          PFAF_ID11 PFAF_ID3 PFAF_ID4 PFAF_ID5 PFAF_ID6 PFAF_ID7  PFAF_ID8  \\\n",
      "904887  72582801010      725     7258    72582   725828  7258280  72582801   \n",
      "904957  72582801031      725     7258    72582   725828  7258280  72582801   \n",
      "904959  72582801020      725     7258    72582   725828  7258280  72582801   \n",
      "905050  72582801032      725     7258    72582   725828  7258280  72582801   \n",
      "905108  72582803010      725     7258    72582   725828  7258280  72582803   \n",
      "...             ...      ...      ...      ...      ...      ...       ...   \n",
      "933377  74289780900      742     7428    74289   742897  7428978  74289780   \n",
      "933386  74288609310      742     7428    74288   742886  7428860  74288609   \n",
      "933387  74288609200      742     7428    74288   742886  7428860  74288609   \n",
      "933532  74288609320      742     7428    74288   742886  7428860  74288609   \n",
      "933590  74288609330      742     7428    74288   742886  7428860  74288609   \n",
      "\n",
      "         PFAF_ID9   PFAF_ID10  \n",
      "904887  725828010  7258280101  \n",
      "904957  725828010  7258280103  \n",
      "904959  725828010  7258280102  \n",
      "905050  725828010  7258280103  \n",
      "905108  725828030  7258280301  \n",
      "...           ...         ...  \n",
      "933377  742897809  7428978090  \n",
      "933386  742886093  7428860931  \n",
      "933387  742886092  7428860920  \n",
      "933532  742886093  7428860932  \n",
      "933590  742886093  7428860933  \n",
      "\n",
      "[6542 rows x 9 columns]\n",
      "          PFAF_ID11 PFAF_ID3 PFAF_ID4 PFAF_ID5 PFAF_ID6 PFAF_ID7  PFAF_ID8  \\\n",
      "924083  74287310010      742     7428    74287   742873  7428731  74287310   \n",
      "924199  74287310031      742     7428    74287   742873  7428731  74287310   \n",
      "924200  74287310020      742     7428    74287   742873  7428731  74287310   \n",
      "924273  74287310032      742     7428    74287   742873  7428731  74287310   \n",
      "924500  74287331010      742     7428    74287   742873  7428733  74287331   \n",
      "...             ...      ...      ...      ...      ...      ...       ...   \n",
      "931686  74287696030      742     7428    74287   742876  7428769  74287696   \n",
      "931762  74287699010      742     7428    74287   742876  7428769  74287699   \n",
      "931764  74287698010      742     7428    74287   742876  7428769  74287698   \n",
      "931855  74287698020      742     7428    74287   742876  7428769  74287698   \n",
      "931944  74287699020      742     7428    74287   742876  7428769  74287699   \n",
      "\n",
      "         PFAF_ID9   PFAF_ID10  \n",
      "924083  742873100  7428731001  \n",
      "924199  742873100  7428731003  \n",
      "924200  742873100  7428731002  \n",
      "924273  742873100  7428731003  \n",
      "924500  742873310  7428733101  \n",
      "...           ...         ...  \n",
      "931686  742876960  7428769603  \n",
      "931762  742876990  7428769901  \n",
      "931764  742876980  7428769801  \n",
      "931855  742876980  7428769802  \n",
      "931944  742876990  7428769902  \n",
      "\n",
      "[590 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_matching_hydrobasin(in_basinatlas_path,\n",
    "                            in_xytab=None, lon_col=None, lat_col=None,\n",
    "                            in_vector=None, \n",
    "                            in_id_list=None, \n",
    "                            in_refids_parquet=None,\n",
    "                            hull=True, sjoin_predicate='intersects'):\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        bas_lev6 = gpd.read_file(filename=in_basinatlas_path, \n",
    "                                 layer='BasinATLAS_v10_lev06',\n",
    "                                 columns=['PFAF_ID']\n",
    "                                ).rename(columns={\"PFAF_ID\": \"PFAF_ID6\"})\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_bas = gpd.sjoin(gdf_to_join.to_crs(crs=bas_lev6.crs), \n",
    "                               bas_lev6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        in_id_list = matched_bas.PFAF_ID6.tolist()\n",
    "\n",
    "    pfaf_pd = _expand_basin_idlist(\n",
    "        in_id_list, \n",
    "        in_refids_parquet, \n",
    "        refids_col='PFAF_ID11', \n",
    "        out_id_range=range(3, 12))\n",
    "\n",
    "    return(pfaf_pd)\n",
    "\n",
    "test_pfaf_pd = get_matching_hydrobasin(\n",
    "    in_basinatlas_path=basinatlas_path,\n",
    "    in_vector=test_pts_path,\n",
    "    #in_id_list=None, \n",
    "    in_refids_parquet=basinatlas11_parquet,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_pfaf_pd)\n",
    "\n",
    "test_pfaf_pd_idlist = get_matching_hydrobasin(\n",
    "    in_basinatlas_path=basinatlas_path,\n",
    "    in_id_list=[742873, 742875, 742876], \n",
    "    in_refids_parquet=basinatlas11_parquet\n",
    ")\n",
    "print(test_pfaf_pd_idlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b4c209-e270-401d-aa94-c71deb7593d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['714', '709']\n"
     ]
    }
   ],
   "source": [
    "def get_geoglows_vpu(in_geoglows_vpu_path,\n",
    "                     in_xytab=None, lon_col=None, lat_col=None,\n",
    "                     in_vector=None, \n",
    "                     in_id_list=None, \n",
    "                     in_refids_parquet=None,\n",
    "                     hull=True, sjoin_predicate='intersects'):\n",
    "    \n",
    "    if in_xytab or in_vector:\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab, lon_col, lat_col, \n",
    "                                         in_vector, \n",
    "                                         hull)\n",
    "        \n",
    "        #Reach NHD WBD\n",
    "        vpus = gpd.read_file(filename=in_geoglows_vpu_path, \n",
    "                                 layer='vpu-boundaries',\n",
    "                                 columns=['VPU']\n",
    "                                )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        matched_vpus = gpd.sjoin(gdf_to_join.to_crs(crs=vpus.crs), \n",
    "                                 vpus, \n",
    "                                 how='left', \n",
    "                                 predicate=sjoin_predicate)\n",
    "        vpu_list = matched_vpus.VPU.tolist()\n",
    "\n",
    "    return(vpu_list)\n",
    "\n",
    "test_vpu_list = get_geoglows_vpu(\n",
    "    in_geoglows_vpu_path=geoglows_vpu_path,\n",
    "    in_vector=test_pts_path,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_vpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffc30c-426d-41df-bcd4-2fb3a9ffb4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting NHD basin names\n",
      "Creating D:\\WWF_SBTN\\BTT_analysis\\data\\nhd\\nhdplus_hr\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0702_HU4_GDB.zip...\n",
      "Extracting zip file...\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0703_HU4_GDB.zip...\n",
      "Extracting zip file...\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0709_HU4_GDB.zip...\n",
      "Extracting zip file...\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0707_HU4_GDB.zip...\n",
      "Extracting zip file...\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0704_HU4_GDB.zip...\n",
      "Extracting zip file...\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0701_HU4_GDB.zip...\n",
      "Extracting zip file...\n",
      "Downloading from https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/NHDPlusHR/Beta/GDB/NHDPLUS_H_0512_HU4_GDB.zip...\n"
     ]
    }
   ],
   "source": [
    "def get_nhd_hydronyms(in_hucs, in_wbd_path,  out_dir,\n",
    "                      huc_range=range(2, 14, 2)\n",
    "                    ):\n",
    "    print('Getting NHD basin names')\n",
    "    #If panda dataframe\n",
    "    #Get basin names------------------------------------------------------------\n",
    "    for coln in in_hucs.columns:\n",
    "        huc_len = re.sub(r'[a-zA-Z]+', '', coln)\n",
    "        if int(huc_len) is None:\n",
    "            raise ValueError(f\"HUC level cannot be extracted from {coln}\")\n",
    "        if int(huc_len) in huc_range:\n",
    "            wbd = gpd.read_file(filename=in_wbd_path, \n",
    "                                layer=f'WBDHU{huc_len}',\n",
    "                                columns=[coln, 'name'],\n",
    "                                ignore_geometry=True\n",
    "                               )\n",
    "            in_hucs = in_hucs.merge(wbd, on=coln, how='left').\\\n",
    "            rename(columns={\"name\": f\"{coln}_name\"})\n",
    "\n",
    "    #Download data by HU4 if needed\n",
    "    huc4_list = in_hucs.huc4.unique()\n",
    "    nhd_huc4_pathdict = {}\n",
    "    for huc in huc4_list:\n",
    "        download_nhdplus_hr_hu4(\n",
    "            hu4=huc, \n",
    "            out_dir=out_dir, \n",
    "            verbose=True\n",
    "        )\n",
    "        nhd_huc4_pathdict[huc] = os.path.join(out_dir, f'NHD_H_{huc}_HU4_GPKG.gpkg')\n",
    "    #print(nhd_huc4_pathdict)\n",
    "    \n",
    "    #Get river names------------------------------------------------------------\n",
    "    # for huc in nhd_huc4_pathdict:\n",
    "    #     #Read flowlines\n",
    "    #     flowlines_pgd = gpd.read_file(\n",
    "    #         filename=nhd_huc4_pathdict[huc], \n",
    "    #         layer='NHDFlowline',\n",
    "    #         columns=['permanent_identifier', 'reachcode', 'gnis_name', 'fcode']\n",
    "    #     )\n",
    "    #     vaa_pd = gpd.read_file(\n",
    "    #         filename=nhd_huc4_pathdict[huc], \n",
    "    #         layer='NHDFlowline',\n",
    "    #         columns=['permanent_identifier', 'reachcode', 'gnis_name', 'fcode']\n",
    "    #     )\n",
    "    #reach\n",
    "\n",
    "    #Subset FCODE: \n",
    "    # '''\n",
    "    # 46000: Stream/River\n",
    "    # 46003: Stream/River: Hydrographic Category = Intermittent\n",
    "    # 46006: Stream/River: Hydrographic Category = Perennial\n",
    "    # 46007: Stream/River: Hydrographic Category = Ephemeral\n",
    "    # 55800: Artificial path'''\n",
    "    \n",
    "    # fcode_sel_list = ['46000', '46006', '46003','46007', '55800']\n",
    "\n",
    "    # 'gnis_name'\n",
    "    #keep only not None\n",
    "\n",
    "\n",
    "    #Return dictionary with basin names and river names\n",
    "\n",
    "get_nhd_hydronyms(\n",
    "    in_hucs=test_huc_pd,\n",
    "    in_wbd_path=wbd_path,\n",
    "    out_dir = os.path.join(nhd_dir, 'nhdplus_hr'),\n",
    "    huc_range=[2, 4, 6, 8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1fdcd-b9ba-4942-9af3-904dbfeed05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoglows_hydronyms():\n",
    "    print('Getting geoglows river names')\n",
    "\n",
    "def get_nhd_data():\n",
    "    print('Getting NHD data')\n",
    "\n",
    "def get_hydroatlas_data():\n",
    "    print('Getting HydroATLAS data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a995b-fef7-4aaf-8984-203a0d31a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spatial_data(point_locations_path, nhd_path, hydrobasins_path, admin_units_path):\n",
    "    # Convert your points to GeoDataFrame\n",
    "\n",
    "    \n",
    "    # Load boundary files\n",
    "    nhd_gdf = gpd.read_file(nhd_path)\n",
    "    hydrobasins_gdf = gpd.read_file(hydrobasins_path)\n",
    "    admin_gdf = gpd.read_file(admin_units_path)\n",
    "    \n",
    "    return points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f511f18-ad44-486c-8248-8e90ff4146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform spatial joins to get catchment information\n",
    "def get_catchment_info(point_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf):\n",
    "    # Spatial joins\n",
    "    points_nhd = gpd.sjoin(point_gdf, nhd_gdf, how='left', op='within')\n",
    "    points_hydrobasins = gpd.sjoin(point_gdf, hydrobasins_gdf, how='left', op='within')\n",
    "    points_admin = gpd.sjoin(point_gdf, admin_gdf, how='left', op='within')\n",
    "    \n",
    "    # Collect all relevant names\n",
    "    location_names = set()\n",
    "    \n",
    "    # Add names from each source (adjust column names as needed)\n",
    "    name_columns = {\n",
    "        'nhd': ['BASIN_NAME', 'RIVER_NAME'],\n",
    "        'hydrobasins': ['HYBAS_NAME'],\n",
    "        'admin': ['COUNTY_NAME', 'STATE_NAME', 'COUNTRY_NAME']\n",
    "    }\n",
    "    \n",
    "    for df, cols in zip([points_nhd, points_hydrobasins, points_admin], \n",
    "                       name_columns.values()):\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                location_names.update(df[col].dropna().unique())\n",
    "    \n",
    "    return location_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67f9c7-0ab0-450b-b060-0d50462acbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process OpenAlex records\n",
    "def process_openalex_records(openalex_df, location_names):\n",
    "    # Function to generate n-grams\n",
    "    def get_ngrams(text, n_range=(1, 3)):\n",
    "        if pd.isna(text):\n",
    "            return set()\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        all_ngrams = set()\n",
    "        \n",
    "        for n in range(n_range[0], n_range[1] + 1):\n",
    "            text_ngrams = set(' '.join(gram) for gram in ngrams(tokens, n))\n",
    "            all_ngrams.update(text_ngrams)\n",
    "            \n",
    "        return all_ngrams\n",
    "\n",
    "    # Convert location names to lowercase for matching\n",
    "    location_names = set(name.lower() for name in location_names)\n",
    "    \n",
    "    # Process each record\n",
    "    matched_records = []\n",
    "    \n",
    "    for _, record in openalex_df.iterrows():\n",
    "        # Combine all text fields\n",
    "        text_fields = [\n",
    "            str(record.get('title', '')),\n",
    "            str(record.get('abstract', '')),\n",
    "            str(record.get('keywords', ''))\n",
    "        ]\n",
    "        \n",
    "        combined_text = ' '.join(text_fields).lower()\n",
    "        \n",
    "        # Generate n-grams from the combined text\n",
    "        record_ngrams = get_ngrams(' '.join(text_fields))\n",
    "        \n",
    "        # Check for matches\n",
    "        if any(location in combined_text for location in location_names) or \\\n",
    "           any(location in record_ngrams for location in location_names):\n",
    "            matched_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(matched_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41e85c-cc5c-46e8-8a3e-28a3034fd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Load your data (replace with actual file paths)\n",
    "    points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf = load_spatial_data(\n",
    "        'points.csv',\n",
    "        'nhd.shp',\n",
    "        'hydrobasins.shp',\n",
    "        'admin_units.shp'\n",
    "    )\n",
    "    \n",
    "    # Get catchment information\n",
    "    location_names = get_catchment_info(points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf)\n",
    "    \n",
    "    # Load and process OpenAlex records\n",
    "    openalex_df = pd.read_csv('openalex_records.csv')  # Replace with actual file path\n",
    "    \n",
    "    # Filter records based on location names\n",
    "    matched_records = process_openalex_records(openalex_df, location_names)\n",
    "    \n",
    "    # Save results\n",
    "    matched_records.to_csv('matched_records.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
