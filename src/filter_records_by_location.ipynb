{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c0545b1-2abe-4e10-ad2b-987cc7c6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_geographic_refs.ipynb\n",
    "%run set_up.py \n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b9edc0a-cc1e-4f28-9aa3-1be09ea46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of NHD HUC12\n",
    "def create_huc12_list(wbd_path, \n",
    "                      out_hu12_parquet,\n",
    "                      verbose=True):\n",
    "    if not os.path.exists(out_hu12_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of HUC 12 and saving it to \\\n",
    "            {out_hu12_parquet}')\n",
    "        wbdhu12_list = gpd.read_file(filename=wbd_path, \n",
    "                                     layer='WBDHU12', \n",
    "                                     rows=105000,\n",
    "                                     columns=['huc12'],\n",
    "                                     ignore_geometry=True)\n",
    "        wbdhu12_list.to_parquet(out_hu12_parquet)\n",
    "    else:\n",
    "        wbdhu12_list = pd.read_parquet(out_hu12_parquet)\n",
    "    return(wbdhu12_list)\n",
    "\n",
    "wbd_path = os.path.join(nhd_dir, 'WBD_National_GDB.gdb')\n",
    "hu12_parquet = os.path.join(nhd_dir, 'wbd_hu12list.parquet')\n",
    "hu12_list = create_huc12_list(wbd_path, hu12_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f92e4a12-8af2-4549-9500-e7d3131fdfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a list of PFAF ID level 11 and saving it to             D:\\WWF_SBTN\\BTT_analysis\\data\\hydroatlas\\basinatlas_lev11_idlist.parquet\n"
     ]
    }
   ],
   "source": [
    "#Create a list of PFAF_ID for basins level 11\n",
    "def create_basinatlas11_list(basinatlas_path, \n",
    "                             out_basinatlas11_parquet, \n",
    "                             verbose=True):\n",
    "    if not os.path.exists(out_basinatlas11_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of PFAF ID level 11 and saving it to \\\n",
    "            {out_basinatlas11_parquet}')\n",
    "            \n",
    "        basinatlas11_list = gpd.read_file(\n",
    "            filename=basinatlas_path, \n",
    "            layer='BasinATLAS_v10_lev11', \n",
    "            columns=['PFAF_ID'],\n",
    "            rows=1031785,\n",
    "            ignore_geometry=True).\\\n",
    "        rename(columns={\"PFAD_ID\": \"PFAF_ID11\"})\n",
    "        \n",
    "        basinatlas11_list.to_parquet(out_basinatlas11_parquet)\n",
    "    else:\n",
    "        basinatlas11_list = pd.read_parquet(out_basinatlas11_parquet)\n",
    "    return(basinatlas11_list)\n",
    "\n",
    "basinatlas_path = os.path.join(\n",
    "    hydroatlas_dir, \n",
    "    'BasinATLAS_v10.gdb')\n",
    "basinatlas11_parquet = os.path.join(\n",
    "    hydroatlas_dir, \n",
    "    'basinatlas_lev11_idlist.parquet')\n",
    "basinatlas11_list = create_basinatlas11_list(\n",
    "    basinatlas_path, \n",
    "    basinatlas11_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d87a9b2-9181-4c1c-bf5c-b28cf3ab190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               huc12 huc2  huc4    huc6      huc8       huc10\n",
      "0       070200090402   07  0702  070200  07020009  0702000904\n",
      "1       070200030503   07  0702  070200  07020003  0702000305\n",
      "2       070200030602   07  0702  070200  07020003  0702000306\n",
      "6       070200030701   07  0702  070200  07020003  0702000307\n",
      "7       070300040804   07  0703  070300  07030004  0703000408\n",
      "...              ...  ...   ...     ...       ...         ...\n",
      "101242  071200010601   07  0712  071200  07120001  0712000106\n",
      "101243  071200011301   07  0712  071200  07120001  0712000113\n",
      "101244  071200010501   07  0712  071200  07120001  0712000105\n",
      "101246  071200011002   07  0712  071200  07120001  0712000110\n",
      "101248  071200030306   07  0712  071200  07120003  0712000303\n",
      "\n",
      "[5152 rows x 6 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "517ef609-07c4-4b50-b632-33960e215a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               huc12 huc2  huc4    huc6      huc8       huc10\n",
      "0       070200090402   07  0702  070200  07020009  0702000904\n",
      "1       070200030503   07  0702  070200  07020003  0702000305\n",
      "2       070200030602   07  0702  070200  07020003  0702000306\n",
      "6       070200030701   07  0702  070200  07020003  0702000307\n",
      "7       070300040804   07  0703  070300  07030004  0703000408\n",
      "...              ...  ...   ...     ...       ...         ...\n",
      "101242  071200010601   07  0712  071200  07120001  0712000106\n",
      "101243  071200011301   07  0712  071200  07120001  0712000113\n",
      "101244  071200010501   07  0712  071200  07120001  0712000105\n",
      "101246  071200011002   07  0712  071200  07120001  0712000110\n",
      "101248  071200030306   07  0712  071200  07120003  0712000303\n",
      "\n",
      "[8704 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your spatial data\n",
    "\n",
    "#point locations\n",
    "#geographic coordinate table\n",
    "#huc number\n",
    "\n",
    "\n",
    "def _subset_expand_huclist(in_id_list, in_refids_parquet):\n",
    "    in_huc_len = len(in_id_list[0])\n",
    "    huc_all_pd = pd.read_parquet(in_refids_parquet)\n",
    "    \n",
    "    huc_pd = huc_all_pd[\n",
    "    huc_all_pd['huc12'].str[:(in_huc_len)].isin(in_id_list)\n",
    "    ].copy()\n",
    "    \n",
    "    for huc_level in huc_range:\n",
    "        if f'huc{huc_level}' not in huc_pd:\n",
    "            huc_pd.loc[:, f'huc{huc_level}'] = huc_pd['huc12'].str[:(huc_level)]\n",
    "    return(huc_pd)\n",
    "\n",
    "def get_matching_NHD_HU(in_wbd_path,\n",
    "                        in_points=None, lon_col=None, lat_col=None,\n",
    "                        in_polygons=None, \n",
    "                        in_id_list=None, \n",
    "                        in_refids_parquet=None,\n",
    "                        hull=True, sjoin_predicate='intersects'):\n",
    "\n",
    "    #If a list of HUCs are provided --------------------------------------------\n",
    "    if in_id_list and in_refids_parquet:\n",
    "        huc_range = range(2, 14, 2)\n",
    "        if (not isinstance(in_id_list, 'str')) or (len(in_id_list) in huc_range) :\n",
    "            raise TypeError(\"in_id_list argument must be a string of even number \\\n",
    "            of digits between 2 and 12\")\n",
    "        huc_pd = _subset_expand_huclist(in_id_list, in_refids_parquet)\n",
    "\n",
    "    #If points or polygons are provided ----------------------------------------\n",
    "    elif in_points or in_polygons:\n",
    "        #Read points\n",
    "        if in_points:\n",
    "            points_ext = os.path.splitext(in_points)[1]\n",
    "            if points_ext =='.csv':\n",
    "                points_df = pd.read_csv(point_locations_path)\n",
    "                gdf_to_join = gpd.GeoDataFrame(\n",
    "                    points_df,\n",
    "                    geometry=[Point(xy) for xy \n",
    "                              in zip(points_df[lon_col], points_df[lat_col])]\n",
    "                )\n",
    "            elif points_ext in ['.gpkg', '.shp', '.gdb']:\n",
    "                gdf_to_join = gpd.read_file(in_points)\n",
    "            else:\n",
    "                raise TypeError(\"in_points type not recognized: can be .csv, .gpkg, .shp, or .gdb\")\n",
    "                \n",
    "        #Read polygons\n",
    "        if in_polygons:\n",
    "            gdf_to_join = gpd.read_file(in_polygons)\n",
    "    \n",
    "        if hull:\n",
    "            # Create convex hull using union_all() (current recommended method)\n",
    "            gdf_to_join = gpd.GeoDataFrame(\n",
    "                geometry=[gdf_to_join.geometry.union_all().convex_hull], \n",
    "                crs=gdf_to_join.crs\n",
    "            )\n",
    "            \n",
    "        #Reach NHD WBD\n",
    "        wbdhu6 = gpd.read_file(filename=in_wbd_path, \n",
    "                               layer='WBDHU6',\n",
    "                               columns=['huc6']\n",
    "                              )\n",
    "    \n",
    "        #Spatially join to hydrologic units\n",
    "        points_nhd = gpd.sjoin(gdf_to_join.to_crs(crs=wbdhu6.crs), \n",
    "                               wbdhu6, \n",
    "                               how='left', \n",
    "                               predicate=sjoin_predicate)\n",
    "        huc_pd = _subset_expand_huclist(\n",
    "             points_nhd.huc6.values.tolist(), \n",
    "            in_refids_parquet)\n",
    "\n",
    "    return(huc_pd)\n",
    "\n",
    "test_pts_path = os.path.join(datdir, 'test_gages', 'test_gages.shp')\n",
    "test_huc_pd = get_matching_NHD_HU(\n",
    "    in_wbd_path=wbd_path,\n",
    "    in_points=test_pts_path,\n",
    "    #lon_col=None, lat_col=None,\n",
    "    #in_polygons=None, \n",
    "    #in_id_list=None, \n",
    "    in_refids_parquet=hu12_parquet,\n",
    "    hull=True,\n",
    "    sjoin_predicate='intersects'\n",
    ")\n",
    "print(test_huc_pd)\n",
    "\n",
    "#in_id_list = in_umrb_huc4s = [f'07{str(i).zfill(2)}' for i in range(2,15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db12f04-2eb3-4a68-9c84-1bc7920825c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752ba54-3c3c-484a-bdb1-bc440a3096ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_hydrobasin(in_points=None, in_polygon=None, \n",
    "                            in_table=None, lon_col=None, lat_col=None,\n",
    "                            in_id=None, hull=True):\n",
    "\n",
    "\n",
    "def get_matching_geoglows_vpu(in_points=None, in_polygon=None, \n",
    "                              in_table=None, lon_col=None, lat_col=None,\n",
    "                              in_id=None, hull=True):\n",
    "    \n",
    "\n",
    "def load_spatial_data(point_locations_path, nhd_path, hydrobasins_path, admin_units_path):\n",
    "    # Convert your points to GeoDataFrame\n",
    "\n",
    "    \n",
    "    # Load boundary files\n",
    "    nhd_gdf = gpd.read_file(nhd_path)\n",
    "    hydrobasins_gdf = gpd.read_file(hydrobasins_path)\n",
    "    admin_gdf = gpd.read_file(admin_units_path)\n",
    "    \n",
    "    return points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f511f18-ad44-486c-8248-8e90ff4146e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Perform spatial joins to get catchment information\n",
    "def get_catchment_info(point_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf):\n",
    "    # Spatial joins\n",
    "    points_nhd = gpd.sjoin(point_gdf, nhd_gdf, how='left', op='within')\n",
    "    points_hydrobasins = gpd.sjoin(point_gdf, hydrobasins_gdf, how='left', op='within')\n",
    "    points_admin = gpd.sjoin(point_gdf, admin_gdf, how='left', op='within')\n",
    "    \n",
    "    # Collect all relevant names\n",
    "    location_names = set()\n",
    "    \n",
    "    # Add names from each source (adjust column names as needed)\n",
    "    name_columns = {\n",
    "        'nhd': ['BASIN_NAME', 'RIVER_NAME'],\n",
    "        'hydrobasins': ['HYBAS_NAME'],\n",
    "        'admin': ['COUNTY_NAME', 'STATE_NAME', 'COUNTRY_NAME']\n",
    "    }\n",
    "    \n",
    "    for df, cols in zip([points_nhd, points_hydrobasins, points_admin], \n",
    "                       name_columns.values()):\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                location_names.update(df[col].dropna().unique())\n",
    "    \n",
    "    return location_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67f9c7-0ab0-450b-b060-0d50462acbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process OpenAlex records\n",
    "def process_openalex_records(openalex_df, location_names):\n",
    "    # Function to generate n-grams\n",
    "    def get_ngrams(text, n_range=(1, 3)):\n",
    "        if pd.isna(text):\n",
    "            return set()\n",
    "        \n",
    "        text = str(text).lower()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        all_ngrams = set()\n",
    "        \n",
    "        for n in range(n_range[0], n_range[1] + 1):\n",
    "            text_ngrams = set(' '.join(gram) for gram in ngrams(tokens, n))\n",
    "            all_ngrams.update(text_ngrams)\n",
    "            \n",
    "        return all_ngrams\n",
    "\n",
    "    # Convert location names to lowercase for matching\n",
    "    location_names = set(name.lower() for name in location_names)\n",
    "    \n",
    "    # Process each record\n",
    "    matched_records = []\n",
    "    \n",
    "    for _, record in openalex_df.iterrows():\n",
    "        # Combine all text fields\n",
    "        text_fields = [\n",
    "            str(record.get('title', '')),\n",
    "            str(record.get('abstract', '')),\n",
    "            str(record.get('keywords', ''))\n",
    "        ]\n",
    "        \n",
    "        combined_text = ' '.join(text_fields).lower()\n",
    "        \n",
    "        # Generate n-grams from the combined text\n",
    "        record_ngrams = get_ngrams(' '.join(text_fields))\n",
    "        \n",
    "        # Check for matches\n",
    "        if any(location in combined_text for location in location_names) or \\\n",
    "           any(location in record_ngrams for location in location_names):\n",
    "            matched_records.append(record)\n",
    "    \n",
    "    return pd.DataFrame(matched_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41e85c-cc5c-46e8-8a3e-28a3034fd48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Load your data (replace with actual file paths)\n",
    "    points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf = load_spatial_data(\n",
    "        'points.csv',\n",
    "        'nhd.shp',\n",
    "        'hydrobasins.shp',\n",
    "        'admin_units.shp'\n",
    "    )\n",
    "    \n",
    "    # Get catchment information\n",
    "    location_names = get_catchment_info(points_gdf, nhd_gdf, hydrobasins_gdf, admin_gdf)\n",
    "    \n",
    "    # Load and process OpenAlex records\n",
    "    openalex_df = pd.read_csv('openalex_records.csv')  # Replace with actual file path\n",
    "    \n",
    "    # Filter records based on location names\n",
    "    matched_records = process_openalex_records(openalex_df, location_names)\n",
    "    \n",
    "    # Save results\n",
    "    matched_records.to_csv('matched_records.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
