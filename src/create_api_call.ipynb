{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99592c59-ac28-4801-b22c-16df6f0de8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\messa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run create_search_strings.ipynb\n",
    "%run set_up.py\n",
    "\n",
    "import pyalex #https://github.com/J535D165/pyalex\n",
    "from pyalex import config\n",
    "from pyalex import Works\n",
    "\n",
    "config.max_retries = 1\n",
    "pyalex.config.email = \"mathis.messager@mail.mcgill.ca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7737d4df-02b7-4ced-b397-efd5019d0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_dict = create_generic_search_terms()\n",
    "oalex_string_dict = {}\n",
    "for search_number, search_terms in search_terms_dict.items():\n",
    "    oalex_string_dict[search_number] = create_search_string(\n",
    "        search_terms, inflect=False, or_chars=' OR ', and_chars=' AND ',\n",
    "        inner_separators=[\" \", \"-\", \"\"],  use_quotes=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4d181d-ea17-4278-b4b5-26c4331e4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_url_segment(url):\n",
    "    \"\"\"\n",
    "    Extracts the last segment of a URL path.  Handles various URL formats\n",
    "    and potential errors robustly.\n",
    "\n",
    "    Args:\n",
    "        url: The URL string.\n",
    "\n",
    "    Returns:\n",
    "        The last segment of the URL path, or None if the URL is invalid\n",
    "        or has no path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        if not path:\n",
    "            return None  # No path component\n",
    "\n",
    "        # Split the path by '/' and get the last element\n",
    "        segments = path.split('/')\n",
    "        return segments[-1]  # Handle cases with trailing slashes correctly\n",
    "\n",
    "    except Exception:  # Catch any parsing errors\n",
    "        return None\n",
    "\n",
    "def extract_concept_from_url_df(df, url_col, include_col=None):\n",
    "    \"\"\"\n",
    "    Extracts the last segment of URLs from a specific column in a DataFrame,\n",
    "    filtering by a boolean column, and adds the result as a new column.\n",
    "\n",
    "    Args:\n",
    "      df: The Pandas DataFrame.\n",
    "      url_col: The name of the column containing URLs (string).\n",
    "      include_col: The name of the boolean column to filter by (string).\n",
    "\n",
    "    Returns:\n",
    "        A new Pandas DataFrame with an additional column 'openalex_id_last_segment'\n",
    "        containing the extracted last segment, or None if the input is invalid.\n",
    "    \"\"\"\n",
    "    # Input validation: Check for required columns\n",
    "    required_columns = [url_col]\n",
    "    if include_col is not None:\n",
    "        required_columns.append(include_col)\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"DataFrame is missing some columns.\")\n",
    "        return None\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if include_col is not None:\n",
    "        # Convert 'Include?' (or whatever include_col is) to boolean, handling various representations.\n",
    "        df_copy[include_col] = df_copy[include_col].astype(str).str.lower().isin(['y', 'yes', 'true', '1', 't'])\n",
    "        # Apply the extraction function ONLY to rows where 'include_col' is True,\n",
    "        # and ONLY to the 'url_col' of those rows. Use .loc for proper indexing.\n",
    "        included_clist = df_copy.loc[df_copy[include_col], url_col].apply(extract_last_url_segment)\n",
    "    else:\n",
    "        included_clist = df_copy.loc[:, url_col].apply(extract_last_url_segment)\n",
    "\n",
    "    return included_clist.tolist()\n",
    "\n",
    "#Get open alex concepts to filter with\n",
    "concepts_toinclude_pd = pd.read_csv(\n",
    "    os.path.join(datdir, 'openalex_concepts_toinclude.csv'))\n",
    "concepts_toinclude_list =  extract_concept_from_url_df(\n",
    "    df = concepts_toinclude_pd,\n",
    "    url_col = 'openalex_id', \n",
    "    include_col = 'include')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435b5d24-3723-46bc-8eb2-e3886726baaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving search1\n",
      "Retrieving search2\n",
      "Retrieving search3\n",
      "Retrieving search4\n",
      "Retrieving search5\n",
      "Retrieving search6\n",
      "[200, 200, 200, 200, 117, 200]\n"
     ]
    }
   ],
   "source": [
    "#Run on each search string\n",
    "oalex_records_dict = {}\n",
    "for search_number, search_terms in oalex_string_dict.items():\n",
    "    print(f'Retrieving {search_number}')\n",
    "    oa_query = Works().search_filter(title_and_abstract=search_terms).filter(\n",
    "        concept={\"id\": '|'.join(concepts_toinclude_list)},\n",
    "        is_retracted='False')\n",
    "\n",
    "    #print(oa_query.url)\n",
    "    \n",
    "    oalex_records_dict[search_number] = [\n",
    "        record for record in itertools.chain(\n",
    "        *oa_query.paginate(per_page=200, n_max=200)\n",
    "    )]\n",
    "\n",
    "print([len(rec_list) for rec_list in oalex_records_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "402426ad-c189-4e93-86db-1aa8e92b7a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search1\n",
      "200\n",
      "200\n",
      "search2\n",
      "200\n",
      "178\n",
      "search3\n",
      "200\n",
      "194\n",
      "search4\n",
      "200\n",
      "195\n",
      "search5\n",
      "117\n",
      "115\n",
      "search6\n",
      "200\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicates\n",
    "def remove_oalex_search_duplicates(records_dict):\n",
    "    seen_ids = set()\n",
    "    new_records_dict = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(search_number)\n",
    "        print(len(records_list))\n",
    "        new_records_list = []  # Create a new list for each search_number\n",
    "        for record in records_list:\n",
    "            rid = record['id']\n",
    "            if rid not in seen_ids:\n",
    "                new_records_list.append(record)\n",
    "                seen_ids.add(rid)\n",
    "        new_records_dict[search_number] = new_records_list\n",
    "        print(len(new_records_list))\n",
    "    return(new_records_dict)\n",
    "    \n",
    "oalex_records_dict = remove_oalex_search_duplicates(oalex_records_dict) # Replace the old dict\n",
    "\n",
    "# #Serialize\n",
    "# #All results from PyAlex can be serialized. For example, save the results to a JSON file:\n",
    "# import json\n",
    "# from pathlib import Path\n",
    "# from pyalex import Work\n",
    "\n",
    "# with open(Path(\"works.json\"), \"w\") as f:\n",
    "#     json.dump(Works().get(), f)\n",
    "\n",
    "# # with open(Path(\"works.json\")) as f:\n",
    "# #     works = [Work(w) for w in json.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891a3b50-341f-419e-aa2b-64518eb47701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_parentheses(in_str):\n",
    "    #Remove unnecessary parentheses *if* they enclose the entire expression:\n",
    "    if in_str.startswith('(') and in_str.endswith(')'):\n",
    "        # Check if they are *actually* unnecessary (i.e., not (a|b) AND (c|d))\n",
    "        open_count = 0\n",
    "        unnecessary = True\n",
    "        while unnecessary:\n",
    "            for i, char in enumerate(in_str):\n",
    "                if char == '(':\n",
    "                    open_count += 1\n",
    "                elif char == ')':\n",
    "                    open_count -= 1\n",
    "                if open_count == 0 and i < len(in_str) - 1:\n",
    "                    unnecessary = False\n",
    "                    break\n",
    "            if unnecessary:\n",
    "                in_str = in_str[1:-1]\n",
    "        return(in_str)\n",
    "    \n",
    "#Generate regex to filter OpenAlex after the search\n",
    "#to make up for lemmatization performed by Open Alex\n",
    "post_oalex_regex_dict = {}\n",
    "for search_number, search_terms in search_terms_dict.items():\n",
    "    #Generated an initial filter to be adjusted \n",
    "    #(split it in two regex queries for the AND rather than using greedy lookaheads\n",
    "    post_oalex_regex_dict[search_number] = create_search_string(\n",
    "        search_terms, inflect=True, or_chars='|', and_chars='AND',\n",
    "        inner_separators=[r\"[-\\s]*\"], use_quotes=False)\n",
    "    #Remove redundant parentheses and split for nested regex filters\n",
    "    post_oalex_regex_dict[search_number] = remove_redundant_parentheses(\n",
    "        post_oalex_regex_dict[search_number]).split('AND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48c99ca5-c4ae-41c2-bd7b-46313c648f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search1\n",
      "200\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Pre-compile the regex patterns:\u001b[39;00m\n\u001b[32m     22\u001b[39m post_oalex_regex_compiled_dict = {\n\u001b[32m     23\u001b[39m     search_number: [re.compile(pattern) \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns]\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m search_number, patterns \u001b[38;5;129;01min\u001b[39;00m post_oalex_regex_dict.items()\n\u001b[32m     25\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m oalex_records_dict_filtered = \u001b[43mfilter_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecords_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moalex_records_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mregex_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpost_oalex_regex_compiled_dict\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mfilter_records\u001b[39m\u001b[34m(records_dict, regex_dict)\u001b[39m\n\u001b[32m      7\u001b[39m new_records_list = []  \u001b[38;5;66;03m# Create a new list for each search_number\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records_list:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     str_to_check = \u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mabstract\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m         \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdisplay_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkeywords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m#Make sure that the regex matches with something in the title, abstract\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m#or keywords\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(re_pattern.search(str_to_check) \n\u001b[32m     16\u001b[39m            \u001b[38;5;28;01mfor\u001b[39;00m re_pattern \u001b[38;5;129;01min\u001b[39;00m regex_dict[search_number]):\n",
      "\u001b[31mTypeError\u001b[39m: sequence item 1: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "def filter_records(records_dict, regex_dict):\n",
    "    new_records_dict = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(search_number)\n",
    "        print(len(records_list))\n",
    "        if len(records_list) > 0:\n",
    "            new_records_list = []  # Create a new list for each search_number\n",
    "            for record in records_list:\n",
    "                str_to_check = ' '.join(\n",
    "                    ([record['title'], record['abstract']] \n",
    "                     + [kw['display_name'] for kw in record['keywords']])\n",
    "                )\n",
    "                #Make sure that the regex matches with something in the title, abstract\n",
    "                #or keywords\n",
    "                if all(re_pattern.search(str_to_check) \n",
    "                       for re_pattern in regex_dict[search_number]):\n",
    "                    new_records_list.append(record)\n",
    "            print(len(new_records_list))\n",
    "            new_records_dict[search_number] = new_records_list\n",
    "\n",
    "# Pre-compile the regex patterns:\n",
    "post_oalex_regex_compiled_dict = {\n",
    "    search_number: [re.compile(pattern) for pattern in patterns]\n",
    "    for search_number, patterns in post_oalex_regex_dict.items()\n",
    "}\n",
    "\n",
    "oalex_records_dict_filtered = filter_records(\n",
    "    records_dict=oalex_records_dict,\n",
    "    regex_dict=post_oalex_regex_compiled_dict\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8057e210-9610-49a0-abfc-12473bad6ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'doi', 'title', 'display_name', 'relevance_score', 'publication_year', 'publication_date', 'ids', 'language', 'primary_location', 'type', 'type_crossref', 'indexed_in', 'open_access', 'authorships', 'institution_assertions', 'countries_distinct_count', 'institutions_distinct_count', 'corresponding_author_ids', 'corresponding_institution_ids', 'apc_list', 'apc_paid', 'fwci', 'has_fulltext', 'fulltext_origin', 'cited_by_count', 'citation_normalized_percentile', 'cited_by_percentile_year', 'biblio', 'is_retracted', 'is_paratext', 'primary_topic', 'topics', 'keywords', 'concepts', 'mesh', 'locations_count', 'locations', 'best_oa_location', 'sustainable_development_goals', 'grants', 'datasets', 'versions', 'referenced_works_count', 'referenced_works', 'related_works', 'abstract_inverted_index', 'abstract_inverted_index_v3', 'cited_by_api_url', 'counts_by_year', 'updated_date', 'created_date'])\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thermal regime of a headwater stream within a clear-cut, coastal British Columbia, Canada This study examined the thermal regime of a headwater stream within a clear-cut. The stream had a complex morphology dominated by step–pool features, many formed by sediment accumulation upstream of woody debris. Maximum daily temperatures increased up to 5 °C after logging, and were positively associated with maximum daily air temperature and negatively with discharge. Maximum daily temperatures generally increased with downstream distance through the cut block, but decreased with distance in two segments over distances of tens of metres, where the topography indicated relatively concentrated lateral inflow. Localized cool areas within a step–pool unit were associated with zones of concentrated upwelling. Bed temperatures tended to be higher and have greater ranges in areas of downwelling flow into the bed. Heat budget estimates were made using meteorological measurements over the water surface and a model of net radiation using canopy characteristics derived from fisheye photography. Heat exchange driven by hyporheic flow through the channel step was a cooling effect during daytime, with a magnitude up to approximately 25% that of net radiation during the period of maximum daytime warming. Heat budget calculations in these headwater streams are complicated by the heterogeneity of incident solar radiation and channel geometry, as well as uncertainty in estimating heat and water exchanges between the stream and the subsurface via hyporheic exchange and heat conduction. Copyright © 2005 John Wiley & Sons, Ltd. Downwelling Shortwave radiation Microclimate Inflow'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record = record_list[4000]\n",
    "# print(record.keys())\n",
    "# print(record['is_retracted'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e1159-6904-464f-9956-276c3523b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "~~~~~ Search for works in OpenAlex based on search string ~~~~~~~~~~~~~~~~~~~~~~\n",
    "Reference info on the API: \n",
    "# https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/search-entities\n",
    "# https://docs.openalex.org/api-entities/works/search-works\n",
    "\n",
    "#EX: https://api.openalex.org/works?search=(elmo AND \"sesame street\") NOT (cookie OR monster)\n",
    "#Filter categories based on csv\n",
    "#do not lemmatize\n",
    "#&per-page=100&cursor=*\n",
    "\n",
    "#~~~~~~~~~~~~~~~~ PAGING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Basic paging only works to get the first 10,000 results of any list. If you want to see more than 10,000 results, you'll need to use cursor paging.\n",
    "To use cursor paging, you request a cursor by adding the cursor=* parameter-value pair to your query.\n",
    "    Get a cursor in order to start cursor pagination:\n",
    "    https://api.openalex.org/works?filter=publication_year:2020&per-page=100&cursor=*\n",
    "The response to your query will include a next_cursor value in the response's meta object. Here's what it looks like:\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"count\": 8695857,\n",
    "    \"db_response_time_ms\": 28,\n",
    "    \"page\": null,\n",
    "    \"per_page\": 100,\n",
    "    \"next_cursor\": \"IlsxNjA5MzcyODAwMDAwLCAnaHR0cHM6Ly9vcGVuYWxleC5vcmcvVzI0ODg0OTk3NjQnXSI=\"\n",
    "  },\n",
    "  \"results\" : [\n",
    "    // the first page of results\n",
    "  ]\n",
    "}\n",
    "\n",
    "To retrieve the next page of results, copy the meta.next_cursor value into the cursor field of your next request.\n",
    "\n",
    "    Get the next page of results using a cursor value:\n",
    "    https://api.openalex.org/works?filter=publication_year:2020&per-page=100&cursor=IlsxNjA5MzcyODAwMDAwLCAnaHR0cHM6Ly9vcGVuYWxleC5vcmcvVzI0ODg0OTk3NjQnXSI=\n",
    "\n",
    "To get all the results, keep repeating this process until meta.next_cursor is null and the results set is empty.\n",
    "'''\n",
    "\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
