{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99592c59-ac28-4801-b22c-16df6f0de8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\messa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run set_up.py\n",
    "%run lit_utility_functions_2025.ipynb\n",
    "%run create_search_strings.ipynb\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, UTC\n",
    "import pyalex #https://github.com/J535D165/pyalex\n",
    "from pyalex import config, Works\n",
    "from typing import Any, Dict, List, Pattern, Tuple, Union\n",
    "\n",
    "config.max_retries = 1\n",
    "config.email = \"mathis.messager@mail.mcgill.ca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7737d4df-02b7-4ced-b397-efd5019d0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms_dict = create_generic_search_terms()\n",
    "oalex_string_dict = {}\n",
    "for search_number, search_terms in search_terms_dict.items():\n",
    "    oalex_string_dict[search_number] = create_search_string(\n",
    "        search_terms, inflect=False, or_chars=' OR ', and_chars=' AND ',\n",
    "        inner_separators=[\" \", \"-\", \"\"],  use_quotes=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4d181d-ea17-4278-b4b5-26c4331e4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_url_segment(url):\n",
    "    \"\"\"\n",
    "    Extracts the last segment of a URL path. \n",
    "\n",
    "    Args:\n",
    "        url: The URL string.\n",
    "\n",
    "    Returns:\n",
    "        The last segment of the URL path, or None if the URL is invalid\n",
    "        or has no path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        if not path:\n",
    "            return None  # No path component\n",
    "\n",
    "        # Split the path by '/' and get the last element\n",
    "        segments = path.split('/')\n",
    "        return segments[-1]  # Handle cases with trailing slashes correctly\n",
    "\n",
    "    except Exception:  # Catch any parsing errors\n",
    "        return None\n",
    "\n",
    "def extract_concept_from_url_df(df, url_col, include_col=None):\n",
    "    \"\"\"\n",
    "    Extracts the last segment of URLs from a specific column in a DataFrame,\n",
    "    filtering by a boolean column, and adds the result as a new column.\n",
    "\n",
    "    Args:\n",
    "      df: The Pandas DataFrame.\n",
    "      url_col: The name of the column containing URLs (string).\n",
    "      include_col: The name of the boolean column to filter by (string).\n",
    "\n",
    "    Returns:\n",
    "        A new Pandas DataFrame with an additional column 'openalex_id_last_segment'\n",
    "        containing the extracted last segment, or None if the input is invalid.\n",
    "    \"\"\"\n",
    "    # Input validation: Check for required columns\n",
    "    required_columns = [url_col]\n",
    "    if include_col is not None:\n",
    "        required_columns.append(include_col)\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"DataFrame is missing some columns.\")\n",
    "        return None\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if include_col is not None:\n",
    "        # Convert 'Include?' (or whatever include_col is) to boolean, handling various representations.\n",
    "        df_copy[include_col] = df_copy[include_col].astype(str).str.lower().isin(['y', 'yes', 'true', '1', 't'])\n",
    "        # Apply the extraction function ONLY to rows where 'include_col' is True,\n",
    "        # and ONLY to the 'url_col' of those rows. Use .loc for proper indexing.\n",
    "        included_clist = df_copy.loc[df_copy[include_col], url_col].apply(extract_last_url_segment)\n",
    "    else:\n",
    "        included_clist = df_copy.loc[:, url_col].apply(extract_last_url_segment)\n",
    "\n",
    "    return included_clist.tolist()\n",
    "\n",
    "#Get open alex concepts to filter with\n",
    "concepts_toinclude_pd = pd.read_csv(\n",
    "    os.path.join(datdir, 'openalex_concepts_toinclude.csv'))\n",
    "concepts_toinclude_list =  extract_concept_from_url_df(\n",
    "    df = concepts_toinclude_pd,\n",
    "    url_col = 'openalex_id', \n",
    "    include_col = 'include')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a432b2f2-1635-46b1-87d9-37e8966e5caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get toponyms to filter with\n",
    "target_toponyms_pkl = regex_list_files(in_dir=resdir, \n",
    "                       in_pattern='target_toponyms_umrb_.*'\n",
    "                      )[-1]\n",
    "\n",
    "with open(target_toponyms_pkl, 'rb') as f:\n",
    "    target_toponyms_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "435b5d24-3723-46bc-8eb2-e3886726baaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving search1\n",
      "Retrieving search2\n",
      "Retrieving search3\n",
      "Retrieving search4\n",
      "Retrieving search5\n",
      "Retrieving search6\n",
      "[200, 200, 200, 200, 117, 200]\n"
     ]
    }
   ],
   "source": [
    "#Run on each search string\n",
    "def retrieve_oalex_records_dict(title_and_abstract_string_dict, \n",
    "                                concepts_list=None, \n",
    "                                fulltext_string_dict=None):\n",
    "    out_records_dict = {}\n",
    "    for search_number, search_terms in title_and_abstract_string_dict.items():\n",
    "        print(f'Retrieving {search_number}')\n",
    "        oa_query = Works().\\\n",
    "        search_filter(title_and_abstract=search_terms).\\\n",
    "        filter(is_retracted='False')\n",
    "\n",
    "        if concepts_list:\n",
    "            oa_query = oa_query.filter(concept={\"id\": '|'.join(concepts_list)})\n",
    "        #if\n",
    "    \n",
    "        #print(oa_query.url)\n",
    "        out_records_dict[search_number] = [\n",
    "            record for record in itertools.chain(\n",
    "            *oa_query.paginate(per_page=200, n_max=200)\n",
    "        )]\n",
    "    return(out_records_dict)\n",
    "\n",
    "oalex_records_dict = retrieve_oalex_records_dict(\n",
    "    title_and_abstract_string_dict=oalex_string_dict, \n",
    "    concepts_list=concepts_toinclude_list\n",
    ")\n",
    "\n",
    "print([len(rec_list) for rec_list in oalex_records_dict.values() if rec_list])\n",
    "\n",
    "\n",
    "#https://api.openalex.org/works?filter=fulltext.search:climate%20change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402426ad-c189-4e93-86db-1aa8e92b7a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search1\n",
      "200\n",
      "200\n",
      "search2\n",
      "200\n",
      "178\n",
      "search3\n",
      "200\n",
      "194\n",
      "search4\n",
      "200\n",
      "197\n",
      "search5\n",
      "116\n",
      "115\n",
      "search6\n",
      "200\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicates\n",
    "def remove_oalex_search_duplicates(records_dict):\n",
    "    seen_ids = set()\n",
    "    new_records_dict = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(search_number)\n",
    "        print(len(records_list))\n",
    "        new_records_list = []  # Create a new list for each search_number\n",
    "        for record in records_list:\n",
    "            rid = record['id']\n",
    "            if rid not in seen_ids:\n",
    "                new_records_list.append(record)\n",
    "                seen_ids.add(rid)\n",
    "        new_records_dict[search_number] = new_records_list\n",
    "        print(len(new_records_list))\n",
    "    return(new_records_dict)\n",
    "    \n",
    "oalex_records_dict = remove_oalex_search_duplicates(oalex_records_dict) # Replace the old dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891a3b50-341f-419e-aa2b-64518eb47701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_parentheses(in_str):\n",
    "    #Remove unnecessary parentheses *if* they enclose the entire expression:\n",
    "    if in_str.startswith('(') and in_str.endswith(')'):\n",
    "        # Check if they are *actually* unnecessary (i.e., not (a|b) AND (c|d))\n",
    "        open_count = 0\n",
    "        unnecessary = True\n",
    "        while unnecessary:\n",
    "            for i, char in enumerate(in_str):\n",
    "                if char == '(':\n",
    "                    open_count += 1\n",
    "                elif char == ')':\n",
    "                    open_count -= 1\n",
    "                if open_count == 0 and i < len(in_str) - 1:\n",
    "                    unnecessary = False\n",
    "                    break\n",
    "            if unnecessary:\n",
    "                in_str = in_str[1:-1]\n",
    "        return(in_str)\n",
    "    \n",
    "#Generate regex to filter OpenAlex after the search\n",
    "#to make up for lemmatization performed by Open Alex\n",
    "post_oalex_regex_dict = {}\n",
    "for search_number, search_terms in search_terms_dict.items():\n",
    "    #Generated an initial filter to be adjusted \n",
    "    #(split it in two regex queries for the AND rather than using greedy lookaheads\n",
    "    post_oalex_regex_dict[search_number] = create_search_string(\n",
    "        search_terms, inflect=True, or_chars='|', and_chars='AND',\n",
    "        inner_separators=[r\"[-\\s]*\"], use_quotes=False)\n",
    "    #Remove redundant parentheses and split for nested regex filters\n",
    "    post_oalex_regex_dict[search_number] = remove_redundant_parentheses(\n",
    "        post_oalex_regex_dict[search_number]).split('AND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "714b3b28-14ee-48e2-aa7f-fa08e7aa6603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['((restoratives)|(compensation)|(eco[-\\\\s]*hydrologic)|(minimal)|(augmented)|(maintenance)|(acceptable)|(experimental)|(minimus)|(in[-\\\\s]*stream)|(ecologically)|(minima)|(minimuss)|(eco[-\\\\s]*hydrologically)|(minimum)|(restorative)|(restorations)|(hydro[-\\\\s]*ecologically)|(restoration)|(optimum)|(ecological)|(maintenances)|(compensations)|(augmentations)|(optima)|(environmental)|(hydro[-\\\\s]*ecological)|(eco[-\\\\s]*hydrological)|(ecologic)|(hydro[-\\\\s]*ecologic)|(flushing)|(augmentation))',\n",
       " '((discharging)|(flooding)|(discharges)|(floods)|(discharge)|(flowing)|(flow)|(flood)|(water[-\\\\s]*level)|(flows))']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_oalex_regex_dict['search1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c99ca5-c4ae-41c2-bd7b-46313c648f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing search number: search1\n",
      "Initial number of records: 200\n",
      "Number of records after filtering: 126\n",
      "Processing search number: search2\n",
      "Initial number of records: 178\n",
      "Number of records after filtering: 154\n",
      "Processing search number: search3\n",
      "Initial number of records: 194\n",
      "Number of records after filtering: 8\n",
      "Processing search number: search4\n",
      "Initial number of records: 197\n",
      "Number of records after filtering: 117\n",
      "Processing search number: search5\n",
      "Initial number of records: 115\n",
      "Number of records after filtering: 102\n",
      "Processing search number: search6\n",
      "Initial number of records: 190\n",
      "Number of records after filtering: 168\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Record:\n",
    "    \"\"\"Represents a scientific record from open \"\"\"\n",
    "    title: str\n",
    "    abstract: str\n",
    "    keywords: List[Dict[str, str]]\n",
    "    \n",
    "def _matches_all_patterns(record: Record, patterns: List[Pattern]) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a record matches all given regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        record: The record (dictionary) to check\n",
    "        patterns: List of compiled regex patterns\n",
    "    \n",
    "    Returns:\n",
    "        True if the record matches all patterns, False otherwise\n",
    "    \"\"\"\n",
    "    #Create a single string from all record information to filter\n",
    "    searchable_text = ' '.join([\n",
    "        str(record['title']),\n",
    "        str(record['abstract']),\n",
    "        *[str(kw['display_name']) for kw in record['keywords']]\n",
    "    ])\n",
    "\n",
    "    #the all-for loop combination enables to implement the AND between two regex patterns\n",
    "    match_bool = all(\n",
    "        pattern.search(searchable_text) \n",
    "        for pattern in patterns\n",
    "    )\n",
    "    \n",
    "    return(match_bool)\n",
    "\n",
    "def filter_records(\n",
    "    records_dict: Dict[str, List[Record]], \n",
    "    regex_dict: Dict[str, List[Pattern]],\n",
    ") -> Dict[str, List[Record]]:\n",
    "    \"\"\"\n",
    "    Filters records based on regex patterns matching in title, abstract, or keywords.\n",
    "    \n",
    "    Args:\n",
    "        records_dict: Dictionary mapping search numbers to lists of records\n",
    "        regex_dict: Dictionary mapping search numbers to lists of compiled regex patterns\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing filtered records that match all regex patterns\n",
    "        for their respective search number\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_records = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(f\"Processing search number: {search_number}\")\n",
    "        print(f\"Initial number of records: {len(records_list)}\")\n",
    "\n",
    "        if not records_list:\n",
    "            continue\n",
    "\n",
    "        matching_records = [\n",
    "            record for record in records_list\n",
    "            if _matches_all_patterns(record, regex_dict[search_number])\n",
    "        ]\n",
    "\n",
    "        print(f\"Number of records after filtering: {len(matching_records)}\")\n",
    "        filtered_records[search_number] = matching_records\n",
    "\n",
    "    return(filtered_records)\n",
    "\n",
    "# Pre-compile the regex patterns:\n",
    "post_oalex_regex_compiled_dict = {\n",
    "    search_number: [re.compile(pattern) for pattern in patterns]\n",
    "    for search_number, patterns in post_oalex_regex_dict.items()\n",
    "}\n",
    "\n",
    "oalex_records_dict_filtered = filter_records(\n",
    "    records_dict=oalex_records_dict,\n",
    "    regex_dict=post_oalex_regex_compiled_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "552dddda-919d-4fd4-ad5b-9be3b89bc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _matches_patterns(\n",
    "    record: Dict[str, List[Any]], \n",
    "    patterns: List[re.Pattern], \n",
    "    match_all: bool = True, \n",
    "    include_ngrams: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a record matches regex patterns, \n",
    "    with options for 'all' or 'any' matching and including ngrams.\n",
    "\n",
    "    Args:\n",
    "        record: The record to check.\n",
    "        patterns: List of compiled regex patterns.\n",
    "        match_all: If True, all patterns must match.  If False, at least one must match.\n",
    "        include_ngrams: If True, include ngrams in the searchable text.\n",
    "\n",
    "    Returns:\n",
    "        True if the record matches the patterns according to the conditions, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(patterns, list):\n",
    "        raise TypeError(\"patterns must be a list of compiled regex patterns.\")\n",
    "    if not all(isinstance(p, re.Pattern) for p in patterns):\n",
    "         raise TypeError(\"patterns must be a list of compiled regex patterns.\")\n",
    "    if not isinstance(match_all, bool):\n",
    "        raise TypeError(\"match_all must be a boolean.\")\n",
    "    if not isinstance(include_ngrams, bool):\n",
    "        raise TypeError(\"include_ngrams must be a boolean.\")\n",
    "\n",
    "    searchable_text_parts = [\n",
    "        str(record['title']),\n",
    "        str(record['abstract']),\n",
    "        *[str(kw['display_name']) for kw in record['keywords']]\n",
    "    ]\n",
    "\n",
    "    if include_ngrams:\n",
    "        searchable_text_parts.extend(record.ngrams())  # Conditionally include ngrams\n",
    "\n",
    "    searchable_text = \" \".join(searchable_text_parts)\n",
    "\n",
    "    if match_all:\n",
    "        match_bool = all(pattern.search(searchable_text) for pattern in patterns)\n",
    "    else:\n",
    "        match_bool = any(pattern.search(searchable_text) for pattern in patterns)\n",
    "\n",
    "    return match_bool\n",
    "\n",
    "\n",
    "def filter_records(\n",
    "    records_dict: Dict[str, List[Dict]],\n",
    "    regex_dict: Dict[str, List[re.Pattern]],\n",
    "    match_all: bool = True,\n",
    "    include_ngrams: bool = False,\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Filters records based on regex patterns, with options for n-gram inclusion.\n",
    "\n",
    "    Args:\n",
    "        records_dict: Dictionary mapping search numbers to lists of raw OpenAlex records (dictionaries).\n",
    "        regex_dict: Dictionary mapping search numbers to lists of compiled regex patterns.\n",
    "        match_all: If True, all patterns must match. If False, at least one must match.\n",
    "        include_ngrams: If True, include ngrams in the searchable text for matching.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing filtered records (as Record objects) that match the\n",
    "        regex patterns for their respective search number, according to the conditions.\n",
    "    \"\"\"\n",
    "    if not isinstance(records_dict, dict):\n",
    "        raise TypeError(\"records_dict must be a dictionary.\")\n",
    "    if not isinstance(regex_dict, dict):\n",
    "        raise TypeError(\"regex_dict must be a dictionary.\")\n",
    "    if not isinstance(match_all, bool):\n",
    "        raise TypeError(\"match_all must be a boolean.\")\n",
    "    if not isinstance(include_ngrams, bool):\n",
    "         raise TypeError(\"include_ngrams must be a boolean.\")\n",
    "\n",
    "     # Check if keys in records_dict and regex_dict match\n",
    "    if records_dict.keys() != regex_dict.keys():\n",
    "        raise ValueError(\"Keys in records_dict and regex_dict must be identical.\")\n",
    "\n",
    "    #check if values are lists in dict\n",
    "    if not all(isinstance(val, list) for val in regex_dict.values()):\n",
    "          raise TypeError(\"Values of regex_dict must be lists.\")\n",
    "\n",
    "    filtered_records = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(f\"Processing search number: {search_number}\")\n",
    "        print(f\"Initial number of records: {len(records_list)}\")\n",
    "\n",
    "        if not records_list:\n",
    "            print(f\"No records for search number {search_number}, skipping.\")\n",
    "            filtered_records[search_number] = []  # Consistent return type\n",
    "            continue\n",
    "\n",
    "        # Convert raw OpenAlex records (dictionaries) to Record objects *and* extract ngrams\n",
    "        matching_records = [\n",
    "            record \n",
    "            for record in records_list\n",
    "            if _matches_patterns(record, regex_dict[search_number],\n",
    "                                 match_all, include_ngrams)\n",
    "        ]\n",
    "\n",
    "        print(f\"Number of records after filtering: {len(matching_records)}\")\n",
    "        filtered_records[search_number] = matching_records\n",
    "\n",
    "    return filtered_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61162241-0508-43f5-b416-54b42da0b78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing search number: search1\n",
      "Initial number of records: 200\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: NOT FOUND for url: https://api.openalex.org/works/W2766361688/ngrams",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m post_oalex_regex_compiled_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     search_number: [re\u001b[38;5;241m.\u001b[39mcompile(pattern) \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m search_number, patterns \u001b[38;5;129;01min\u001b[39;00m post_oalex_regex_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Run filter\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m oalex_records_dict_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_records\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecords_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moalex_records_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregex_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_oalex_regex_compiled_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_ngrams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 98\u001b[0m, in \u001b[0;36mfilter_records\u001b[1;34m(records_dict, regex_dict, match_all, include_ngrams)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Convert raw OpenAlex records (dictionaries) to Record objects *and* extract ngrams\u001b[39;00m\n\u001b[0;32m     95\u001b[0m matching_records \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     96\u001b[0m     record \n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records_list\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_matches_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43msearch_number\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmatch_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_ngrams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m ]\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of records after filtering: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(matching_records)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    103\u001b[0m filtered_records[search_number] \u001b[38;5;241m=\u001b[39m matching_records\n",
      "Cell \u001b[1;32mIn[60], line 36\u001b[0m, in \u001b[0;36m_matches_patterns\u001b[1;34m(record, patterns, match_all, include_ngrams)\u001b[0m\n\u001b[0;32m     29\u001b[0m searchable_text_parts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mstr\u001b[39m(record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mstr\u001b[39m(record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mstr\u001b[39m(kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay_name\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     33\u001b[0m ]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_ngrams:\n\u001b[1;32m---> 36\u001b[0m     searchable_text_parts\u001b[38;5;241m.\u001b[39mextend(\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Conditionally include ngrams\u001b[39;00m\n\u001b[0;32m     38\u001b[0m searchable_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(searchable_text_parts)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match_all:\n",
      "File \u001b[1;32mD:\\bin\\Lib\\site-packages\\pyalex\\api.py:896\u001b[0m, in \u001b[0;36mWork.ngrams\u001b[1;34m(self, return_meta)\u001b[0m\n\u001b[0;32m    893\u001b[0m n_gram_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mopenalex_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/works/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopenalex_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ngrams\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m res \u001b[38;5;241m=\u001b[39m _get_requests_session()\u001b[38;5;241m.\u001b[39mget(n_gram_url, auth\u001b[38;5;241m=\u001b[39mOpenAlexAuth(config))\n\u001b[1;32m--> 896\u001b[0m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m results \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    899\u001b[0m resp_list \u001b[38;5;241m=\u001b[39m OpenAlexResponseList(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mngrams\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mD:\\bin\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: NOT FOUND for url: https://api.openalex.org/works/W2766361688/ngrams"
     ]
    }
   ],
   "source": [
    "# Pre-compile the regex patterns:\n",
    "post_oalex_regex_compiled_dict = {\n",
    "    search_number: [re.compile(pattern) for pattern in patterns]\n",
    "    for search_number, patterns in post_oalex_regex_dict.items()\n",
    "}\n",
    "\n",
    "#Run filter\n",
    "oalex_records_dict_filtered = filter_records(\n",
    "    records_dict=oalex_records_dict,\n",
    "    regex_dict=post_oalex_regex_compiled_dict,\n",
    "    match_all=True,\n",
    "    include_ngrams=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8057e210-9610-49a0-abfc-12473bad6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialize list of records\n",
    "oalex_records_pkl = os.path.join(\n",
    "    resdir, \n",
    "    f\"oalex_records_{datetime.now(UTC).strftime('%Y%m%d%H%M')}\"\n",
    ")\n",
    "with open(oalex_records_pkl, 'wb') as f:\n",
    "    pickle.dump(oalex_records_dict_filtered, f)\n",
    "\n",
    "\n",
    "#Import openalex records\n",
    "oalex_records_pkl = regex_list_files(in_dir=resdir, \n",
    "                       in_pattern='oalex_records_.*'\n",
    "                      )[-1]\n",
    "\n",
    "with open(oalex_records_pkl, 'rb') as f:\n",
    "    oalex_records = pickle.load(f)\n",
    "\n",
    "print(list(list(oalex_records.values())[0][0].values()))\n",
    "#Get n-grams for records, filter by title, abstract, keywords and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e1159-6904-464f-9956-276c3523b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "~~~~~ Search for works in OpenAlex based on search string ~~~~~~~~~~~~~~~~~~~~~~\n",
    "Reference info on the API: \n",
    "# https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/search-entities\n",
    "# https://docs.openalex.org/api-entities/works/search-works\n",
    "\n",
    "#EX: https://api.openalex.org/works?search=(elmo AND \"sesame street\") NOT (cookie OR monster)\n",
    "#Filter categories based on csv\n",
    "#&per-page=100&cursor=*\n",
    "\n",
    "#~~~~~~~~~~~~~~~~ PAGING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Basic paging only works to get the first 10,000 results of any list. If you want to see more than 10,000 results, you'll need to use cursor paging.\n",
    "To use cursor paging, you request a cursor by adding the cursor=* parameter-value pair to your query.\n",
    "    Get a cursor in order to start cursor pagination:\n",
    "    https://api.openalex.org/works?filter=publication_year:2020&per-page=100&cursor=*\n",
    "The response to your query will include a next_cursor value in the response's meta object. Here's what it looks like:\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"count\": 8695857,\n",
    "    \"db_response_time_ms\": 28,\n",
    "    \"page\": null,\n",
    "    \"per_page\": 100,\n",
    "    \"next_cursor\": \"IlsxNjA5MzcyODAwMDAwLCAnaHR0cHM6Ly9vcGVuYWxleC5vcmcvVzI0ODg0OTk3NjQnXSI=\"\n",
    "  },\n",
    "  \"results\" : [\n",
    "    // the first page of results\n",
    "  ]\n",
    "}\n",
    "\n",
    "To retrieve the next page of results, copy the meta.next_cursor value into the cursor field of your next request.\n",
    "\n",
    "    Get the next page of results using a cursor value:\n",
    "    https://api.openalex.org/works?filter=publication_year:2020&per-page=100&cursor=IlsxNjA5MzcyODAwMDAwLCAnaHR0cHM6Ly9vcGVuYWxleC5vcmcvVzI0ODg0OTk3NjQnXSI=\n",
    "\n",
    "To get all the results, keep repeating this process until meta.next_cursor is null and the results set is empty.\n",
    "'''\n",
    "\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
