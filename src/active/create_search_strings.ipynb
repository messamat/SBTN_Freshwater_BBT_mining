{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fc3e68-1090-4bb7-a986-ce3defb9da8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Create search strings\n",
    "\n",
    "Generates structured search strings for querying OpenAlex based on pre-defined keywords and patterns. The script leverages text-processing utilities to ensure that the queries are adaptable to lemmatization or strict searches (using american and english spelling, word inflection, participles, plural forms, if needed). Then combine groups of search strings with boolean operators.\n",
    "This scripts only defines functions, which are then used in create_api_call.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c60fd3a-61cb-4250-9eca-8f3f07bc16a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\messa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run lit_utility_functions_2025.ipynb\n",
    "\n",
    "import bream\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from pyinflect import getInflection\n",
    "import spacy\n",
    "from textblob import Word\n",
    "\n",
    "\n",
    "# Load NLTK resources\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "# Load spaCy model\n",
    "try:\n",
    "    spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading en_core_web_sm model...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c96c664-2429-433b-8efa-e3e713eba316",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_matching_words(pattern: str, \n",
    "                        bound_pattern: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "      Finds all English words that match a given regular expression pattern.\n",
    "      Args:\n",
    "        pattern: The regular expression pattern (string).\n",
    "        bound_pattern (bool, optional): If True, adds start (^) and end ($)\n",
    "                                        anchors to the pattern to ensure the\n",
    "                                        entire word matches. Defaults to True.\n",
    "      Returns:\n",
    "        A list of English words that match the pattern. Returns an empty list if no\n",
    "        words match or if there's an invalid regex pattern. Prints a warning if the\n",
    "        NLTK words corpus is not found (this behavior depends on NLTK setup).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Optionally add start (^) and end ($) anchors to the regex pattern.\n",
    "        # This ensures the pattern matches the *entire* word, not just a substring.\n",
    "        if bound_pattern:\n",
    "            pattern = f'^{pattern}$'\n",
    "\n",
    "        # Compile the regular expression for efficiency, especially when matching many words.\n",
    "        regex = re.compile(pattern)\n",
    "        \n",
    "        # Load the list of English words from the NLTK corpus.\n",
    "        english_words = words.words()\n",
    "\n",
    "        # Filter words that match the regex.\n",
    "        matching_words = [word for word in english_words if regex.search(word)]\n",
    "\n",
    "        return matching_words\n",
    "    except re.error as e:\n",
    "        # Handle cases where the provided pattern is an invalid regular expression.\n",
    "        print(f\"Warning: Invalid regex pattern provided: {pattern}. Error: {e}\")\n",
    "        return []\n",
    "    except LookupError:\n",
    "        # Handle cases where the NLTK 'words' corpus is not downloaded.\n",
    "        print(\"Warning: NLTK 'words' corpus not found. Please download it (e.g., nltk.download('words')).\")\n",
    "        return []\n",
    "    except NameError:\n",
    "        # Handle cases where 'words' object (from NLTK) is not defined/imported.\n",
    "        print(\"Warning: NLTK 'words' corpus object not found. Ensure NLTK is imported correctly.\")\n",
    "        return []\n",
    "\n",
    "def combinate_concats(\n",
    "    prefixes: List[str], \n",
    "    suffixes: List[str], \n",
    "    separators: List[str] = [\" \", \"-\", \"\"], \n",
    "    add_quotes: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generates all combinations of prefixes, separators, and suffixes.\n",
    "\n",
    "    Args:\n",
    "        prefixes (List[str]): A list of prefix strings.\n",
    "        suffixes (List[str]): A list of suffix strings.\n",
    "        separators (List[str], optional): A list of separator strings. \n",
    "                                          Defaults to [\" \", \"-\", \"\"].\n",
    "        add_quotes (bool, optional): If True, encloses each combination in \n",
    "                                     double quotes. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, each being a combination of a prefix, \n",
    "                   a separator, and a suffix. \n",
    "    Details:\n",
    "        Uses itertools.product for efficiency.\n",
    "    \"\"\"\n",
    "    # Generate the Cartesian product of the input iterables (prefixes, separators, suffixes).\n",
    "    product_iterator = itertools.product(prefixes, separators, suffixes)\n",
    "\n",
    "    combinations: List[str]\n",
    "    if add_quotes:\n",
    "        # If quotes are requested, format each combination accordingly.\n",
    "        combinations = [\n",
    "            '\"' + \"\".join(combination) + '\"'\n",
    "            for combination in product_iterator\n",
    "        ]\n",
    "    else:\n",
    "        # Otherwise, just join the parts of each combination tuple.\n",
    "        combinations = [\n",
    "            \"\".join(combination)\n",
    "            for combination in product_iterator\n",
    "        ]\n",
    "        \n",
    "    return combinations\n",
    "\n",
    "\n",
    "def plural_form_exists(word: str, \n",
    "                       in_lemmatizer: WordNetLemmatizer) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a plausible plural form of a word exists, using WordNet and \n",
    "    rule-based fallbacks. Handles some irregular plurals via lemmatization.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to check.\n",
    "        in_lemmatizer (WordNetLemmatizer): An initialized NLTK WordNetLemmatizer \n",
    "                                           instance.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a plausible plural form is found (either the word itself\n",
    "              seems plural, or a known plural form exists), False otherwise.\n",
    "    \"\"\"\n",
    "    # Ensure necessary NLTK data is available (handling potential errors)\n",
    "    try:\n",
    "        # 1. Check if the word is *already* potentially plural in WordNet.\n",
    "        #    This checks if any lemma associated with the word's synsets ends in 's'.\n",
    "        #    It's a heuristic, not foolproof.\n",
    "        if wordnet.synsets(word) and any(lemma.name().endswith('s') \n",
    "                                         for synset in wordnet.synsets(word)\n",
    "                                         for lemma in synset.lemmas()):\n",
    "            return True\n",
    "\n",
    "        # 2. Lemmatize the word to find its base (singular) form for nouns.\n",
    "        #    This helps identify the root even if the input 'word' is already plural.\n",
    "        lemma: str = in_lemmatizer.lemmatize(word, pos=wordnet.NOUN) # Specify Part of Speech\n",
    "\n",
    "        # 3. Check if the original word is different from its lemma.\n",
    "        #    If word != lemma, it implies the original 'word' was likely an inflected \n",
    "        #    form (potentially plural) that was reduced to its base 'lemma'.\n",
    "        if lemma != word:\n",
    "            return True # Word is not the lemma, suggesting it's likely already plural.\n",
    "    \n",
    "        # 4. If lemma and word are the same (word is likely singular), try adding 's'.\n",
    "        #    Check if this simple plural form exists in WordNet.\n",
    "        simple_plural_s: str = word + 's'\n",
    "        if wordnet.synsets(simple_plural_s):\n",
    "            return True # The basic 'word + s' plural exists in WordNet.\n",
    "\n",
    "        # 5. Apply common English pluralization rules as a fallback.\n",
    "        #    This handles cases not directly covered by WordNet lookups above.\n",
    "        plural: str = \"\" # Initialize plural form variable\n",
    "        if word.endswith((\"s\", \"x\", \"z\", \"ch\", \"sh\")):\n",
    "            plural = word + \"es\" # Add 'es' for words ending in s, x, z, ch, sh\n",
    "        elif word.endswith(\"y\") and len(word) > 1 and word[-2].lower() not in \"aeiou\":\n",
    "            plural = word[:-1] + \"ies\" # Change 'y' to 'ies' if preceded by a consonant\n",
    "        else:\n",
    "            # Default rule: just add 's'. This also covers cases like vowel + 'y'.\n",
    "            plural = word + \"s\" \n",
    "        \n",
    "        # Check if the rule-based plural form exists in WordNet.\n",
    "        if wordnet.synsets(plural):\n",
    "            return True # The rule-based plural form is found in WordNet.\n",
    "\n",
    "    except NameError:\n",
    "        # Handle cases where 'wordnet' or the lemmatizer is not defined/imported.\n",
    "        print(\"Warning: NLTK 'wordnet' or lemmatizer not found. Ensure NLTK is imported/initialized.\")\n",
    "        return False # Cannot perform check\n",
    "    except LookupError:\n",
    "        # Handle cases where NLTK 'wordnet' corpus is not downloaded.\n",
    "        print(\"Warning: NLTK 'wordnet' corpus not found. Please download it (e.g., nltk.download('wordnet')).\")\n",
    "        return False # Cannot perform check\n",
    "\n",
    "    # If none of the above checks found a plausible plural form.\n",
    "    return False\n",
    "\n",
    "\n",
    "def textblob_pluralize(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates the plural form of a word using the TextBlob library.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to pluralize.\n",
    "\n",
    "    Returns:\n",
    "        str: The pluralized form of the word as determined by TextBlob.\n",
    "             Returns the original word if TextBlob fails or is not available.\n",
    "    \"\"\"\n",
    "    # Create a TextBlob Word object from the input string.\n",
    "    w = Word(word)\n",
    "    return w.pluralize()\n",
    "\n",
    "\n",
    "def get_spelling_variants(word: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Gets American and British spelling variants of a word using the 'bream' library.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word for which to find spelling variants.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list containing the original word and its American/British \n",
    "                   spelling variants if found. Duplicates are automatically handled.\n",
    "                   Returns a list with only the original word if 'bream' is unavailable\n",
    "                   or if variants are not found/cause errors.\n",
    "    \"\"\"\n",
    "    # Use a set to automatically handle duplicate entries.\n",
    "    variants: Set[str] = set()\n",
    "    variants.add(word)  # Always include the original word\n",
    "\n",
    "    try:\n",
    "        # Attempt to convert the word to American English spelling.\n",
    "        # This might raise an error if the word is not in bream's dictionary.\n",
    "        american: str = bream.to_american(word)\n",
    "        variants.add(american)\n",
    "    except Exception:\n",
    "        # Ignore errors if conversion fails (e.g., word not found).\n",
    "        pass # Keep the original word only if American variant fails.\n",
    "        \n",
    "    try:\n",
    "        # Attempt to convert the word to British English spelling.\n",
    "        british: str = bream.to_british(word)\n",
    "        variants.add(british)\n",
    "    except Exception:\n",
    "        # Ignore errors if conversion fails.\n",
    "        pass # Keep the existing variants if British variant fails.\n",
    "\n",
    "    # Convert the set back to a list before returning.\n",
    "    return list(variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08690266-641a-4901-9685-06739712e23d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_eflows_search_terms():\n",
    "    combo1_1 = [\"ecologic\\\\S*\", [\"eco\", \"hydrologic\\\\S*\"], \n",
    "                [\"hydro\", \"ecologic\\\\S*\"],\n",
    "                'environmental', 'minim\\\\S\\\\S', 'acceptable',\n",
    "                'augmented', 'augmentation', 'compensation', \n",
    "                'experimental', 'flushing', ['in', 'stream'], 'maintenance',\n",
    "                'optimum', 'restorati\\\\S{2}']\n",
    "\n",
    "    combo1_2 = ['flood', 'flow', ['water', 'level'], 'discharge']\n",
    "    \n",
    "    combo2_1 = ['compensat[a-z]{1,3}', 'conservation', 'cultural', ['cut', 'off'], \n",
    "                'design', 'fish', 'functional', 'indigenous', 'limit', 'maintenance',\n",
    "                'management', 'maximum', 'natural', 'preference', \n",
    "                'protection', 'rating', 'regime[a-z]{0,1}', 'residual',\n",
    "                'right', 'sanita(ry|tion)', 'scenario', 'standard', \n",
    "                'suitable', 'surplus', 'sustainable', 'threshold',\n",
    "                'use', 'vital']\n",
    "    combo2_2 = ['flow']\n",
    "    \n",
    "    combo3_1 = ['downstream', 'dam', 'reservoir']\n",
    "    combo3_2 = [['water', 'release'], ['flow', 'release'], 'reoperation']\n",
    "    \n",
    "    combo4_1 = ['controlled', 'artificial']\n",
    "    combo4_2 = ['flood']\n",
    "    \n",
    "    combo5_1 = ['hydrologic(al)*']\n",
    "    combo5_2 = ['requirement', 'manipulation']\n",
    "    \n",
    "    combo6_1 = ['flow', ['stream', 'flow'], 'freshwater', 'water', ['water', 'level']]\n",
    "    combo6_2 = ['abstraction', 'allocation', 'criteri\\\\S{1,2}', 'delivery*', \n",
    "                'demand', 'guideline',\n",
    "                'need', 'prescription', 'recommendation', 'recovery', 'requirement', \n",
    "                'reserve', 'restoration', 'restriction', 'withdrawal']\n",
    "    \n",
    "    search_dict = {\n",
    "         'search1': ['with', [combo1_1, combo1_2]],\n",
    "         'search2':  ['with', [combo2_1, combo2_2]],\n",
    "         'search3': ['pre', [combo3_1, combo3_2]],\n",
    "         'search4':  ['pre', [combo4_1, combo4_2]],\n",
    "         'search5':  ['pre', [combo5_1, combo5_2]],\n",
    "         'search6':  ['with', [combo6_1, combo6_2]]\n",
    "    }\n",
    "    return(search_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174e182-2042-41a2-8f98-fcd034670258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_string(\n",
    "    in_search_duo: List[str], \n",
    "    inflect: bool, \n",
    "    or_chars: str = ' OR ', \n",
    "    and_chars: str = ' AND ',\n",
    "    inner_separators: List[str] = [\" \", \"-\", \"\"], \n",
    "    use_quotes: bool = True # Default for potential quoting, but logic inside might override\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Creates a search query string based on structured input, \n",
    "    optionally handling word inflections, combinations, and logical operators.\n",
    "\n",
    "    Args:\n",
    "        in_search_duo (Tuple[str, Tuple[List[Union[str, List[str]]], List[Union[str, List[str]]]]]): \n",
    "            A tuple containing:\n",
    "            - [0] (str): The type of combination ('pre', 'with', etc.).\n",
    "            - [1] (Tuple[List, List]): A tuple containing two lists. Each list \n",
    "              represents a conceptual part of the search and contains elements \n",
    "              that are either single regex patterns (str) or lists of regex \n",
    "              patterns (List[str]) intended to be combined.\n",
    "        inflect (bool): If True, attempt to expand words by adding plural forms,\n",
    "                      present participles, and spelling variants. If False, \n",
    "                      words found via regex matching may be lemmatized using spaCy \n",
    "                      (depending on internal logic).\n",
    "        or_chars (str, optional): String used to join alternatives within a group. \n",
    "                                Defaults to ' OR '.\n",
    "        and_chars (str, optional): String used to join the two main groups when \n",
    "                                 in_search_duo[0] is 'with'. Defaults to ' AND '.\n",
    "        inner_separators (List[str], optional): Separators used when combining \n",
    "                                            multi-word patterns via `combinate_concats`. \n",
    "                                            Defaults to [\" \", \"-\", \"\"].\n",
    "        use_quotes (bool, optional): Initial preference for using quotes. Actual quoting \n",
    "                                 behaviour for combined multi-word patterns depends \n",
    "                                 on whether in_search_duo[0] is 'with'. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted search query string.\n",
    "\n",
    "    Raises:\n",
    "        NameError: If required functions (find_matching_words, combinate_concats, etc.) \n",
    "                   or objects (spacy_nlp, WordNetLemmatizer) are not defined/imported.\n",
    "        TypeError: If input structures don't match expected types (e.g., if \n",
    "                   `getInflection` returns unexpected type).\n",
    "\n",
    "    Dependencies:\n",
    "        Requires NLTK (WordNetLemmatizer, wordnet), TextBlob, spaCy, and potentially\n",
    "        a 'bream' library. \n",
    "    \"\"\"\n",
    "    if inflect:\n",
    "        # Initialize NLTK lemmatizer only if needed for inflection checks.\n",
    "        nltk_lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "    # Stores the processed lists of words/phrases for each of the two main parts.\n",
    "    combo_list_formatted = [] \n",
    "\n",
    "    # Iterate through the two main lists provided in in_search_duo[1]\n",
    "    for combo_list in in_search_duo[1]:\n",
    "        # Stores processed words/phrases for the current combo_list\n",
    "        word_group_formatted = [] \n",
    "        \n",
    "        # Iterate through each element (a pattern or list of patterns) in the current combo_list\n",
    "        for repattern_group in combo_list:\n",
    "            \n",
    "            repattern_group_processed = [] # Holds results for this repattern_group\n",
    "\n",
    "            # --- Case 1: repattern_group is a list (multiple patterns to combine) ---\n",
    "            if isinstance(repattern_group, list):\n",
    "                # This block handles cases like [\"pattern1\", \"pattern2\"] which might become \"word1 word2\", \"word1-word2\", \"word1word2\" etc.\n",
    "                \n",
    "                # Process each pattern in the inner list\n",
    "                repattern_sub_results = [] # Collect results for each sub-pattern\n",
    "                for repattern in repattern_group:\n",
    "                    # Find English words matching the regex pattern\n",
    "                    k = find_matching_words(repattern) \n",
    "                    \n",
    "                    # If no matching words found, use the original pattern as a fallback\n",
    "                    if not k: # Check if list is empty or if string is empty (though find_matching_words returns list)\n",
    "                        k = [repattern] # Use original pattern, ensure it's a list\n",
    "                    elif not isinstance(k, list):\n",
    "                         # Ensure k is always a list, even if find_matching_words changed behavior\n",
    "                         k = [k]\n",
    "\n",
    "                    # Lemmatization happens if 'inflect' is False. This might seem counter-intuitive.\n",
    "                    # It standardizes terms found via regex *unless* full inflection generation is requested later.\n",
    "                    if not inflect:\n",
    "                        try:\n",
    "                            # Lemmatize the found words/original pattern using spaCy\n",
    "                            token_list = spacy_nlp(\" \".join(k)) \n",
    "                            k = [token.lemma_ for token in token_list if token.lemma_] # Ensure lemma exists\n",
    "                        except NameError:\n",
    "                             print(\"Warning: 'spacy_nlp' object not found. Cannot lemmatize. Ensure spaCy is loaded.\")\n",
    "                             # Keep 'k' as is if spacy fails\n",
    "\n",
    "                    repattern_sub_results.append(k)\n",
    "\n",
    "                # Combine the processed sub-results\n",
    "                # Only use quotes if the overall search type is 'with' AND the initial use_quotes flag was True.\n",
    "                # Otherwise, combined phrases generated here won't be quoted.\n",
    "                should_quote_combination = (in_search_duo[0] == 'with') and use_quotes\n",
    "                \n",
    "                try:\n",
    "                    # Combine the lists using specified inner separators\n",
    "                    repattern_group_processed = combinate_concats(\n",
    "                        prefixes=repattern_sub_results[0],\n",
    "                        suffixes=repattern_sub_results[1],\n",
    "                        separators=inner_separators,\n",
    "                        add_quotes=should_quote_combination \n",
    "                    )\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Expected two lists in repattern_sub_results for combination, but got {len(repattern_sub_results)}. Skipping combination.\")\n",
    "                    # Fallback: just flatten the list\n",
    "                    repattern_group_processed = [word for sublist in repattern_sub_results for word in sublist]\n",
    "\n",
    "\n",
    "            # --- Case 2: repattern_group is a single string pattern ---\n",
    "            else: \n",
    "                # Find English words matching the regex pattern\n",
    "                k = find_matching_words(repattern_group)\n",
    "                \n",
    "                # Fallback to original pattern if no matches\n",
    "                if not k:\n",
    "                     k = [repattern_group] # Use original pattern, ensure list\n",
    "                elif not isinstance(k, list):\n",
    "                     k = [k] # Ensure list\n",
    "\n",
    "                # Lemmatize using spaCy if 'inflect' is False\n",
    "                if not inflect:\n",
    "                    try:\n",
    "                        token_list = spacy_nlp(\" \".join(k))\n",
    "                        k = [token.lemma_ for token in token_list if token.lemma_]\n",
    "                    except NameError:\n",
    "                        print(\"Warning: 'spacy_nlp' object not found. Cannot lemmatize.\")\n",
    "                        # Keep 'k' as is\n",
    "\n",
    "                repattern_group_processed = k # Assign the list k to the processed variable\n",
    "\n",
    "            # --- Post-processing for the current repattern_group (applies to both cases) ---\n",
    "            \n",
    "            # Add the processed words/phrases, removing duplicates encountered so far within this group\n",
    "            # Note: set conversion here might change order\n",
    "            current_set = set(word_group_formatted)\n",
    "            current_set.update(repattern_group_processed)\n",
    "            word_group_formatted = list(current_set)\n",
    "        \n",
    "        # --- Inflection/Variant Generation (applied to the whole word_group_formatted) ---\n",
    "        # This block runs *only* if inflect=True, expanding the word list further.\n",
    "        \n",
    "        # Create a new list to store all variations for this word group\n",
    "        new_word_group: List[str] = [] \n",
    "        if inflect: \n",
    "            for word in word_group_formatted:\n",
    "                # Clean word from potential quotes added by combinate_concats if needed\n",
    "                clean_word = word.strip('\"') \n",
    "                \n",
    "                # Always add the current word/phrase itself\n",
    "                new_word_group.append(word) \n",
    "                 \n",
    "                # Add plural form if it likely exists (only works well for single words)\n",
    "                if \" \" not in clean_word and \"-\" not in clean_word: # Basic check for single word\n",
    "                   try:\n",
    "                       if plural_form_exists(clean_word, nltk_lemmatizer):\n",
    "                            plural_word = textblob_pluralize(clean_word)\n",
    "                            if plural_word != clean_word: # Avoid adding if plural is same as singular\n",
    "                                new_word_group.append(plural_word)\n",
    "                                # Optionally add quotes back if original had them\n",
    "                                if word.startswith('\"') and word.endswith('\"'):\n",
    "                                    new_word_group.append(f'\"{plural_word}\"') \n",
    "                   except NameError:\n",
    "                       print(\"Warning: textblob_pluralize or plural_form_exists not found.\")\n",
    "                   except Exception as e:\n",
    "                       print(f\"Warning: Error during pluralization of '{clean_word}': {e}\")\n",
    "\n",
    "\n",
    "                # Add present participle\n",
    "                if \" \" not in clean_word and \"-\" not in clean_word: # Basic check for single word\n",
    "                    try:\n",
    "                        pre_participle_list= getInflection(clean_word, 'VBG')\n",
    "                        if pre_participle_list: \n",
    "                            for pp_word in pre_participle_list:\n",
    "                                if pp_word != clean_word: # Avoid adding if participle is same as original\n",
    "                                    new_word_group.append(pp_word)\n",
    "                                    # Optionally add quotes back\n",
    "                                    if word.startswith('\"') and word.endswith('\"'):\n",
    "                                        new_word_group.append(f'\"{pp_word}\"')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error during present participle generation for '{clean_word}': {e}\")\n",
    "\n",
    "\n",
    "                # Add spelling variants (british vs american)\n",
    "                if \" \" not in clean_word and \"-\" not in clean_word: # Basic check for single word\n",
    "                    try:\n",
    "                        spelling_variants = get_spelling_variants(clean_word)\n",
    "                        for variant in spelling_variants:\n",
    "                            if variant != clean_word: # Avoid adding the original word again\n",
    "                                new_word_group.append(variant)\n",
    "                                # Optionally add quotes back\n",
    "                                if word.startswith('\"') and word.endswith('\"'):\n",
    "                                    new_word_group.append(f'\"{variant}\"')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error during spelling variant generation for '{clean_word}': {e}\")\n",
    "        else:\n",
    "             # If not inflecting, just use the words processed so far\n",
    "             new_word_group.extend(word_group_formatted) \n",
    "\n",
    "        # Remove duplicates introduced by inflection/variant generation\n",
    "        word_group_formatted = list(set(new_word_group))\n",
    "\n",
    "        # Add the finalized list of words/phrases for this combo_list\n",
    "        combo_list_formatted.append(word_group_formatted)\n",
    "         \n",
    "    # --- Final Combination of the two processed groups ---    \n",
    "    try:\n",
    "        # Combine based on the type specified in in_search_duo[0]\n",
    "        if in_search_duo[0] == 'pre':\n",
    "            # Case 'pre': Combine elements from the first list with elements from the second list, separated by space.\n",
    "            # Then join all these combinations with OR.\n",
    "\n",
    "            combinations = combinate_concats(\n",
    "                prefixes=combo_list_formatted[0], \n",
    "                suffixes=combo_list_formatted[1],\n",
    "                separators=[\" \"],\n",
    "                add_quotes=use_quotes\n",
    "            )\n",
    "            # Join all generated combinations using the specified OR characters\n",
    "            search_duo_formatted = recomb(combinations, \n",
    "                                          recomb_sep=or_chars)\n",
    "        \n",
    "        elif in_search_duo[0] == 'with':\n",
    "            # Case 'with': Create two blocks of OR-separated terms, and join the blocks with AND.\n",
    "            # Block 1: Join all terms from the first list with OR\n",
    "            block1 = recomb(combo_list_formatted[0], recomb_sep=or_chars)\n",
    "            # Block 2: Join all terms from the second list with OR\n",
    "            block2 = recomb(combo_list_formatted[1], recomb_sep=or_chars)\n",
    "            # Join the two blocks with the specified AND characters\n",
    "            search_duo_formatted = f\"{block1}{and_chars}{block2}\"\n",
    "        \n",
    "        else:\n",
    "            # Handle unknown combination types if necessary\n",
    "            print(f\"Warning: Unknown search duo type '{in_search_duo[0]}'. Returning empty string.\")\n",
    "            search_duo_formatted = \"\"\n",
    "            \n",
    "    except NameError as e:\n",
    "         print(f\"Warning: Function 'combinate_concats' or 'recomb' not found. Error: {e}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error during final combination: {e}\")\n",
    "             \n",
    "    return search_duo_formatted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
