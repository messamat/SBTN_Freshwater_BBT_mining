{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d481ea4-7da1-4ef3-a305-7f09a3ebc0f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Retrieve and filter OpenAlex records through API calls\n",
    "\n",
    "Create and run API calls for OpenAlex. Integrates search strings and sets up parameters for interfacing with OpenAlex and Zotero APIs, then make the literature search calls in OpenAlex, filter the results by location and specific search strings and deduplicate them across searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99592c59-ac28-4801-b22c-16df6f0de8c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\messa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run set_up.py\n",
    "%run lit_utility_functions_2025.ipynb\n",
    "%run create_search_strings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a03aa3eb-a33d-40ca-a952-f21d66a712ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import pyalex #https://github.com/J535D165/pyalex\n",
    "from pyalex import config, Works\n",
    "from typing import Any, Dict, List, Pattern, Tuple, Union\n",
    "\n",
    "config.max_retries = 1\n",
    "config.email = \"mathis.messager@mail.mcgill.ca\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63a0e8-78ea-4ac8-bf8e-4b1bbd222c33",
   "metadata": {},
   "source": [
    "## Get dictionnary of search strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7737d4df-02b7-4ced-b397-efd5019d0ce7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_terms_dict = create_eflows_search_terms()\n",
    "oalex_string_dict = {}\n",
    "for search_number, search_terms in search_terms_dict.items():\n",
    "    oalex_string_dict[search_number] = create_search_string(\n",
    "        search_terms, inflect=False, or_chars=' OR ', and_chars=' AND ',\n",
    "        inner_separators=[\" \", \"-\", \"\"],  use_quotes=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185199a8-dd15-4856-bce9-5e27bf91d76f",
   "metadata": {},
   "source": [
    "## Get OpenAlex concepts to filter search with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f4d181d-ea17-4278-b4b5-26c4331e4fe1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_last_url_segment(url) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the last segment of a URL path. \n",
    "\n",
    "    Args:\n",
    "        url: The URL string.\n",
    "\n",
    "    Returns:\n",
    "        The last segment of the URL path, or None if the URL is invalid\n",
    "        or has no path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        if not path:\n",
    "            return None  # No path component\n",
    "\n",
    "        # Split the path by '/' and get the last element\n",
    "        segments = path.split('/')\n",
    "        return segments[-1]  # Handle cases with trailing slashes correctly\n",
    "\n",
    "    except Exception:  # Catch any parsing errors\n",
    "        return None\n",
    "\n",
    "def extract_url_end_from_df(df: pd.DataFrame, \n",
    "                                url_col: str, \n",
    "                                include_col: str = None\n",
    "                               ) -> pd.Dataframe:\n",
    "    \"\"\"\n",
    "    Extracts the last segment of URLs from a specific column in a DataFrame,\n",
    "    filtering by a boolean column, and adds the result as a new column.\n",
    "\n",
    "    Args:\n",
    "      df: The Pandas DataFrame.\n",
    "      url_col: The name of the column containing URLs (string).\n",
    "      include_col: The name of the boolean column to filter by (string).\n",
    "\n",
    "    Returns:\n",
    "        A new Pandas DataFrame with an additional column 'openalex_id_last_segment'\n",
    "        containing the extracted last segment, or None if the input is invalid.\n",
    "    \"\"\"\n",
    "    # Input validation: Check for required columns\n",
    "    required_columns = [url_col]\n",
    "    if include_col is not None:\n",
    "        required_columns.append(include_col)\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"DataFrame is missing some columns.\")\n",
    "        return None\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if include_col is not None:\n",
    "        # Convert include_col to boolean, handling various representations.\n",
    "        df_copy[include_col] = df_copy[include_col].astype(str).str.lower().isin(['y', 'yes', 'true', '1', 't'])\n",
    "        # Apply the extraction function ONLY to rows where 'include_col' is True,\n",
    "        # and ONLY to the 'url_col' of those rows. Use .loc for proper indexing.\n",
    "        included_clist = df_copy.loc[df_copy[include_col], url_col].apply(extract_last_url_segment)\n",
    "    else:\n",
    "        included_clist = df_copy.loc[:, url_col].apply(extract_last_url_segment)\n",
    "\n",
    "    return included_clist.tolist()\n",
    "\n",
    "#Get open alex concepts to filter with\n",
    "concepts_toinclude_pd = pd.read_csv(\n",
    "    os.path.join(datdir, 'openalex_concepts_toinclude.csv'))\n",
    "concepts_toinclude_list =  extract_url_end_from_df(\n",
    "    df = concepts_toinclude_pd,\n",
    "    url_col = 'openalex_id', \n",
    "    include_col = 'include')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe7f1b-624f-4627-ab15-355a6f7aa97c",
   "metadata": {},
   "source": [
    "## Get hydrographic and administrative toponyms to filter with\n",
    "Right now, this only includes toponyms for the Upper Mississippi River Basin (UMRB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a432b2f2-1635-46b1-87d9-37e8966e5caa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_toponyms_pkl = regex_list_files(in_dir=resdir, \n",
    "                       in_pattern='target_toponyms_umrb_.*'\n",
    "                      )[-1]\n",
    "\n",
    "with open(target_toponyms_pkl, 'rb') as f:\n",
    "    target_toponyms_set = pickle.load(f)\n",
    "target_toponyms_quoted_set = {f'\"{t}\"' for t in target_toponyms_set}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6cdd85-0834-487f-bc61-62427ce95359",
   "metadata": {},
   "source": [
    "## Run OpenAlex searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf22299-a575-4bab-a15b-869e2edba6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _retrieve_oalex_records(title_and_abstract_string: str, \n",
    "                            concepts_list: list = None, \n",
    "                            search_all_string: str = None,\n",
    "                            n_max: int = 200,\n",
    "                            verbose: bool = False) -> list(dict):\n",
    "    \"\"\"\n",
    "    Retrieves records from OpenAlex based on title/abstract search, filtered by concepts, and optional broad search.\n",
    "\n",
    "    Args:\n",
    "        title_and_abstract_string (str): Search string for the title and abstract fields.\n",
    "        concepts_list (list, optional): List of OpenAlex concept IDs (strings) to filter by (OR logic). \n",
    "                                        Defaults to None.\n",
    "        search_all_string (str, optional): A broad search string applied to title, abstract, AND full text\n",
    "                                           before other filters. Defaults to None.\n",
    "        n_max (int, optional): Maximum number of records to retrieve. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of OpenAlex work records (dictionaries) matching the criteria.\n",
    "              Returns an empty list if no records are found or in case of API errors (not explicitly handled here).\n",
    "              \n",
    "    Dependencies:\n",
    "        Requires an OpenAlex client library (e.g., pyalex) providing Works() and its methods.\n",
    "        Requires `itertools`.\n",
    "    \"\"\"\n",
    "    # Start building the OpenAlex Works query.\n",
    "    if search_all_string:\n",
    "        oa_query = Works().search(search_all_string)\n",
    "    else:\n",
    "        # Otherwise, start with an unfiltered Works() query object.\n",
    "        oa_query = Works()\n",
    "     \n",
    "    # Apply mandatory filters to the query:\n",
    "    oa_query = oa_query.\\\n",
    "        search_filter(title_and_abstract=title_and_abstract_string).\\\n",
    "        filter(is_retracted='False')\n",
    "\n",
    "    # Optionally, filter by a list of concepts.\n",
    "    if concepts_list:\n",
    "        concept_filter_value = {\"id\": '|'.join(concepts_list)}\n",
    "        oa_query = oa_query.filter(concept=concept_filter_value)\n",
    "\n",
    "    #Print the generated OpenAlex API query URL for debugging.\n",
    "    if verbose:\n",
    "        print(oa_query.url)\n",
    "    \n",
    "    # --- Paginate and Retrieve Records ---\n",
    "    # Use pagination to retrieve results efficiently.\n",
    "    # The paginate method returns an iterator yielding lists of records (one list per page).\n",
    "    pages_iterator = oa_query.paginate(per_page=min([n_max, 200]), n_max=n_max)\n",
    "    \n",
    "    # Flatten the lists of records from each page into a single list\n",
    "    out_records_list = list(itertools.chain(*pages_iterator))\n",
    "    \n",
    "    return(out_records_lis)\n",
    "\n",
    "# --- Core function to orchestrate multiple OpenAlex queries ---\n",
    "def retrieve_oalex_records_dict(title_and_abstract_string_dict: dict,\n",
    "                                 concepts_list: list = None, \n",
    "                                 search_all_string_list: list = None, \n",
    "                                 n_max: int = 200,\n",
    "                                 verbose: bool = True\n",
    "                                ):\n",
    "    \"\"\"\n",
    "    Retrieves OpenAlex records for multiple search criteria, de-duplicates globally, \n",
    "    and returns results grouped by the initial search key.\n",
    "\n",
    "    Args:\n",
    "        title_and_abstract_string_dict (dict): A dictionary where keys are identifiers \n",
    "                                               for searches (e.g., search numbers) and values \n",
    "                                               are the specific title/abstract search strings.\n",
    "        concepts_list (list, optional): List of OpenAlex concept IDs (strings) applied to ALL searches. \n",
    "                                        Defaults to None.\n",
    "        search_all_string_list (list, optional): A list of broad search strings to be applied to title,\n",
    "                                                 abstract, and full text. Must match the order of title_and_abstract \n",
    "        n_max (int, optional): Maximum number of records to retrieve *per individual call* to the helper function `_retrieve_oalex_records`. Defaults to 200.\n",
    "        verbose (bool, optional): If True, prints status messages during retrieval. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        collections.defaultdict: A dictionary where keys are the same as the input \n",
    "                                 `title_and_abstract_string_dict` keys, and values are lists \n",
    "                                 of unique OpenAlex work records (dictionaries) matching the \n",
    "                                 criteria for that key. Duplicates across different keys are removed.\n",
    "                                 \n",
    "    Dependencies:\n",
    "        Requires `collections`.\n",
    "        Calls the helper function `_retrieve_oalex_records`.\n",
    "    \"\"\"\n",
    "    # Initialize a defaultdict to store lists of records for each search key.\n",
    "    out_records_dict = collections.defaultdict(list)\n",
    "    # Initialize a set to keep track of all unique record IDs encountered across all searches.\n",
    "    seen_ids = set()\n",
    "    \n",
    "    # Iterate through the dictionary of title/abstract search strings.\n",
    "    for search_number, search_terms in title_and_abstract_string_dict.items():\n",
    "        print(f'Retrieving {search_number}')\n",
    "        # List to hold all records retrieved for the current 'search_terms' across all broad searches.\n",
    "        initial_records_list = []\n",
    "        \n",
    "        # Check if a list of broad search strings was provided.\n",
    "        if search_all_string_list:\n",
    "            # Iterate through each broad search string provided.\n",
    "            for search_all_string in search_all_string_list:\n",
    "                if verbose:\n",
    "                    print(f'Subsetting for {search_all_string}')\n",
    "                # Call the helper function to retrieve records for the specific title/abstract terms,\n",
    "                # combined with the current broad search string and concepts.\n",
    "                retrieved_recs = _retrieve_oalex_records(\n",
    "                    title_and_abstract_string=search_terms,\n",
    "                    concepts_list=concepts_list, \n",
    "                    search_all_string=search_all_string, \n",
    "                    n_max=n_max\n",
    "                )\n",
    "                # If records were retrieved, add them to the list for this search number.\n",
    "                if retrieved_recs:\n",
    "                    initial_records_list.extend(retrieved_recs)\n",
    "        else:\n",
    "             # If no broad search list provided, run the search once without a broad filter.\n",
    "             retrieved_recs = _retrieve_oalex_records(\n",
    "                 title_and_abstract_string=search_terms,\n",
    "                 concepts_list=concepts_list, \n",
    "                 search_all_string=None, # No broad search string\n",
    "                 n_max=n_max \n",
    "             )\n",
    "             if retrieved_recs:\n",
    "                 initial_records_list.extend(retrieved_recs)\n",
    "\n",
    "        # --- De-duplicate Records Globally ---\n",
    "        # Create a new list to store unique records for the current search_number.\n",
    "        new_records_list = []  \n",
    "        for record in initial_records_list:\n",
    "            rid = record['id']\n",
    "            # Check if this record ID has already been seen across *any* previous search_number.\n",
    "            if rid not in seen_ids:\n",
    "                # If it's a new record, add it to the list for the current search_number.\n",
    "                new_records_list.append(record)\n",
    "                # Add the ID to the set of seen IDs to prevent duplicates later or in other search_numbers.\n",
    "                seen_ids.add(rid)\n",
    "                \n",
    "        # Assign the de-duplicated list of records to the current search_number key in the output dictionary.\n",
    "        out_records_dict[search_number] = new_records_list\n",
    "         \n",
    "    return(out_records_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "435b5d24-3723-46bc-8eb2-e3886726baaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving search1\n",
      "Retrieving search2\n",
      "Retrieving search3\n",
      "Retrieving search4\n",
      "Retrieving search5\n",
      "Retrieving search6\n",
      "[1438, 618, 158, 38, 3, 665]\n"
     ]
    }
   ],
   "source": [
    "oalex_records_dict = retrieve_oalex_records_dict(\n",
    "    title_and_abstract_string_dict=oalex_string_dict,\n",
    "    concepts_list=concepts_toinclude_list, \n",
    "    search_all_string_list=list(target_toponyms_quoted_set)[0:100],\n",
    "    n_max=100,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print([len(rec_list) for rec_list in oalex_records_dict.values() if rec_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "891a3b50-341f-419e-aa2b-64518eb47701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_parentheses(in_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove parentheses *if* they enclose the entire expression\n",
    "\n",
    "    Args:\n",
    "        in_str: string with (or without) parentheses to remove\n",
    "\n",
    "    Returns:\n",
    "        str: string without surrounding parentheses\n",
    "    \"\"\"\n",
    "    if in_str.startswith('(') and in_str.endswith(')'):\n",
    "        # Check if they are *actually* unnecessary (i.e., not (a|b) AND (c|d))\n",
    "        open_count = 0\n",
    "        unnecessary = True\n",
    "        while unnecessary:\n",
    "            for i, char in enumerate(in_str):\n",
    "                if char == '(':\n",
    "                    open_count += 1\n",
    "                elif char == ')':\n",
    "                    open_count -= 1\n",
    "                if open_count == 0 and i < len(in_str) - 1:\n",
    "                    unnecessary = False\n",
    "                    break\n",
    "            if unnecessary:\n",
    "                in_str = in_str[1:-1]\n",
    "        return(in_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58fe367-0f80-421d-8c01-9a39a34a6a3e",
   "metadata": {},
   "source": [
    "## Generate regex to filter OpenAlex after the search\n",
    "To make up for lemmatization performed by Open Alex, which may introduce many irrelevant records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b448f-9caf-489c-86e6-4f310924bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_oalex_regex_dict = {}\n",
    "for search_number, search_terms in search_terms_dict.items():\n",
    "    #Generated an initial filter to be adjusted \n",
    "    #(split it in two regex queries for the AND rather than using greedy lookaheads\n",
    "    post_oalex_regex_dict[search_number] = create_search_string(\n",
    "        search_terms, inflect=True, or_chars='|', and_chars='AND',\n",
    "        inner_separators=[r\"[-\\s]*\"], use_quotes=False)\n",
    "    #Remove redundant parentheses and split for nested regex filters\n",
    "    post_oalex_regex_dict[search_number] = remove_redundant_parentheses(\n",
    "        post_oalex_regex_dict[search_number]).split('AND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "552dddda-919d-4fd4-ad5b-9be3b89bc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _matches_patterns(\n",
    "    record: Dict[str, List[Any]], \n",
    "    patterns: List[re.Pattern], \n",
    "    match_all: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a record matches regex patterns, \n",
    "    with options for 'all' or 'any' matching and including ngrams.\n",
    "\n",
    "    Args:\n",
    "        record: The record to check.\n",
    "        patterns: List of compiled regex patterns.\n",
    "        match_all: If True, all patterns must match.  If False, at least one must match.\n",
    "        include_ngrams: If True, include ngrams in the searchable text.\n",
    "\n",
    "    Returns:\n",
    "        True if the record matches the patterns according to the conditions, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(patterns, list):\n",
    "        raise TypeError(\"patterns must be a list of compiled regex patterns.\")\n",
    "    if not all(isinstance(p, re.Pattern) for p in patterns):\n",
    "         raise TypeError(\"patterns must be a list of compiled regex patterns.\")\n",
    "    if not isinstance(match_all, bool):\n",
    "        raise TypeError(\"match_all must be a boolean.\")\n",
    "\n",
    "    searchable_text_parts = [\n",
    "        str(record['title']),\n",
    "        str(record['abstract']),\n",
    "        *[str(kw['display_name']) for kw in record['keywords']]\n",
    "    ]\n",
    "\n",
    "    searchable_text = \" \".join(searchable_text_parts)\n",
    "\n",
    "    if match_all:\n",
    "        match_bool = all(pattern.search(searchable_text) for pattern in patterns)\n",
    "    else:\n",
    "        match_bool = any(pattern.search(searchable_text) for pattern in patterns)\n",
    "\n",
    "    return match_bool\n",
    "\n",
    "\n",
    "def filter_records(\n",
    "    records_dict: Dict[str, List[Dict]],\n",
    "    regex_dict: Dict[str, List[re.Pattern]],\n",
    "    match_all: bool = True\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Filters records based on regex patterns, with options for n-gram inclusion.\n",
    "\n",
    "    Args:\n",
    "        records_dict: Dictionary mapping search numbers to lists of raw OpenAlex records (dictionaries).\n",
    "        regex_dict: Dictionary mapping search numbers to lists of compiled regex patterns.\n",
    "        match_all: If True, all patterns must match. If False, at least one must match.\n",
    "        include_ngrams: If True, include ngrams in the searchable text for matching.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing filtered records (as Record objects) that match the\n",
    "        regex patterns for their respective search number, according to the conditions.\n",
    "    \"\"\"\n",
    "    if not isinstance(records_dict, dict):\n",
    "        raise TypeError(\"records_dict must be a dictionary.\")\n",
    "    if not isinstance(regex_dict, dict):\n",
    "        raise TypeError(\"regex_dict must be a dictionary.\")\n",
    "    if not isinstance(match_all, bool):\n",
    "        raise TypeError(\"match_all must be a boolean.\")\n",
    "\n",
    "     # Check if keys in records_dict and regex_dict match\n",
    "    if records_dict.keys() != regex_dict.keys():\n",
    "        raise ValueError(\"Keys in records_dict and regex_dict must be identical.\")\n",
    "\n",
    "    #check if values are lists in dict\n",
    "    if not all(isinstance(val, list) for val in regex_dict.values()):\n",
    "          raise TypeError(\"Values of regex_dict must be lists.\")\n",
    "\n",
    "    filtered_records = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(f\"Processing search number: {search_number}\")\n",
    "        print(f\"Initial number of records: {len(records_list)}\")\n",
    "\n",
    "        if not records_list:\n",
    "            print(f\"No records for search number {search_number}, skipping.\")\n",
    "            filtered_records[search_number] = []  # Consistent return type\n",
    "            continue\n",
    "\n",
    "        # Convert raw OpenAlex records (dictionaries) to Record objects *and* extract ngrams\n",
    "        matching_records = [\n",
    "            record \n",
    "            for record in records_list\n",
    "            if _matches_patterns(record, \n",
    "                                 regex_dict[search_number],\n",
    "                                 match_all)\n",
    "        ]\n",
    "\n",
    "        print(f\"Number of records after filtering: {len(matching_records)}\")\n",
    "        filtered_records[search_number] = matching_records\n",
    "\n",
    "    return filtered_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61162241-0508-43f5-b416-54b42da0b78d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing search number: search1\n",
      "Initial number of records: 1438\n",
      "Number of records after filtering: 835\n",
      "Processing search number: search2\n",
      "Initial number of records: 618\n",
      "Number of records after filtering: 583\n",
      "Processing search number: search3\n",
      "Initial number of records: 158\n",
      "Number of records after filtering: 1\n",
      "Processing search number: search4\n",
      "Initial number of records: 38\n",
      "Number of records after filtering: 14\n",
      "Processing search number: search5\n",
      "Initial number of records: 3\n",
      "Number of records after filtering: 3\n",
      "Processing search number: search6\n",
      "Initial number of records: 665\n",
      "Number of records after filtering: 618\n"
     ]
    }
   ],
   "source": [
    "# Pre-compile the regex patterns:\n",
    "post_oalex_regex_compiled_dict = {\n",
    "    search_number: [re.compile(pattern) for pattern in patterns]\n",
    "    for search_number, patterns in post_oalex_regex_dict.items()\n",
    "}\n",
    "\n",
    "#Run filter\n",
    "oalex_records_dict_filtered = filter_records(\n",
    "    records_dict=oalex_records_dict,\n",
    "    regex_dict=post_oalex_regex_compiled_dict,\n",
    "    match_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8057e210-9610-49a0-abfc-12473bad6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialize list of records\n",
    "oalex_records_pkl = os.path.join(\n",
    "    resdir, \n",
    "    f\"oalex_records_{datetime.now(UTC).strftime('%Y%m%d%H%M')}.pkl\"\n",
    ")\n",
    "with open(oalex_records_pkl, 'wb') as f:\n",
    "    pickle.dump(oalex_records_dict_filtered, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f6091eb-85f8-45b9-9d4e-34b3f860f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing out D:\\WWF_SBTN\\BTT_analysis\\results\\oalex_records_search1_ris_202503221849.ris\n",
      "Writing out D:\\WWF_SBTN\\BTT_analysis\\results\\oalex_records_search2_ris_202503221849.ris\n",
      "Writing out D:\\WWF_SBTN\\BTT_analysis\\results\\oalex_records_search3_ris_202503221849.ris\n",
      "Writing out D:\\WWF_SBTN\\BTT_analysis\\results\\oalex_records_search4_ris_202503221849.ris\n",
      "Writing out D:\\WWF_SBTN\\BTT_analysis\\results\\oalex_records_search5_ris_202503221849.ris\n",
      "Writing out D:\\WWF_SBTN\\BTT_analysis\\results\\oalex_records_search6_ris_202503221849.ris\n"
     ]
    }
   ],
   "source": [
    "#Export to RIS\n",
    "for search_number, records_list in oalex_records_dict_filtered.items():\n",
    "    oalex_records_ris_path = os.path.join(\n",
    "        resdir, \n",
    "        f\"oalex_records_{search_number}_ris_{datetime.now(UTC).strftime('%Y%m%d%H%M')}.ris\"\n",
    "    )\n",
    "    print(f'Writing out {oalex_records_ris_path}')\n",
    "    export_oalex_works_to_ris(\n",
    "        works=records_list,\n",
    "        filename=oalex_records_ris_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf9785-f229-4a7a-9f6c-af3b97fb7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### USEFUL UNUSED FUNCTIONS ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09634897-3e05-4504-8839-c985e50e72fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "def remove_oalex_search_duplicates(records_dict):\n",
    "    seen_ids = set()\n",
    "    new_records_dict = {}\n",
    "    for search_number, records_list in records_dict.items():\n",
    "        print(search_number)\n",
    "        print(len(records_list))\n",
    "        new_records_list = []  # Create a new list for each search_number\n",
    "        for record in records_list:\n",
    "            rid = record['id']\n",
    "            if rid not in seen_ids:\n",
    "                new_records_list.append(record)\n",
    "                seen_ids.add(rid)\n",
    "        new_records_dict[search_number] = new_records_list\n",
    "        print(len(new_records_list))\n",
    "    return(new_records_dict)\n",
    "#oalex_records_dict = remove_oalex_search_duplicates(oalex_records_dict) # Replace the old dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e1159-6904-464f-9956-276c3523b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "~~~~~ Search for works in OpenAlex based on search string ~~~~~~~~~~~~~~~~~~~~~~\n",
    "Reference info on the API: \n",
    "# https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/search-entities\n",
    "# https://docs.openalex.org/api-entities/works/search-works\n",
    "\n",
    "#EX: https://api.openalex.org/works?search=(elmo AND \"sesame street\") NOT (cookie OR monster)\n",
    "#Filter categories based on csv\n",
    "#&per-page=100&cursor=*\n",
    "\n",
    "#~~~~~~~~~~~~~~~~ PAGING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Basic paging only works to get the first 10,000 results of any list. If you want to see more than 10,000 results, you'll need to use cursor paging.\n",
    "To use cursor paging, you request a cursor by adding the cursor=* parameter-value pair to your query.\n",
    "    Get a cursor in order to start cursor pagination:\n",
    "    https://api.openalex.org/works?filter=publication_year:2020&per-page=100&cursor=*\n",
    "The response to your query will include a next_cursor value in the response's meta object. Here's what it looks like:\n",
    "{\n",
    "  \"meta\": {\n",
    "    \"count\": 8695857,\n",
    "    \"db_response_time_ms\": 28,\n",
    "    \"page\": null,\n",
    "    \"per_page\": 100,\n",
    "    \"next_cursor\": \"IlsxNjA5MzcyODAwMDAwLCAnaHR0cHM6Ly9vcGVuYWxleC5vcmcvVzI0ODg0OTk3NjQnXSI=\"\n",
    "  },\n",
    "  \"results\" : [\n",
    "    // the first page of results\n",
    "  ]\n",
    "}\n",
    "\n",
    "To retrieve the next page of results, copy the meta.next_cursor value into the cursor field of your next request.\n",
    "\n",
    "    Get the next page of results using a cursor value:\n",
    "    https://api.openalex.org/works?filter=publication_year:2020&per-page=100&cursor=IlsxNjA5MzcyODAwMDAwLCAnaHR0cHM6Ly9vcGVuYWxleC5vcmcvVzI0ODg0OTk3NjQnXSI=\n",
    "\n",
    "To get all the results, keep repeating this process until meta.next_cursor is null and the results set is empty.\n",
    "'''\n",
    "\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
