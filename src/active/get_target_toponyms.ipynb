{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "117404d8-b306-4583-a8d3-e2adf2433854",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Get target toponyms\n",
    "\n",
    "Retrieve and format a list of toponyms and unique hydrographic identifiers (IDs) associated with a set of input locations\n",
    "Possible inputs include: a table of coordinates, a vector dataset (points, lines or polygons), or specific basin IDs\n",
    "The toponyms and IDs are retrieved from the US National Hydrography Dataset Plus (NHDPlus), Global Administrative Areas (GADM) dataset, and HydroATLAS.\n",
    "This scripts only defines functions, which are then used in create_location_filter_string.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0545b1-2abe-4e10-ad2b-987cc7c6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run download_geographic_refs.ipynb\n",
    "%run set_up.py \n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd #require pip install pyarrow\n",
    "from shapely.geometry import Point\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "verbose=True #Whether to print detailed progress messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad174fb-a2a9-4e12-99b2-6a3ea69c45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pts_path = os.path.join(datdir, 'test_gages', 'test_gages.shp') #Example shapefile to show how the code functions: retire\n",
    "\n",
    "wbd_path = os.path.join(nhd_dir, 'WBD_National_GDB.gdb') #Local path to US Watershed Boundary Dataset\n",
    "hu12_parquet = os.path.join(nhd_dir, 'wbd_hu12list.parquet') #Local path to output table of all HUC12\n",
    "\n",
    "basinatlas_path = os.path.join(hydroatlas_dir,  'BasinATLAS_v10.gdb')\n",
    "basinatlas11_parquet = os.path.join(hydroatlas_dir, 'basinatlas_lev11_idlist.parquet') Local path to output table of all HydroBASINS level 11\n",
    "#geoglows_vpu_path = os.path.join(geoglows_dir, 'vpu-boundaries.gpkg')\n",
    "gadm_path = os.path.join(gadm_dir, 'gadm_410-levels.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c216daa-b88a-49f9-ba50-677d7669e109",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _expand_basin_idlist(in_id_list: List,              \n",
    "                         in_refids_parquet: str,       \n",
    "                         refids_col: str,               \n",
    "                         out_id_range: List[int]):       \n",
    "    \"\"\"\n",
    "    Expands a list of basin IDs by extracting IDs at different levels from a reference Parquet file.\n",
    "\n",
    "    Args:\n",
    "        in_id_list (list): List of input basin IDs.  Can be integers or strings.\n",
    "        in_refids_parquet (str): Path to the reference Parquet file.\n",
    "        refids_col (str): Name of the column in the Parquet file containing the full basin IDs.\n",
    "        out_id_range (list): List of integer levels to extract.  e.g., [6, 9, 12]\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the expanded basin IDs.  The returned DataFrame\n",
    "                          will have columns named based on `refids_col` and levels in `out_id_range`.\n",
    "                          The data type of the output columns will match the input type of `in_id_list`.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the length of input IDs is inconsistent with the length in reference table.\n",
    "        FileNotFoundError: If the input Parquet file does not exist.\n",
    "        ValueError:  If `out_id_range` contains values greater than the maximum ID length.\n",
    "                   Or if an empty DataFrame is returned by filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    id_all_pd = pd.read_parquet(in_refids_parquet)\n",
    "\n",
    "    # --- Determine input type and maximum ID length ---\n",
    "    in_id_type = type(in_id_list[0])\n",
    "    if in_id_type == str:\n",
    "      in_id_len = len(in_id_list[0])\n",
    "    elif in_id_type == int:\n",
    "      in_id_len = len(str(in_id_list[0]))\n",
    "    else:\n",
    "      raise TypeError(\"in_id_list must be a list of strings or integers\")\n",
    "\n",
    "    # --- Determine the reference ID type and length ---\n",
    "    first_ref_id = id_all_pd[refids_col].iloc[0] #Get first item in col\n",
    "    refid_type = type(first_ref_id)\n",
    "\n",
    "    if refid_type == str:\n",
    "      refid_len = len(first_ref_id)\n",
    "    elif pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "      refid_len = len(str(first_ref_id))\n",
    "    else:\n",
    "      raise TypeError(f\"{refids_col} must contain strings or integers\")\n",
    "\n",
    "\n",
    "    if in_id_len > refid_len:\n",
    "        raise ValueError(f\"Input ID length ({in_id_len}) is greater than reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Input Validation on out_id_range ---\n",
    "    if any(level > refid_len for level in out_id_range):\n",
    "        raise ValueError(f\"out_id_range values cannot exceed reference ID length ({refid_len})\")\n",
    "\n",
    "    # --- Filtering ---\n",
    "    # Convert to string for consistent prefix matching, then convert back later\n",
    "    id_all_pd[refids_col] = id_all_pd[refids_col].astype(str)\n",
    "    in_id_list_str = [str(x) for x in in_id_list]\n",
    "\n",
    "    id_pd = id_all_pd[id_all_pd[refids_col].str.startswith(tuple(in_id_list_str))].copy()\n",
    "\n",
    "    if id_pd.empty:\n",
    "        raise ValueError(\"No matching IDs found. Check in_id_list and refids_col.\")\n",
    "\n",
    "    # --- Column Expansion ---\n",
    "    # Determine the base name for column\n",
    "    colroot = re.sub(r'[0-9]+', '', refids_col)\n",
    "    # Loop through each requested ID level (e.g., 6, 9, 12).\n",
    "    for id_level in out_id_range:\n",
    "        col_name = f'{colroot}{id_level}'\n",
    "        if col_name not in id_pd.columns:\n",
    "            id_pd.loc[:, col_name] = id_pd[refids_col].str[:id_level]\n",
    "            # Convert back to original type, if necessary\n",
    "            if in_id_type == int:\n",
    "                id_pd.loc[:, col_name] = pd.to_numeric(id_pd[col_name],\n",
    "                                                       errors='raise') \n",
    "\n",
    "    # Convert refids_col back to original type\n",
    "    if pd.api.types.is_integer_dtype(refid_type) or  refid_type == np.int64:\n",
    "        id_pd[refids_col] = pd.to_numeric(id_pd[refids_col], \n",
    "                                          errors='raise')\n",
    "\n",
    "    return id_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b629b3-15d0-4616-88d3-dcdd5dd59e93",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _format_gdf_tojoin(in_xytab: str = None, \n",
    "                       lon_col: str = None, \n",
    "                       lat_col: str = None, \n",
    "                       in_crs: str = \"EPSG:4326\",\n",
    "                       in_vector: str = None, \n",
    "                       hull: bool = True):\n",
    "    \"\"\"\n",
    "    Creates a GeoDataFrame from either XY coordinates in a table or a vector file,\n",
    "    optionally calculating the convex hull of the resulting geometry.\n",
    "\n",
    "    Note: This function expects EITHER in_xytab (with lon/lat cols) OR in_vector \n",
    "          to be provided, not both. If both are provided, in_vector will take precedence.\n",
    "\n",
    "    Args:\n",
    "        in_xytab (str, optional): Path to a table file (e.g., CSV, TSV) containing point coordinates. \n",
    "                                   Defaults to None. Assumed delimited by whitespace/tabs by pd.read_table.\n",
    "        lon_col (str, optional): Name of the longitude column in the table file specified by in_xytab. \n",
    "                                 Required if in_xytab is used. Defaults to None.\n",
    "        lat_col (str, optional): Name of the latitude column in the table file specified by in_xytab. \n",
    "                                 Required if in_xytab is used. Defaults to None.\n",
    "        in_vector (str, optional): Path to a vector file (e.g., Shapefile, GeoJSON, GeoPackage) readable by GeoPandas.\n",
    "                                   Defaults to None.\n",
    "        hull (bool, optional): If True, computes the convex hull of all geometries in the resulting \n",
    "                               GeoDataFrame, returning a GDF with a single polygon geometry. \n",
    "                               Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: A GeoDataFrame containing geometries derived from the input. \n",
    "                                If hull=True, it contains a single row with the convex hull polygon.\n",
    "                                If hull=False, it contains geometries read from the input source.\n",
    "                                Returns None or raises an error if input arguments are invalid or files not found.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the path provided in in_xytab or in_vector does not exist.\n",
    "        KeyError: If lon_col or lat_col are not found in the DataFrame read from in_xytab.\n",
    "        AttributeError: If lon_col or lat_col are None when in_xytab is provided.\n",
    "        Exception: Depending on pandas or geopandas file reading errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Branch 1: Create GeoDataFrame from an XY table ---\n",
    "    if in_xytab:\n",
    "        points_df = pd.read_table(in_xytab) \n",
    "        # Create a GeoPandas GeoDataFrame from the pandas DataFrame.\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            points_df,\n",
    "            geometry=[Point(xy) for xy \n",
    "                      in zip(points_df[lon_col], points_df[lat_col])] \n",
    "            crs=in_crs\n",
    "        )\n",
    "     \n",
    "    # --- Branch 2: Read GeoDataFrame directly from a vector file ---\n",
    "    if in_vector:\n",
    "        # If both in_xytab and in_vector were provided, this will overwrite \n",
    "        # the gdf_to_join created from in_xytab.\n",
    "        gdf_to_join = gpd.read_file(in_vector)\n",
    "\n",
    "    # --- Optional Step: Calculate Convex Hull ---\n",
    "    if hull and gdf_to_join is not None:\n",
    "        # The result is wrapped in a new GeoDataFrame containing a single row with this hull polygon.\n",
    "        # The CRS (Coordinate Reference System) is preserved from the original GeoDataFrame.\n",
    "        gdf_to_join = gpd.GeoDataFrame(\n",
    "            geometry=[gdf_to_join.geometry.union_all().convex_hull], \n",
    "            crs=gdf_to_join.crs\n",
    "        )\n",
    "        \n",
    "    # Return the final GeoDataFrame (either original geometries, the convex hull, or potentially None if no input provided).\n",
    "    return gdf_to_join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f92e4a12-8af2-4549-9500-e7d3131fdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of PFAF_ID for basins level 11\n",
    "def create_basinatlas11_list(basinatlas_path: str, \n",
    "                             out_basinatlas11_parquet: str, \n",
    "                             verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Creates or loads a list of PFAF_ID values for BasinATLAS level 11 basins.\n",
    "\n",
    "    This function acts as a cache. If the output Parquet file exists, it loads it.\n",
    "    Otherwise, it reads the specific layer from the input BasinATLAS file,\n",
    "    extracts, renames, and types the PFAF_ID column, saves it to Parquet, \n",
    "    and then returns it.\n",
    "\n",
    "    Args:\n",
    "        basinatlas_path (str): Path to the input BasinATLAS dataset \n",
    "                              (e.g., GeoPackage or File Geodatabase).\n",
    "        out_basinatlas11_parquet (str): Path where the extracted list of \n",
    "                                        level 11 PFAF IDs will be saved \n",
    "                                        or loaded from (as a Parquet file).\n",
    "        verbose (bool, optional): If True, prints messages about generating \n",
    "                                  or loading the file. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing a single column named 'PFAF_ID11' \n",
    "                          with the level 11 basin IDs as nullable integers.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If basinatlas_path does not exist when generation is needed.\n",
    "        Exception: Potentially other exceptions from geopandas or pandas file IO \n",
    "                   or if the specified layer/column doesn't exist in basinatlas_path.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Check if the processed list already exists as a Parquet file ---\n",
    "    if not os.path.exists(out_basinatlas11_parquet):\n",
    "        if verbose:\n",
    "            print(f'Generating a list of PFAF ID level 11 and saving it to '\n",
    "                  f'{out_basinatlas11_parquet}')\n",
    "              \n",
    "        # Read the specific layer containing level 11 basins from the input file.\n",
    "        basinatlas11_list = gpd.read_file(\n",
    "            filename=basinatlas_path,        \n",
    "            layer='BasinATLAS_v10_lev11',    \n",
    "            columns=['PFAF_ID'],         \n",
    "            # WARNING: Hardcoded row count. This assumes the layer *always* has exactly \n",
    "            # 1,031,785 rows. \n",
    "            rows=1031785,                    \n",
    "            ignore_geometry=True   \n",
    "        ).\\\n",
    "        astype(pd.Int64Dtype()).\\ # Convert the 'PFAF_ID' column to pandas nullable integer type (Int64).(avoid NAs)\n",
    "        rename(columns={\"PFAF_ID\": \"PFAF_ID11\"}) # Rename the column for clarity (indicating it's level 11).\n",
    "         \n",
    "        basinatlas11_list.to_parquet(out_basinatlas11_parquet)\n",
    "\n",
    "    # --- File Loading Branch ---\n",
    "    else:\n",
    "        if verbose:\n",
    "             print(f\"Found existing file. Loading BasinATLAS level 11 list from {out_basinatlas11_parquet}\")\n",
    "             \n",
    "        # Load the list directly from the existing Parquet file.\n",
    "        basinatlas11_list = pd.read_parquet(out_basinatlas11_parquet)\n",
    "        \n",
    "    return basinatlas11_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f6737-54ea-4aec-a173-6e55ed17915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_hydrobasin(in_basinatlas_path: str,\n",
    "                            in_xytab: str = None, lon_col: str = None, lat_col: str = None,\n",
    "                            in_vector: str = None, \n",
    "                            in_id_list: list = None, \n",
    "                            in_refids_parquet: str = None,\n",
    "                            hull: bool = True, \n",
    "                            sjoin_predicate: str = 'intersects'):\n",
    "    \"\"\"\n",
    "    Finds corresponding HydroBASINS IDs (levels 3-11) based on input spatial data \n",
    "    (XY coordinates in a table, points, polygons) or a provided list of basin IDs.\n",
    "\n",
    "    Workflow:\n",
    "    1. If spatial data (in_xytab or in_vector) is provided:\n",
    "       a. Formats the input into a GeoDataFrame (optionally using convex hull).\n",
    "       b. Reads level 6 HydroBASINS polygons.\n",
    "       c. Performs a spatial join to find level 6 basins intersecting the input geometry.\n",
    "       d. Extracts the list of matching level 6 PFAF IDs. This list overrides any `in_id_list` passed as input.\n",
    "    2. If only `in_id_list` is provided (and no spatial data):\n",
    "       a. Uses the provided `in_id_list` directly.\n",
    "    3. Expands the determined list of basin IDs (from step 1d or 2a) to levels 3-11 \n",
    "       using a reference Parquet file containing level 11 IDs.\n",
    "\n",
    "    Args:\n",
    "        in_basinatlas_path (str): Path to the HydroBASINS dataset (e.g., GeoPackage).\n",
    "        in_xytab (str, optional): Path to table with XY coordinates. Defaults to None.\n",
    "        lon_col (str, optional): Longitude column name in in_xytab. Defaults to None.\n",
    "        lat_col (str, optional): Latitude column name in in_xytab. Defaults to None.\n",
    "        in_vector (str, optional): Path to vector file. Defaults to None.\n",
    "        in_id_list (list, optional): A pre-defined list of basin IDs (typically level 6 PFAF_IDs \n",
    "                                     if bypassing spatial join). Defaults to None. \n",
    "                                     Note: This is ignored if in_xytab or in_vector is provided.\n",
    "        in_refids_parquet (str, optional): Path to reference Parquet file containing level 11 \n",
    "                                           PFAF IDs (required for expansion). Defaults to None.\n",
    "        hull (bool, optional): If True and using spatial input, compute convex hull before \n",
    "                               spatial join. Defaults to True.\n",
    "        sjoin_predicate (str, optional): Spatial predicate for the join ('intersects', 'within', etc.). \n",
    "                                         Defaults to 'intersects'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with expanded basin IDs for levels 3 through 11, \n",
    "                          corresponding to the input features or ID list.\n",
    "\n",
    "    Raises:\n",
    "        Depends on the underlying functions: FileNotFoundError, KeyError, ValueError, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1: Determine initial basin IDs (Level 6) ---\n",
    "    if in_xytab or in_vector:\n",
    "        # --- Workflow based on spatial input ---\n",
    "        # Format the input spatial data into a GeoDataFrame using the helper function.\n",
    "        # 'hull=True' (default) will create a convex hull polygon before joining.\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab=in_xytab, lon_col=lon_col, lat_col=lat_col, \n",
    "                                         in_vector=in_vector, \n",
    "                                         hull=hull)\n",
    "         \n",
    "        # Read the level 6 BasinATLAS layer\n",
    "        bas_lev6 = gpd.read_file(filename=in_basinatlas_path, \n",
    "                                 layer='BasinATLAS_v10_lev06', # Specific layer for level 6\n",
    "                                 columns=['PFAF_ID'] # Only load necessary column\n",
    "                                ).rename(columns={\"PFAF_ID\": \"PFAF_ID6\"})\n",
    "     \n",
    "        # Perform a spatial join to find which level 6 basins match the input geometry.\n",
    "        # 'gdf_to_join' is reprojected to match the CRS of the basin layer before joining.\n",
    "        matched_bas = gpd.sjoin(gdf_to_join.to_crs(crs=bas_lev6.crs),\n",
    "                                bas_lev6, \n",
    "                                how='left', \n",
    "                                predicate=sjoin_predicate)\n",
    "                                \n",
    "        # Extract the list of unique, non-null level 6 PFAF IDs found from the spatial join.\n",
    "        # IMPORTANT: This result overrides any `in_id_list` provided as an argument \n",
    "        # if spatial data (in_xytab or in_vector) was given.\n",
    "        in_id_list = matched_bas['PFAF_ID6'].dropna().unique().tolist()\n",
    "\n",
    "    # --- Step 2: Expand the determined list of basin IDs to levels 3-11 ---\n",
    "    # This step runs using either the `in_id_list` from the spatial join (if performed)\n",
    "    # or the `in_id_list` provided directly as input (if no spatial data was given).\n",
    "    \n",
    "     # Call the helper function to expand the list of IDs (assumed level 6) \n",
    "     # to levels 3 through 11 using the reference Parquet file.\n",
    "     pfaf_pd = _expand_basin_idlist(\n",
    "         in_id_list=in_id_list,              \n",
    "         in_refids_parquet=in_refids_parquet,\n",
    "         refids_col='PFAF_ID11',           \n",
    "         out_id_range=range(3, 12)  \n",
    "     )\n",
    "\n",
    "    return pfaf_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e034ba-e56e-413a-8cab-eeb9c3a89e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO BE COMPLETED\n",
    "def get_hydroatlas_data():\n",
    "    print('Getting HydroATLAS data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198ae95-d5cf-42c0-9e8c-cf760c802748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huc12_list(wbd_path: str, \n",
    "                      out_hu12_parquet: str,\n",
    "                      verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Creates or loads a list of NHD HUC12 codes from a Watershed Boundary Dataset (WBD).\n",
    "\n",
    "    This function acts as a cache. If the output Parquet file exists, it loads it.\n",
    "    Otherwise, it reads the specified WBD layer, extracts the HUC12 codes, \n",
    "    saves the list to a Parquet file, and then returns it.\n",
    "\n",
    "    Args:\n",
    "        wbd_path (str): Path to the Watershed Boundary Dataset \n",
    "                       (e.g., GeoPackage or File Geodatabase).\n",
    "        out_hu12_parquet (str): Path where the extracted list of HUC12 codes\n",
    "                                will be saved or loaded from (as a Parquet file).\n",
    "        verbose (bool, optional): If True, prints messages about generating or \n",
    "                                  loading the file. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing a single column named 'huc12' \n",
    "                          with the HUC12 codes (likely as strings or objects, \n",
    "                          depending on source data).\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If wbd_path does not exist when generation is needed.\n",
    "        Exception: Potentially other exceptions from geopandas or pandas file IO \n",
    "                   or if the specified layer/column doesn't exist in wbd_path.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(out_hu12_parquet):\n",
    "        # --- File Generation Branch ---        \n",
    "        if verbose:\n",
    "            print(f'Generating a list of HUC 12 and saving it to '\n",
    "                  f'{out_hu12_parquet}')\n",
    "            \n",
    "        # Read the specific layer containing HUC12 units from the WBD dataset.\n",
    "        wbdhu12_list = gpd.read_file(\n",
    "            filename=wbd_path,       \n",
    "            layer='WBDHU12',                     \n",
    "            columns=['huc12'],         \n",
    "            ignore_geometry=True     \n",
    "        ) \n",
    "        \n",
    "        wbdhu12_list.to_parquet(out_hu12_parquet)\n",
    "        \n",
    "    else:\n",
    "        # --- File Loading Branch ---\n",
    "        if verbose:\n",
    "            print(f\"Found existing file. Loading HUC12 list from {out_hu12_parquet}\")\n",
    "        wbdhu12_list = pd.read_parquet(out_hu12_parquet)\n",
    "        \n",
    "    return wbdhu12_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194bc546-fc2b-42d9-a71a-1538a779ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_NHD_HU(in_wbd_path: str,\n",
    "                        in_xytab: str = None, lon_col: str = None, lat_col: str = None,\n",
    "                        in_vector: str = None, \n",
    "                        in_id_list: list = None, \n",
    "                        in_refids_parquet: str = None,\n",
    "                        hull: bool = True, \n",
    "                        sjoin_predicate: str = 'intersects'):\n",
    "    \"\"\"\n",
    "    Finds corresponding NHD Hydrologic Unit Codes (HUC2 to HUC12) based on \n",
    "    input spatial data (points, polygons) or a provided list of HUC IDs.\n",
    "\n",
    "    Workflow:\n",
    "    1. If spatial data (in_xytab or in_vector) is provided:\n",
    "       a. Formats the input into a GeoDataFrame (optionally using convex hull).\n",
    "       b. Reads HUC6 (level 6) polygons from the Watershed Boundary Dataset (WBD).\n",
    "       c. Performs a spatial join to find HUC6 units intersecting the input geometry.\n",
    "       d. Extracts the list of matching HUC6 codes. This list overrides any `in_id_list` passed as input.\n",
    "    2. If only `in_id_list` is provided (and no spatial data):\n",
    "       a. Uses the provided `in_id_list` directly (assumed to be HUC6 codes).\n",
    "    3. Expands the determined list of HUC IDs (from step 1d or 2a) to levels 2, 4, 6, 8, 10, 12 \n",
    "       using a reference Parquet file containing HUC12 codes.\n",
    "\n",
    "    Args:\n",
    "        in_wbd_path (str): Path to the NHD Watershed Boundary Dataset (WBD) \n",
    "                          (e.g., GeoPackage or File Geodatabase).\n",
    "        in_xytab (str, optional): Path to table with XY coordinates. Defaults to None.\n",
    "        lon_col (str, optional): Longitude column name in in_xytab. Defaults to None.\n",
    "        lat_col (str, optional): Latitude column name in in_xytab. Defaults to None.\n",
    "        in_vector (str, optional): Path to vector file. Defaults to None.\n",
    "        in_id_list (list, optional): A pre-defined list of HUC IDs (typically HUC6 codes \n",
    "                                     if bypassing spatial join). Defaults to None. \n",
    "                                     Note: This is ignored if in_xytab or in_vector is provided.\n",
    "        in_refids_parquet (str, optional): Path to reference Parquet file containing HUC12 codes \n",
    "                                           (required for expansion). Defaults to None.\n",
    "        hull (bool, optional): If True and using spatial input, compute convex hull before \n",
    "                               spatial join. Defaults to True.\n",
    "        sjoin_predicate (str, optional): Spatial predicate for the join ('intersects', 'within', etc.). \n",
    "                                         Defaults to 'intersects'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with expanded HUC codes for levels 2, 4, 6, 8, 10, and 12, \n",
    "                          corresponding to the input features or ID list.\n",
    "\n",
    "    Raises:\n",
    "        Depends on the underlying functions: FileNotFoundError, KeyError, ValueError, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Determine initial HUC IDs (Level 6) ---\n",
    "    if in_xytab or in_vector:\n",
    "        # --- Workflow based on spatial input ---\n",
    "        # Format the input spatial data into a GeoDataFrame using the helper function.\n",
    "        # 'hull=True' (default) will create a convex hull polygon before joining.\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab=in_xytab, lon_col=lon_col, lat_col=lat_col, \n",
    "                                         in_vector=in_vector, \n",
    "                                         hull=hull)\n",
    "         \n",
    "        wbdhu6 = gpd.read_file(filename=in_wbd_path, \n",
    "                               layer='WBDHU6',        \n",
    "                               columns=['huc6']    \n",
    "                              )\n",
    "     \n",
    "        # Perform a spatial join to find which HUC6 units match the input geometry.\n",
    "        # Input geometry ('gdf_to_join') is reprojected to match the CRS of the HUC6 layer.\n",
    "        matched_nhd = gpd.sjoin(gdf_to_join.to_crs(crs=wbdhu6.crs), \n",
    "                                wbdhu6, \n",
    "                                how='left', \n",
    "                                predicate=sjoin_predicate)\n",
    "                                \n",
    "        # Extract the list of HUC6 codes found from the spatial join result.\n",
    "        # IMPORTANT: This result overrides any `in_id_list` provided as an argument \n",
    "        # if spatial data (in_xytab or in_vector) was given.\n",
    "        in_id_list = matched_nhd.huc6.values.tolist()\n",
    "         \n",
    "    # --- Step 2: Expand the determined list of HUC IDs to levels 2-12 ---\n",
    "    # This step runs using either the `in_id_list` from the spatial join (if performed)\n",
    "    # or the `in_id_list` provided directly as input (if no spatial data was given).\n",
    "    \n",
    "    # Call the helper function to expand the list of HUC IDs (assumed HUC6) \n",
    "    # to other HUC levels using the reference Parquet file (assumed HUC12).\n",
    "    huc_pd = _expand_basin_idlist(\n",
    "        in_id_list=in_id_list,                \n",
    "        in_refids_parquet=in_refids_parquet, \n",
    "        refids_col='huc12',                  \n",
    "        out_id_range=range(2, 14, 2)          \n",
    "    )\n",
    "\n",
    "    return huc_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f10d4-5016-4ddd-a8fb-728e218b1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nhd_hydronyms(in_hucs: pd.DataFrame,\n",
    "                      in_wbd_path: str,  \n",
    "                      out_dir: str,\n",
    "                      huc_range: range = range(2, 14, 2),\n",
    "                      flatten: bool = True,\n",
    "                      verbose: bool = True\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Retrieves hydronyms (basin names and river names) associated with input HUC units.\n",
    "\n",
    "    Steps:\n",
    "    1. Reads basin names for specified HUC levels from the Watershed Boundary Dataset (WBD).\n",
    "    2. Downloads NHDPlus High Resolution (HR) data for relevant HUC4 units if not already present locally.\n",
    "    3. Extracts significant river names (StreamOrder >= 6 with GNIS names) from the NHDPlus HR flowlines.\n",
    "    4. Returns either a flattened set of all unique hydronyms or a dictionary containing detailed results.\n",
    "\n",
    "    Args:\n",
    "        in_hucs (pd.DataFrame): DataFrame containing HUC columns (e.g., 'huc2', 'huc4', ..., 'huc12').\n",
    "                                Column names must contain the HUC level number (e.g., 'huc6').\n",
    "        in_wbd_path (str): Path to the Watershed Boundary Dataset (WBD) \n",
    "                          (e.g., GeoPackage or File Geodatabase).\n",
    "        out_dir (str): Directory where NHDPlus HR data will be downloaded to/read from.\n",
    "        huc_range (range, optional): Specifies which HUC levels to retrieve basin names for. \n",
    "                                     Defaults to range(2, 14, 2) (HUCs 2, 4, 6, 8, 10, 12).\n",
    "        flatten (bool, optional): If True, returns a single set containing all unique basin and river names. \n",
    "                                  If False, returns a dictionary with the basin DataFrame and river names dict.\n",
    "                                  Defaults to True.\n",
    "        verbose (bool, optional): If True, prints status messages. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        set | dict: If flatten=True, a set of unique hydronym strings.\n",
    "                    If flatten=False, a dictionary:\n",
    "                        {'basins_all_pd': DataFrame with added basin name columns, \n",
    "                         'rivers_huc4_dict': dict mapping HUC4 codes to lists of river names}.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If HUC level cannot be extracted from a column name in `in_hucs`.\n",
    "        FileNotFoundError: If WBD or NHDPlus HR GDB paths are invalid.\n",
    "        KeyError: If expected columns ('huc4', 'huc8', HUC columns matching huc_range) are missing in `in_hucs`.\n",
    "        Exception: Depending on underlying geopandas/pandas/download function errors.\n",
    "    \"\"\"\n",
    "    # Initial status message.\n",
    "    print('Getting NHD basin names')\n",
    "\n",
    "    # --- Stage 1: Get Basin Names from WBD ---\n",
    "    for coln in in_hucs.columns:\n",
    "        # Extract the numeric part (HUC level) from the column name using regex.\n",
    "        huc_len_str = re.sub(r'[a-zA-Z]+', '', coln)\n",
    "        \n",
    "        # Attempt to convert the extracted string to an integer.\n",
    "        try:\n",
    "            huc_len = int(huc_len_str)\n",
    "        except ValueError:\n",
    "            # If conversion fails (e.g., column name has no digits), skip this column.\n",
    "            if verbose:\n",
    "                print(f\"Skipping column '{coln}': Could not extract HUC level.\")\n",
    "            continue # Skip to the next column\n",
    "\n",
    "        if huc_len in huc_range:\n",
    "            layer_name = f'WBDHU{huc_len}'  \n",
    "            huc_col_in_wbd = f'huc{huc_len}'\n",
    "            \n",
    "            if verbose:\n",
    "                 print(f\"Reading {layer_name} to get names for {coln}\")\n",
    "                 \n",
    "            try:\n",
    "                wbd = gpd.read_file(filename=in_wbd_path, \n",
    "                                    layer=layer_name,\n",
    "                                    columns=[huc_col_in_wbd, 'name'], \n",
    "                                    ignore_geometry=True\n",
    "                                   )\n",
    "                \n",
    "                # Merge the basin names into the input DataFrame based on the HUC column.\n",
    "                in_hucs = in_hucs.merge(wbd, \n",
    "                                        left_on=coln,         \n",
    "                                        right_on=huc_col_in_wbd,\n",
    "                                        how='left').\\\n",
    "                                  rename(columns={\"name\": f\"{coln}_name\"})\n",
    "                if coln != huc_col_in_wbd:\n",
    "                    in_hucs = in_hucs.drop(columns=[huc_col_in_wbd])\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle errors during file/layer reading or merging.\n",
    "                print(f\"Warning: Could not read or merge names for {layer_name}. Error: {e}\")\n",
    "\n",
    "    # --- Stage 2: Ensure NHDPlus HR Data is Available ---\n",
    "    # Get the list of unique HUC4 codes present in the input data.\n",
    "    huc4_list = in_hucs.huc4.unique()\n",
    "    # Dictionary to store paths to the downloaded NHDPlus HR GeoDatabases.\n",
    "    nhd_huc4_pathdict = {}\n",
    "    for huc in huc4_list:\n",
    "        download_nhdplus_hr_hu4(\n",
    "            hu4=huc,         \n",
    "            out_dir=out_dir,   \n",
    "            verbose=False \n",
    "        )\n",
    "        nhd_huc4_pathdict[huc] = os.path.join(\n",
    "            out_dir,\n",
    "            f'NHDPLUS_H_{huc}_HU4_GDB.gdb') # Standard NHDPlus HR GDB naming convention.\n",
    "     \n",
    "    # --- Stage 3: Get River Names from NHDPlus HR ---\n",
    "    #NHD FCode values representing streams/rivers and artificial paths to consider.\n",
    "    # 46000: Stream/River\n",
    "    # 46003: Stream/River: Intermittent\n",
    "    # 46006: Stream/River: Perennial\n",
    "    # 46007: Stream/River: Ephemeral\n",
    "    # 55800: Artificial path\n",
    "    fcode_sel_list = [46000, 46006, 46003, 46007, 55800]\n",
    "    # Dictionary to store river names found, keyed by HUC4.\n",
    "    huc4_rivnames_dict = {}\n",
    "\n",
    "    for huc4 in nhd_huc4_pathdict:\n",
    "        if verbose:\n",
    "            print(f\"Processing rivers for HUC4: {huc4}\")\n",
    "            \n",
    "        # Read NHDFlowline and NHDPlusFlowlineVAA tables for the current HUC4.\n",
    "        # suppress specific UserWarnings from pyogrio related to geometry types \n",
    "        # with M values (common in NHD), as geometry is ignored anyway.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pyogrio.raw\")\n",
    "            \n",
    "            try:\n",
    "                # Read flowline attributes\n",
    "                flowlines_gpd = gpd.read_file(\n",
    "                    filename=nhd_huc4_pathdict[huc4], \n",
    "                    layer='NHDFlowline',\n",
    "                    columns=['NHDPlusID', 'ReachCode', 'GNIS_Name', 'FCode'],\n",
    "                    ignore_geometry=True \n",
    "                )\n",
    "                \n",
    "                # Read VAA table attributes\n",
    "                vaa_pd = gpd.read_file(\n",
    "                    filename=nhd_huc4_pathdict[huc4], \n",
    "                    layer='NHDPlusFlowlineVAA',\n",
    "                    columns=['NHDPlusID', 'StreamOrde'],\n",
    "                    ignore_geometry=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: Could not read NHD layers for HUC4 {huc4}. Skipping. Error: {e}\")\n",
    "                 continue\n",
    "\n",
    "        # --- Filter Flowlines to relevant types ---\n",
    "        flowlines_gpd['huc8'] = flowlines_gpd['ReachCode'].str[:8] \n",
    "        \n",
    "        # Get the unique HUC8s present in the input DataFrame that fall within the current HUC4.\n",
    "        huc8_sel = in_hucs[in_hucs['huc4']==huc4]['huc8'].unique()\n",
    "        \n",
    "        # Subset the flowlines to only those within the relevant HUC8s.\n",
    "        flowlines_sub = flowlines_gpd[flowlines_gpd['huc8'].isin(huc8_sel)].\\\n",
    "                        merge(vaa_pd, how='inner', on='NHDPlusID')\n",
    "     \n",
    "        # Apply filters to identify significant named rivers:\n",
    "        # 1. FCode must be in the predefined list of stream/river types.\n",
    "        # 2. Stream Order must be 6 or greater (identifying larger rivers).\n",
    "        # 3. GNIS_Name must not be null (must have a name).\n",
    "        rivnames = flowlines_sub[\n",
    "            (flowlines_sub['FCode'].isin(fcode_sel_list)) \n",
    "            & (flowlines_sub['StreamOrde'] >= 6) \n",
    "            & (flowlines_sub['GNIS_Name'].notna())\n",
    "        ]['GNIS_Name'].unique()\n",
    "\n",
    "        huc4_rivnames_dict[huc4] = rivnames\n",
    "\n",
    "    # --- Stage 4: Format and Return Output ---\n",
    "    if flatten:\n",
    "        # Create a single set containing all unique hydronyms.\n",
    "        basin_name_cols = [f'huc{lev}_name' for lev in huc_range if f'huc{lev}_name' in in_hucs.columns]\n",
    "        basin_names_set = set(pd.melt(in_hucs, value_vars=basin_name_cols)['value'].dropna().unique())\n",
    "        river_names_set = {name for name_list in huc4_rivnames_dict.values() for name in name_list}\n",
    "        all_hydronyms_set = basin_names_set.union(river_names_set)\n",
    "        return(all_hydronyms_set)\n",
    "    else:\n",
    "        #Return Dictionary \n",
    "        out_dict = {}\n",
    "        out_dict['basins_all_pd'] = in_hucs\n",
    "        out_dict['rivers_huc4_dict'] = huc4_rivnames_dict\n",
    "        return(out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadcd495-5c6e-4e25-8dd0-f17f142041e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nhd_data(in_hucs_pd: pd.DataFrame,\n",
    "                 out_dir: str, \n",
    "                 verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Downloads (if necessary) and reads NHDPlus HR flowline and VAA data \n",
    "    for specified HUC4 units, returning a dictionary of merged GeoDataFrames.\n",
    "\n",
    "    Steps:\n",
    "    1. Identifies unique HUC4s from the input DataFrame.\n",
    "    2. Downloads NHDPlus HR data for each unique HUC4 if not already present in out_dir.\n",
    "    3. For each HUC4, reads the NHDFlowline (with geometry) and NHDPlusFlowlineVAA layers.\n",
    "    4. Merges the flowline geometry and attributes with the VAA attributes.\n",
    "    5. Returns a dictionary mapping each HUC4 code to its corresponding merged GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        in_hucs_pd (pd.DataFrame): DataFrame containing a 'huc4' column indicating which \n",
    "                                   HUC4 units' data is needed.\n",
    "        out_dir (str): Directory where NHDPlus HR data will be downloaded to/read from.\n",
    "        verbose (bool, optional): If True, prints status messages (e.g., which HUC4 is being processed). \n",
    "                                  Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are HUC4 codes (str) and values are \n",
    "              GeoDataFrames containing the merged NHDFlowline (geometry & attributes) \n",
    "              and NHDPlusFlowlineVAA (attributes) data for that HUC4.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If NHDPlus HR GDB paths are invalid after attempting download.\n",
    "        KeyError: If the 'huc4' column is missing in `in_hucs_pd`.\n",
    "        Exception: Depending on underlying geopandas/pandas/download function errors.\n",
    "    \"\"\"\n",
    "    # Initial status message.\n",
    "    print('Getting NHD data for all HU4')\n",
    "     \n",
    "    # --- Stage 1: Ensure NHDPlus HR Data is Available ---\n",
    "    huc4_list = in_hucs_pd.huc4.unique()\n",
    "    nhd_huc4_pathdict = {}\n",
    "    for huc in huc4_list:\n",
    "        download_nhdplus_hr_hu4(\n",
    "            hu4=huc,            \n",
    "            out_dir=out_dir,   \n",
    "            verbose=False      \n",
    "        )\n",
    "        # Construct the expected path to the downloaded GeoDatabase and store it.\n",
    "        nhd_huc4_pathdict[huc] = os.path.join(\n",
    "            out_dir,\n",
    "            f'NHDPLUS_H_{huc}_HU4_GDB.gdb') # Standard NHDPlus HR GDB naming.\n",
    "\n",
    "    # --- Stage 2: Read and Merge Data for Each HUC4 ---\n",
    "    nhdplus_huc4_dict = {}\n",
    "    for huc4 in nhd_huc4_pathdict:\n",
    "        if verbose:\n",
    "            print(f\"Processing NHD data for HUC4: {huc4}\")\n",
    "            \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pyogrio.raw\")\n",
    "            \n",
    "            try:\n",
    "                # Read the NHDFlowline layer, including geometry.\n",
    "                flowlines_gpd = gpd.read_file(\n",
    "                    filename=nhd_huc4_pathdict[huc4], \n",
    "                    layer='NHDFlowline',\n",
    "                    ignore_geometry=False # Load the spatial geometry for flowlines.\n",
    "                )\n",
    "                \n",
    "                # Read the NHDPlusFlowlineVAA (Value Added Attributes) table.\n",
    "                vaa_pd = gpd.read_file(\n",
    "                    filename=nhd_huc4_pathdict[huc4], \n",
    "                    layer='NHDPlusFlowlineVAA',\n",
    "                    ignore_geometry=False \n",
    "                )\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: Could not read NHD layers for HUC4 {huc4}. Skipping. Error: {e}\")\n",
    "                 continue # Skip to the next HUC4\n",
    "\n",
    "        # Merge the flowline GeoDataFrame (with geometry) with the VAA DataFrame (attributes).\n",
    "        flowlines_vaa = flowlines_gpd.merge(\n",
    "            vaa_pd, \n",
    "            how='inner',        # Keep only matching NHDPlusIDs.\n",
    "            on='NHDPlusID',     # Join key.\n",
    "            suffixes=('', '_vaa') # Suffix for overlapping column names from VAA.\n",
    "        )\n",
    "        nhdplus_huc4_dict[huc4] = flowlines_vaa\n",
    "     \n",
    "    return(nhdplus_huc4_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4f3a6-241f-4a65-8b14-31ffc195882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoglows_vpu(in_geoglows_vpu_path: str,\n",
    "                     in_xytab: str = None, lon_col: str = None, lat_col: str = None,\n",
    "                     in_vector: str = None, \n",
    "                     in_id_list: list = None, # NOTE: This argument is accepted but not used.\n",
    "                     hull: bool = True, sjoin_predicate: str = 'intersects'):\n",
    "    \"\"\"\n",
    "    Determines the GEOGloWS VPU (Vector Processing Unit) identifiers corresponding \n",
    "    to input spatial data (points or polygons).\n",
    "\n",
    "    This function uses spatial joining to find VPUs that intersect with the provided\n",
    "    input geometry. It does *not* use the `in_id_list` argument YET.\n",
    "\n",
    "    Args:\n",
    "        in_geoglows_vpu_path (str): Path to the GEOGloWS VPU boundaries dataset \n",
    "                                   (e.g., GeoPackage, Shapefile).\n",
    "        in_xytab (str, optional): Path to table with XY coordinates. Defaults to None.\n",
    "        lon_col (str, optional): Longitude column name in in_xytab. Defaults to None.\n",
    "        lat_col (str, optional): Latitude column name in in_xytab. Defaults to None.\n",
    "        in_vector (str, optional): Path to vector file. Defaults to None.\n",
    "        in_id_list (list, optional): Accepted argument but currently unused by this function. \n",
    "                                     Defaults to None.\n",
    "        in_refids_parquet (str, optional): Accepted argument but currently unused by this function. \n",
    "                                          Defaults to None.\n",
    "        hull (bool, optional): If True and using spatial input, compute convex hull of input \n",
    "                               before spatial join. Defaults to True.\n",
    "        sjoin_predicate (str, optional): Spatial predicate for the join ('intersects', 'within', etc.). \n",
    "                                         Defaults to 'intersects'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of VPU identifiers (from the 'VPU' column of the VPU layer) that spatially \n",
    "              match the input data based on the predicate. Returns None if no spatial input \n",
    "              (in_xytab or in_vector) is provided. May contain duplicates or None values if the \n",
    "              left spatial join doesn't find matches for all input features.\n",
    "\n",
    "    Raises:\n",
    "        Depends on the underlying functions: FileNotFoundError, KeyError, ValueError, etc.\n",
    "    \"\"\"\n",
    "     vpu_list = None \n",
    "    \n",
    "    # --- Process only if spatial input (points table or vector file) is provided ---\n",
    "    if in_xytab or in_vector:\n",
    "        # --- Step 1: Format Input Geometry ---\n",
    "        # Standardize the input spatial data into a GeoDataFrame.\n",
    "        # 'hull=True' (default) will create a convex hull polygon of the input features first.\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab=in_xytab, lon_col=lon_col, lat_col=lat_col, \n",
    "                                         in_vector=in_vector, \n",
    "                                         hull=hull)\n",
    "         \n",
    "        # --- Step 2: Read VPU Boundaries ---\n",
    "        vpus = gpd.read_file(filename=in_geoglows_vpu_path, \n",
    "                             layer='vpu-boundaries', \n",
    "                             columns=['VPU']  \n",
    "                            )\n",
    "     \n",
    "        # --- Step 3: Perform Spatial Join ---\n",
    "        matched_vpus = gpd.sjoin(gdf_to_join.to_crs(crs=vpus.crs), # Ensure CRS match\n",
    "                                 vpus, \n",
    "                                 how='left', \n",
    "                                 predicate=sjoin_predicate) \n",
    "                                 \n",
    "        # --- Step 4: Extract Matched VPU IDs ---\n",
    "        # This list might contain duplicates if an input feature overlaps multiple VPUs (unlikely for VPUs)\n",
    "        # It might contain None/NaN if a feature in gdf_to_join (after hull potentially) \n",
    "        # did not match any VPU based on the predicate in the left join.\n",
    "        vpu_list = matched_vpus['VPU'].tolist().unique()\n",
    "\n",
    "    return(vpu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fecc3-3703-40a5-92d2-d7191282f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gadm_lev1_dict(in_gadm_path: str,\n",
    "                       in_xytab: str = None, lon_col: str = None, lat_col: str = None,\n",
    "                       in_vector: str = None, \n",
    "                       in_id_list: list = None, # NOTE: This argument is accepted but not used.\n",
    "                       hull: bool = True, sjoin_predicate: str = 'intersects'):\n",
    "    \"\"\"\n",
    "    Finds GADM level 1 administrative units that spatially match input geometry.\n",
    "\n",
    "    This function takes input spatial data (points or polygons), finds GADM level 1 \n",
    "    administrative units that intersect (or match based on predicate) with it, \n",
    "    and returns a GeoDataFrame containing those matching GADM units along with \n",
    "    attributes from both sources. \n",
    "    It does *not* use the `in_id_list` or `in_refids_parquet` arguments (YET).\n",
    "\n",
    "    Args:\n",
    "        in_gadm_path (str): Path to the GADM dataset (e.g., GeoPackage).\n",
    "        in_xytab (str, optional): Path to table with XY coordinates. Defaults to None.\n",
    "        lon_col (str, optional): Longitude column name in in_xytab. Defaults to None.\n",
    "        lat_col (str, optional): Latitude column name in in_xytab. Defaults to None.\n",
    "        in_vector (str, optional): Path to vector file. Defaults to None.\n",
    "        in_id_list (list, optional): Accepted argument but currently unused by this function. \n",
    "                                     Defaults to None.\n",
    "        in_refids_parquet (str, optional): Accepted argument but currently unused by this function. \n",
    "                                          Defaults to None.\n",
    "        hull (bool, optional): If True and using spatial input, compute convex hull of input \n",
    "                               before spatial join. Defaults to True.\n",
    "        sjoin_predicate (str, optional): Spatial predicate for the join ('intersects', 'within', etc.). \n",
    "                                         Defaults to 'intersects'.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame | None: \n",
    "            If spatial input is provided, returns a GeoDataFrame containing the GADM level 1 \n",
    "            units that spatially match the input geometry (based on the predicate). \n",
    "            The geometry in the returned GeoDataFrame is that of the GADM units. \n",
    "            Attributes from both the GADM layer and the input data are included.\n",
    "            Returns None if no spatial input (in_xytab or in_vector) is provided.\n",
    "            Note: The function name suggests a Dictionary return type, but it returns a GeoDataFrame.\n",
    "\n",
    "    Raises:\n",
    "        Depends on the underlying functions: FileNotFoundError, KeyError, ValueError, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    matched_adm_units = None\n",
    "    \n",
    "    # --- Process only if spatial input (points table or vector file) is provided ---\n",
    "    if in_xytab or in_vector:\n",
    "        # Standardize the input spatial data into a GeoDataFrame \n",
    "        # 'hull=True' (default) will create a convex hull polygon of the input features first.\n",
    "        gdf_to_join = _format_gdf_tojoin(in_xytab=in_xytab, lon_col=lon_col, lat_col=lat_col, \n",
    "                                         in_vector=in_vector, \n",
    "                                         hull=hull)\n",
    "         \n",
    "        gadm_gpd = gpd.read_file(filename=in_gadm_path, layer='ADM_1')\n",
    "     \n",
    "        # Spatially join the GADM level 1 units with the input geometry.\n",
    "        matched_adm_units = gpd.sjoin(\n",
    "            gadm_gpd, # Keep geometry and attributes from GADM units...\n",
    "            gdf_to_join.to_crs(crs=gadm_gpd.crs), \n",
    "            how='inner',\n",
    "            predicate=sjoin_predicate\n",
    "        )\n",
    "\n",
    "    return (matched_adm_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755649e-dd8e-4dfa-89db-4e0bf4091f7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run functions\n",
    "hu12_list = create_huc12_list(wbd_path, hu12_parquet)\n",
    "\n",
    "basinatlas11_list = create_basinatlas11_list(\n",
    "    basinatlas_path, \n",
    "    basinatlas11_parquet)\n",
    "\n",
    "# test_huc_pd = get_matching_NHD_HU(\n",
    "#     in_wbd_path=wbd_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     in_refids_parquet=hu12_parquet,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )\n",
    "# #print(test_huc_pd)\n",
    "# #in_id_list = in_umrb_huc4s = [f'07{str(i).zfill(2)}' for i in range(2,15)]\n",
    "\n",
    "# test_nhd_hydronyms = get_nhd_hydronyms(\n",
    "#     in_hucs=test_huc_pd,\n",
    "#     in_wbd_path=wbd_path,\n",
    "#     out_dir = os.path.join(nhd_dir, 'nhdplus_hr'),\n",
    "#     huc_range=[2, 4, 6],\n",
    "#     verbose=False\n",
    "# )\n",
    "# len(test_nhd_hydronyms)\n",
    "\n",
    "# test_pfaf_pd = get_matching_hydrobasin(\n",
    "#     in_basinatlas_path=basinatlas_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     #in_id_list=None, \n",
    "#     in_refids_parquet=basinatlas11_parquet,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )\n",
    "# #print(test_pfaf_pd)\n",
    "\n",
    "# test_pfaf_pd_idlist = get_matching_hydrobasin(\n",
    "#     in_basinatlas_path=basinatlas_path,\n",
    "#     in_id_list=[742873, 742875, 742876], \n",
    "#     in_refids_parquet=basinatlas11_parquet\n",
    "# )\n",
    "# #print(test_pfaf_pd_idlist)\n",
    "\n",
    "\n",
    "# test_vpu_list = get_geoglows_vpu(\n",
    "#     in_geoglows_vpu_path=geoglows_vpu_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )\n",
    "# print(test_vpu_list)\n",
    "\n",
    "# test_gadm_lev1 = get_gadm_lev1_dict(\n",
    "#     in_gadm_path=gadm_path,\n",
    "#     in_vector=test_pts_path,\n",
    "#     hull=True,\n",
    "#     sjoin_predicate='intersects'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16249f97-182e-45d5-84ec-a8955c1c4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "################IN DEVELOPMENT ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1fdcd-b9ba-4942-9af3-904dbfeed05a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_geoglows_hydronyms(in_geoglows_path, verbose=True):\n",
    "\n",
    "    country_tab_path = os.path.join(in_geoglows_path, \n",
    "                                    'tables', 'v2-countries-table.parquet')\n",
    "    country_pd = pd.read_parquet(country_tab_path)\n",
    "\n",
    "    meta_tab_path = os.path.join(in_geoglows_path, \n",
    "                                    'tables', 'package-metadata-table.parquet')\n",
    "    meta_pd = pd.read_parquet(meta_tab_path)\n",
    "\n",
    "    model_tab_path = os.path.join(in_geoglows_path, \n",
    "                                    'tables', 'v2-model-table.parquet')\n",
    "    model_pd = pd.read_parquet(model_tab_path)\n",
    "\n",
    "    print('Getting geoglows river names')\n",
    "    for vpu in test_vpu_list[0]:\n",
    "        streams_gpd= gpd.read_file(\n",
    "                filename=nhd_huc4_pathdict[huc4], \n",
    "                layer='NHDFlowline',\n",
    "                ignore_geometry=False\n",
    "        )\n",
    "          \n",
    "# geoglows_path = os.path.join(datdir, 'geoglows')\n",
    "# get_geoglows_hydronyms(in_geoglows_path=geoglows_path, verbose=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
