{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c4b515-c85f-4f97-940b-03e306476595",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Utility functions used throughout analysis\n",
    "\n",
    "Import key libraries, defines utility functions for reading and wrangling files, including .ris and .bib files, get zotero collection from account, create and search for combinations of regex patterns for literature searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507c6c-28d8-4efd-8435-bd2ca38718ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import collections\n",
    "from datetime import datetime, UTC\n",
    "from inspect import getsourcefile\n",
    "import itertools\n",
    "import litstudy  # Use pip install git+https://github.com/NLeSC/litstudy to download dev version\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pyzotero import zotero\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "from typing import List, Union, Dict, Any, Optional\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15eb05c-daaf-43a0-9ea7-2046e1aab61f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regex_list_files(in_dir: Union[str, Path],\n",
    "                     in_pattern: str,\n",
    "                     full_path: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lists files in a directory matching a regular expression.\n",
    "\n",
    "    Args:\n",
    "        in_dir: The directory to search. Can be a string path or a pathlib.Path object.\n",
    "        in_pattern: The regular expression pattern to match filenames against.\n",
    "        full_path: Whether to return full paths (True) or just filenames (False).\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, either full paths or filenames, of files matching the pattern.\n",
    "        Returns an empty list if no matches are found or if an error occurs.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: if input arguments are of incorrect type\n",
    "        ValueError: if input directory does not exist\n",
    "        re.error:  If the regular expression pattern is invalid.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Type Validation ---\n",
    "    if not isinstance(in_dir, (str, Path)):\n",
    "        raise TypeError(\"in_dir must be a string or pathlib.Path object.\")\n",
    "    if not isinstance(in_pattern, str):\n",
    "        raise TypeError(\"in_pattern must be a string.\")\n",
    "    if not isinstance(full_path, bool):\n",
    "        raise TypeError(\"full_path must be a boolean.\")\n",
    "\n",
    "    # --- Convert to Path object for consistency ---\n",
    "    if isinstance(in_dir, str):\n",
    "        in_dir = Path(in_dir)\n",
    "\n",
    "    # --- Input Value Validation ---\n",
    "    if not in_dir.is_dir():\n",
    "        raise ValueError(f\"The directory '{in_dir}' does not exist.\")\n",
    "\n",
    "    # --- Regex Compilation (with error handling) ---\n",
    "    try:\n",
    "        regex = re.compile(in_pattern)\n",
    "    except re.error as e:\n",
    "        raise re.error(f\"Invalid regular expression pattern: {e}\")\n",
    "\n",
    "    file_list = []\n",
    "    for root, _, files in os.walk(in_dir):  # os.walk works with Path objects\n",
    "        for file in files:\n",
    "            if regex.match(file):\n",
    "                if full_path:\n",
    "                    # Use .joinpath for consistent path construction with Path objects\n",
    "                    file_list.append(str(Path(root).joinpath(file)))  # Convert to string for consistent return type\n",
    "                else:\n",
    "                    file_list.append(file)\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652c7330-518c-4905-8e02-1ebeb02583dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rpickle_bibdocset(in_dirpath: Path, \n",
    "                      in_pattern: str, \n",
    "                      out_pickle: Path) -> list:\n",
    "    \"\"\"Loads BibTeX files matching a pattern, combines them, and pickles the result.\n",
    "\n",
    "    If the output pickle file already exists, it loads the data from the pickle\n",
    "    instead of reprocessing the BibTeX files.\n",
    "\n",
    "    Args:\n",
    "        in_dirpath: Path to the directory containing BibTeX files.\n",
    "        in_pattern: Regex pattern to match filenames within in_dirpath.\n",
    "        out_pickle: Path to the output pickle file where the combined list\n",
    "                    of references will be saved/loaded from.\n",
    "\n",
    "    Returns:\n",
    "        A list of reference objects loaded from the BibTeX files or pickle file.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If in_dirpath does not exist.\n",
    "        # Add other potential exceptions from litstudy or pickle\n",
    "    \"\"\"\n",
    "    if not out_pickle.exists():\n",
    "        compiled_pattern = re.compile(in_pattern)\n",
    "        if not in_dirpath.is_dir():\n",
    "           raise FileNotFoundError(f\"Input directory not found: {in_dirpath}\")\n",
    "\n",
    "        # Get list of every bib file\n",
    "        bib_filepaths = [p for p in in_dirpath.glob('*') \n",
    "                         if p.is_file() and compiled_pattern.match(p.name)] \n",
    "        \n",
    "        # Read bib files from first scoping and join them (takes ~15-20 sec/1000 refs)\n",
    "        reflist = []\n",
    "        try:\n",
    "            for bib_path in bib_filepaths: \n",
    "                reflist.extend(litstudy.load_bibtex(bib_path)) \n",
    "        except Exception as e: \n",
    "            print(f\"Error processing file {bib_path}: {e}\")\n",
    "\n",
    "        # Pickle them (save the full document set as a binary file on disk that can be easily retrieved)\n",
    "        try:\n",
    "            with open(out_pickle, 'wb') as f:\n",
    "                pickle.dump(reflist, f)\n",
    "        except (IOError, pickle.PicklingError) as e:\n",
    "            print(f\"Error pickling data to {out_pickle}: {e}\")\n",
    "    else:\n",
    "        # Read pre-saved document set\n",
    "        try:\n",
    "            with open(out_pickle, 'rb') as f:\n",
    "                reflist = pickle.load(f)\n",
    "        except (IOError, pickle.UnpicklingError, EOFError) as e:\n",
    "             print(f\"Error unpickling data from {out_pickle}: {e}\")\n",
    "    return reflist\n",
    "\n",
    "\n",
    "def get_zotero_collection_titles_dois(library_id: int | str, \n",
    "                 api_key_path: Path, \n",
    "                 collection_name: str) -> collections.defaultdict[str, list]:\n",
    "    \"\"\"Fetches item keys, titles, and DOIs from a specific Zotero collection.\n",
    "\n",
    "    Args:\n",
    "        library_id: The Zotero library ID (numeric or potentially username).\n",
    "        api_key_path: Path to a file containing the Zotero API key.\n",
    "        collection_name: The exact name of the Zotero collection.\n",
    "\n",
    "    Returns:\n",
    "        A defaultdict where keys are Zotero item keys and values are lists\n",
    "        containing [title, DOI (or np.nan if missing)].\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If api_key_path does not exist.\n",
    "        ValueError: If the specified collection name is not found.\n",
    "        # Add potential exceptions from zotero library (e.g., connection errors)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_key = api_key_path.read_text().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"API key file not found: {api_key_path}\")\n",
    "        raise # Re-raise the exception\n",
    "\n",
    "    zot = zotero.Zotero(library_id=library_id,\n",
    "                      library_type='group', \n",
    "                      api_key=api_key)\n",
    "    try:\n",
    "        # This is slightly complex, could be a helper function or loop\n",
    "        collection = next((col for col in zot.collections_top() if col['data']['name'] == collection_name), None)\n",
    "        if collection is None:\n",
    "            raise ValueError(f\"Collection '{collection_name}' not found in Zotero library {library_id}.\")\n",
    "        testlist_colID = collection['key'] \n",
    "    except Exception as e: \n",
    "        print(f\"Error fetching Zotero collections: {e}\")\n",
    "        raise \n",
    "\n",
    "    try:\n",
    "        testlist_items = zot.everything(zot.collection_items_top(testlist_colID))\n",
    "    except Exception as e: # Catch specific zotero exceptions\n",
    "        print(f\"Error fetching items from collection ID {testlist_colID}: {e}\")\n",
    "        raise\n",
    "\n",
    "    testlist_title_dois = collections.defaultdict(list)\n",
    "    for ref in testlist_items:\n",
    "        item_key = ref['key']\n",
    "        title = ref.get('data', {}).get('title', 'N/A') # Provide default if missing\n",
    "        doi = ref.get('data', {}).get('DOI', np.nan)\n",
    "\n",
    "        testlist_title_dois[item_key].append(title)\n",
    "        testlist_title_dois[item_key].append(doi)\n",
    "        \n",
    "    return testlist_title_dois\n",
    "\n",
    "# Get all DOIs and titles in references returned from search\n",
    "def tabulate_searchlist(in_reflist, out_csvpath):\n",
    "    if not out_csvpath.exists():\n",
    "        reflist_dict = {}\n",
    "        for i, ref in enumerate(in_reflist):\n",
    "            reflist_dict[i] = [re.sub(r\"[^a-zA-Z\\d\\s]\", \"\", \n",
    "                                      ref.title.replace('\\n', ' ').lower()),\n",
    "                               ref.publication_source,\n",
    "                               ref.publication_year, ref.abstract]\n",
    "            if 'doi' in ref.entry:\n",
    "                reflist_dict[i].append(ref.entry['doi'])\n",
    "            else:\n",
    "                reflist_dict[i].append(np.nan)\n",
    "\n",
    "        reflist_pd = pd.DataFrame.from_dict(reflist_dict, orient='index')\n",
    "        reflist_pd.columns = ['title', 'source', 'year', 'abstract', 'doi']\n",
    "        reflist_pd.to_csv(out_csvpath)\n",
    "    else:\n",
    "        reflist_pd = pd.read_csv(out_csvpath)\n",
    "    return reflist_pd\n",
    "\n",
    "# Erite string y to file x\n",
    "def write(x, y):\n",
    "    with open(x, 'a') as f:\n",
    "        f.write(y)\n",
    "        f.write('\\n')\n",
    "    return _\n",
    "\n",
    "\n",
    "def tabulate_searchlist(in_reflist: list, out_csvpath: Path) -> pd.DataFrame:\n",
    "    \"\"\"Converts a list of reference objects to a Pandas DataFrame and saves/loads it as CSV.\n",
    "\n",
    "    Extracts title, source, year, abstract, and DOI. Cleans the title.\n",
    "    If the output CSV file exists, it loads the DataFrame from the CSV.\n",
    "\n",
    "    Args:\n",
    "        in_reflist: A list of reference objects (assuming attributes like title, \n",
    "                    publication_source, publication_year, abstract, entry['doi']).\n",
    "        out_csvpath: Path to the CSV file to save/load the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame containing the tabulated reference data.\n",
    "    \"\"\"\n",
    "    if not out_csvpath.exists():\n",
    "        processed_refs = []\n",
    "        for i, ref in enumerate(in_reflist):\n",
    "            try:\n",
    "                #Remove line breaks, convert to lower case\n",
    "                cleaned_title = re.sub(r\"[^a-zA-Z\\d\\s]\", \"\", ref.title.replace('\\n', ' ')).lower()\n",
    "                \n",
    "                doi = ref.entry.get('doi', np.nan) if hasattr(ref, 'entry') and isinstance(ref.entry, dict) else np.nan\n",
    "\n",
    "                processed_refs.append({\n",
    "                    'id': i, \n",
    "                    'title': cleaned_title,\n",
    "                    'source': getattr(ref, 'publication_source', None), \n",
    "                    'year': getattr(ref, 'publication_year', None),   \n",
    "                    'abstract': getattr(ref, 'abstract', None),     \n",
    "                    'doi': doi\n",
    "                })\n",
    "            except AttributeError as e:\n",
    "                print(f\"Warning: Skipping reference {i} due to missing attribute: {e}\")\n",
    "                continue # Skip this reference\n",
    "\n",
    "        # Convert list of dicts to DataFrame\n",
    "        reflist_pd = pd.DataFrame(processed_refs)\n",
    "        \n",
    "        # CSV writing\n",
    "        try:\n",
    "            reflist_pd.to_csv(out_csvpath, index=False)\n",
    "        except IOError as e:\n",
    "            print(f\"Error writing CSV to {out_csvpath}: {e}\")\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            reflist_pd = pd.read_csv(out_csvpath)\n",
    "        except (IOError, pd.errors.EmptyDataError) as e:\n",
    "            print(f\"Error reading CSV from {out_csvpath}: {e}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Unexpected error reading CSV {out_csvpath}: {e}\")\n",
    "\n",
    "    return reflist_pd\n",
    "\n",
    "def combine_2w_regex(pattern1: str, \n",
    "                     pattern2: str, \n",
    "                     precede: bool = False) -> str:\n",
    "    \"\"\"Builds a regex pattern to find two words separated by non-word characters.\n",
    "\n",
    "    Args:\n",
    "        pattern1: The first word/pattern.\n",
    "        pattern2: The second word/pattern.\n",
    "        require_order: If True, pattern1 must precede pattern2. \n",
    "                       If False (default), finds either order (pattern1..pattern2 or pattern2..pattern1).\n",
    "\n",
    "    Returns:\n",
    "        The compiled regex pattern string.\n",
    "    \"\"\"\n",
    "    p1_p2 = rf\"{pattern1}\\W*{pattern2}\\b\"\n",
    "    \n",
    "    if precede:\n",
    "        regexp = p1_p2\n",
    "    else:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W*{pattern1}\\\\b)\"\n",
    "    return regexp\n",
    "\n",
    "# Count number a times a simple 2-pattern group occurs in text\n",
    "def find_2w_regex(text: str, \n",
    "                  pattern1: str, \n",
    "                  pattern2: str, \n",
    "                  precede: bool = False) -> int:\n",
    "    \"\"\"Counts occurrences of a two-word pattern in text using regex.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search within.\n",
    "        pattern1: The first word/pattern.\n",
    "        pattern2: The second word/pattern.\n",
    "        require_order: If True, pattern1 must precede pattern2. \n",
    "                       If False (default), finds either order.\n",
    "\n",
    "    Returns:\n",
    "        The number of times the pattern occurs.\n",
    "    \"\"\"\n",
    "    regexp_pattern = build_two_word_regex(pattern1, pattern2, precede)\n",
    "    \n",
    "    matches = re.findall(regexp_pattern, text, flags=re.IGNORECASE) #case-insensitivity\n",
    "    match_count = len(matches)\n",
    "    return match_count\n",
    "\n",
    "# Count number a times a simple pattern occurs in text\n",
    "def find_regex(text: str, pattern: str) -> int:\n",
    "    \"\"\"Counts occurrences of a regex pattern in text.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search within.\n",
    "        pattern: The regex pattern string.\n",
    "\n",
    "    Returns:\n",
    "        The number of non-overlapping occurrences of the pattern.\n",
    "    \"\"\"\n",
    "    matches = re.findall(rf\"{pattern}\", text, flags=re.IGNORECASE) \n",
    "    return len(matches)\n",
    "\n",
    "# Join all strings in a list with | signs and parentheses\n",
    "def recomb(in_str: Union[List[str], str], \n",
    "           recomb_sep: str = '|') -> str:\n",
    "    \"\"\"Builds a regex OR group from a list of strings, or returns the string itself.\n",
    "\n",
    "    Example: ['a', 'b'] -> r'((a)|(b))' \n",
    "             'abc' -> 'abc'\n",
    "\n",
    "    Args:\n",
    "        items: A list of strings (patterns) or a single string.\n",
    "        separator: The separator to use between patterns in the OR group (default '|').\n",
    "\n",
    "    Returns:\n",
    "        A regex string representing the OR group, or the original string if not a list.\n",
    "    \"\"\"\n",
    "    if isinstance(in_str, list):\n",
    "        inner_patterns = f\"({recomb_sep.join(f'({w})' for w in in_str)})\"\n",
    "        return rf\"({inner_patterns})\" # Outer group captures the whole match\n",
    "    else:\n",
    "        return in_str\n",
    "\n",
    "# Find patterns in text based on search dictionary\n",
    "def combo_refind(in_searchdict: dict, text: str) -> int | None:\n",
    "        \"\"\"Searches text based on patterns defined in a dictionary. \n",
    "\n",
    "    Args:\n",
    "        in_searchdict: A dictionary where values define search patterns. \n",
    "                           Expected structure (example):\n",
    "                           { 'key1': ('with', ('patternA', 'patternB')),\n",
    "                             'key2': ('pre', ('patternC', 'patternD')),\n",
    "                             'key3': ('single', 'patternE'), # Using 'single' instead of None\n",
    "                             'key4': 'patternF' # Assuming this means single pattern search\n",
    "                           }\n",
    "        text: The text to search within.\n",
    "\n",
    "    Returns:\n",
    "        The count of matches found for the first applicable definition, or None if \n",
    "        no definition is processed correctly or an unknown type is encountered early.\n",
    "    \"\"\"\n",
    "    for regexp_combo in in_searchdict.values():\n",
    "        if regexp_combo[0] == 'with':\n",
    "            k = find_2w_regex(text, \n",
    "                              regexp_combo[1][0], \n",
    "                              regexp_combo[1][1], \n",
    "                              precede=False)\n",
    "        elif regexp_combo[0] == 'pre':\n",
    "            k = find_2w_regex(text,\n",
    "                              regexp_combo[1][0], \n",
    "                              regexp_combo[1][1], \n",
    "                              precede=True)\n",
    "        elif regexp_combo[0] is None:\n",
    "            k = find_regex(text, regexp_combo)\n",
    "        else:\n",
    "            break\n",
    "        return k\n",
    "\n",
    "# Generate n-grams from DOI\n",
    "#CHECK OUT: from pattern.en import ngrams\n",
    "#print(ngrams(\"He goes to hospital\", n=2))\n",
    "def DOI_ngram(A):\n",
    "    count0 = collections.Counter()\n",
    "    s1 = A[0].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count0 = count0 + (collections.Counter(every))\n",
    "    count0 = count0.most_common()\n",
    "\n",
    "    count1 = collections.Counter()\n",
    "    s1 = A[1].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count1 = count1 + (collections.Counter(every))\n",
    "    count1 = count1.most_common()\n",
    "\n",
    "    count2 = collections.Counter()\n",
    "    for idx, i in enumerate(A[2]):\n",
    "        x = collections.Counter([l.lower() for l in i])\n",
    "        count2 += x\n",
    "    count2 = count2.most_common()\n",
    "\n",
    "    count3 = count0 + count1 + count2\n",
    "    return count3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6256bd67-dd76-4a87-9d5b-d05ce5edb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dataframe(\n",
    "    list_of_dicts: List[Dict[str, Any]], \n",
    "    keys_to_keep: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a list of dictionaries to a Pandas DataFrame, keeping a specified\n",
    "    subset of keys as columns.  Handles missing keys gracefully.\n",
    "\n",
    "    Args:\n",
    "        list_of_dicts: The list of dictionaries.\n",
    "        keys_to_keep: An optional list of keys to keep as columns.\n",
    "            If None, attempts to use all keys present in *any* of the\n",
    "            dictionaries, but prioritizes keys from the *first* dictionary\n",
    "            if there are inconsistencies.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: if input is not a list or contains non-dict elements.\n",
    "        ValueError: if keys_to_keep is provided but is empty.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(list_of_dicts, list):\n",
    "        raise TypeError(\"Input 'list_of_dicts' must be a list of dictionaries.\")\n",
    "    if not all(isinstance(item, dict) for item in list_of_dicts):\n",
    "        raise TypeError(\"All elements in 'list_of_dicts' must be dictionaries.\")\n",
    "    if keys_to_keep is not None and not isinstance(keys_to_keep, list):\n",
    "        raise TypeError(\"'keys_to_keep' must be a list of strings or None.\")\n",
    "    if keys_to_keep is not None and len(keys_to_keep) == 0:\n",
    "        raise ValueError(\"'keys_to_keep' cannot be an empty list.\")\n",
    "\n",
    "\n",
    "    if keys_to_keep is None:\n",
    "        # Attempt to use all keys, prioritizing the first dictionary\n",
    "        if not list_of_dicts:  # Handle empty input list\n",
    "            return pd.DataFrame()\n",
    "        keys_to_keep = list(list_of_dicts[0].keys())  # Start with keys from first dict\n",
    "        # Add any keys present in *other* dicts but missing from the first.\n",
    "        for item in list_of_dicts:\n",
    "            for key in item:\n",
    "                if key not in keys_to_keep:\n",
    "                    keys_to_keep.append(key)\n",
    "    \n",
    "    # Create the DataFrame, handling missing keys\n",
    "    df_data = []\n",
    "    for item in list_of_dicts:\n",
    "        row = {key: item.get(key, None) for key in keys_to_keep}\n",
    "        df_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(df_data, columns=keys_to_keep)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146646f1-ca5f-496c-a665-62758f23e350",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Function to export OpenAlex works to RIS\n",
    "%run lib/openalex_formatter_ris.py\n",
    "\n",
    "def export_oalex_works_to_ris(\n",
    "    works: List[Dict[str, Any]], \n",
    "    filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Export a list of OpenAlex works to an RIS file, including the abstract.\n",
    "\n",
    "    Args:\n",
    "        works: List of OpenAlex work dictionaries.\n",
    "        filename: The name of the RIS file to be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(works, list):\n",
    "        raise TypeError(\"works must be a list of dictionaries.\")\n",
    "    if not isinstance(filename, str):\n",
    "        raise TypeError(\"filename must be a string.\")\n",
    "    if not filename.endswith(\".ris\"):\n",
    "        logging.warning(\"Filename does not end with '.ris'.  This may cause problems with some RIS readers.\")\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as ris_file:\n",
    "            for work in works:\n",
    "                if not isinstance(work, dict):\n",
    "                    print(f\"Skipping invalid work entry (not a dictionary): {work}\")\n",
    "                    continue\n",
    "\n",
    "                ris_entry = build_ris_entry(work)\n",
    "                ris_file.write(ris_entry)\n",
    "\n",
    "                # # --- Article type ----\n",
    "                # ris_file.write(\"TY  - JOUR\\n\")  # Type of reference (Journal Article)\n",
    "                \n",
    "                # # --- Title ---\n",
    "                # title = work.get('title', '') or ''  # Handle None or missing title\n",
    "                # ris_file.write(f\"TI  - {title}\\n\")\n",
    "\n",
    "                # # --- Authors ---\n",
    "                # # Handle authors correctly.  OpenAlex stores authors as a list of dictionaries.\n",
    "                # authors = work.get('authorships', []) or []\n",
    "                # for author_data in authors:\n",
    "                #     author_name = author_data.get('author', {}).get('display_name', '') or ''\n",
    "                #     if author_name:\n",
    "                #         ris_file.write(f\"AU  - {author_name}\\n\")\n",
    "\n",
    "                # # --- Publication Year ---\n",
    "                # year = work.get('publication_year', '') or ''\n",
    "                # ris_file.write(f\"PY  - {year}\\n\")\n",
    "\n",
    "                # # --- Journal (Source) ---\n",
    "                # #  OpenAlex stores source information in 'primary_location' and 'locations'.\n",
    "                # source_title = ''\n",
    "                # if work.get('primary_location') and work.get('primary_location').get('source'):\n",
    "                #     source_title = work['primary_location']['source'].get('display_name', '') or ''\n",
    "                # elif work.get('locations'):\n",
    "                #     for location in work['locations']:\n",
    "                #         if location.get('source'):\n",
    "                #             source_title = location['source'].get('display_name', '') or ''\n",
    "                #             break # Use the first available location.\n",
    "\n",
    "                # ris_file.write(f\"JO  - {source_title}\\n\")\n",
    "\n",
    "                # # --- Volume, Issue, Pages ---\n",
    "                # volume = work.get('volume', '') or ''\n",
    "                # issue = work.get('issue', '') or ''\n",
    "                # #  Pages can be in 'biblio' or as separate 'page_start', 'page_end'\n",
    "                # start_page = work.get('page_start', '') or ''\n",
    "                # end_page = work.get('page_end', '') or ''\n",
    "                # if not start_page and work.get('biblio'):\n",
    "                #     start_page = work['biblio'].get('first_page', '') or ''\n",
    "                #     end_page = work['biblio'].get('last_page', '') or ''\n",
    "                \n",
    "                # ris_file.write(f\"VL  - {volume}\\n\")\n",
    "                # ris_file.write(f\"IS  - {issue}\\n\")\n",
    "                # ris_file.write(f\"SP  - {start_page}\\n\")\n",
    "                # ris_file.write(f\"EP  - {end_page}\\n\")\n",
    "\n",
    "                # # --- DOI ---\n",
    "                # doi = work.get('doi', '') or ''  # Get DOI directly.  It's a top-level field.\n",
    "                # ris_file.write(f\"DO  - {doi}\\n\")\n",
    "\n",
    "                # # --- Abstract ---\n",
    "                # abstract = work['abstract']\n",
    "                # if abstract:  # Only write abstract if it exists\n",
    "                #     ris_file.write(f\"AB  - {abstract}\\n\")\n",
    "\n",
    "\n",
    "                # ris_file.write(\"ER  - \\n\\n\")  # End of reference\n",
    "\n",
    "    except (IOError, OSError) as e:\n",
    "        raise OSError(f\"Error writing to RIS file: {e}\")\n",
    "    except Exception as e: #Catch remaining exceptions\n",
    "        raise Exception(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
