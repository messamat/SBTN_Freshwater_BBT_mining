{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459f9238-a7fb-4b5c-bb60-e52ee70f2115",
   "metadata": {},
   "source": [
    "# Create search strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c60fd3a-61cb-4250-9eca-8f3f07bc16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\messa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run lit_utility_functions_2025.ipynb\n",
    "\n",
    "import bream\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "from pyinflect import getInflection\n",
    "import spacy\n",
    "from textblob import Word\n",
    "\n",
    "\n",
    "# Load NLTK resources\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "# Load spaCy model\n",
    "try:\n",
    "    spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading en_core_web_sm model...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c96c664-2429-433b-8efa-e3e713eba316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_words(pattern, bound_pattern=True):\n",
    "    \"\"\"\n",
    "      Finds all English words that match a given regular expression pattern.\n",
    "      Args:\n",
    "        pattern: The regular expression pattern (string).\n",
    "      Returns:\n",
    "        A list of English words that match the pattern.  Returns an empty list if no\n",
    "        words match or if there's an invalid regex pattern.  Prints a warning if the\n",
    "        NLTK words corpus is not found.\n",
    "    \"\"\"\n",
    "    if bound_pattern:\n",
    "        pattern = f'^{pattern}$'\n",
    "    regex = re.compile(pattern)\n",
    "    english_words = words.words()\n",
    "    matching_words = [word for word in english_words if regex.search(word)]\n",
    "\n",
    "    return matching_words\n",
    "\n",
    "\n",
    "def combinate_concats(\n",
    "    prefixes, suffixes, separators=[\" \", \"-\", \"\"], add_quotes=False):\n",
    "    \"\"\"\n",
    "    Generates all combinations using itertools.product (most efficient).\n",
    "    \"\"\"\n",
    "    if add_quotes:\n",
    "        combinations = [\n",
    "            '\"' + \"\".join(combination) + '\"'\n",
    "            for combination in itertools.product(prefixes, separators, suffixes)\n",
    "        ]\n",
    "    else:\n",
    "        combinations = [\n",
    "            \"\".join(combination)\n",
    "            for combination in itertools.product(prefixes, separators, suffixes)\n",
    "        ]\n",
    "    return combinations\n",
    "\n",
    "def plural_form_exists(word, in_lemmatizer):\n",
    "    \"\"\"\n",
    "    Checks if a plausible plural form of a word exists, with improved logic\n",
    "    and handling of irregular plurals.  Uses WordNet and a rule-based fallback.\n",
    "\n",
    "    Args:\n",
    "      word: The word (string) to check.\n",
    "\n",
    "    Returns:\n",
    "      True if a plausible plural form is found, False otherwise.\n",
    "    \"\"\"\n",
    "    # 1. Check if the word is already plural (common case):\n",
    "    if wordnet.synsets(word) and any(lemma.name().endswith('s') \n",
    "                                     for synset in wordnet.synsets(word)\n",
    "                                     for lemma in synset.lemmas()):\n",
    "       return True\n",
    "\n",
    "    # 2. Lemmatize the word (get the singular form):\n",
    "    lemma = in_lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "\n",
    "\n",
    "    #3. Check if word is the lemma (if so, it is most likely singular)\n",
    "    if lemma != word:\n",
    "        return True #Word is not the lemma (it's likely already a plural form)\n",
    "    \n",
    "    # 4. If lemma and word are the same, then add an s and try again with wordnet\n",
    "    if wordnet.synsets(word + 's'):\n",
    "        return True\n",
    "    \n",
    "    # 5. Try common plural endings\n",
    "    if word.endswith((\"s\", \"x\", \"z\", \"ch\", \"sh\")):\n",
    "        plural = word + \"es\"\n",
    "    elif word.endswith(\"y\") and len(word) > 1 and word[-2] not in \"aeiou\":\n",
    "        plural = word[:-1] + \"ies\"\n",
    "    else:\n",
    "        plural = word + \"s\"\n",
    "    \n",
    "    if wordnet.synsets(plural):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "    \n",
    "def textblob_pluralize(word):\n",
    "    w = Word(word)\n",
    "    return w.pluralize()\n",
    "\n",
    "def get_spelling_variants(word):\n",
    "    \"\"\"Gets American and British spelling variants of a word using bream.\"\"\"\n",
    "    variants = set()\n",
    "    variants.add(word)  # Add the original word\n",
    "\n",
    "    try:\n",
    "      #The following lines will generate errors if the words are not in the\n",
    "        #dictionary. We capture these.\n",
    "        american = bream.to_american(word)\n",
    "        variants.add(american)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        british = bream.to_british(word)\n",
    "        variants.add(british)\n",
    "    except:\n",
    "      pass\n",
    "    return list(variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08690266-641a-4901-9685-06739712e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generic_search_terms():\n",
    "    combo1_1 = [\"ecologic\\\\S*\", [\"eco\", \"hydrologic\\\\S*\"], \n",
    "                [\"hydro\", \"ecologic\\\\S*\"],\n",
    "                'environmental', 'minim\\\\S\\\\S', 'acceptable',\n",
    "                'augmented', 'augmentation', 'compensation', \n",
    "                'experimental', 'flushing', ['in', 'stream'], 'maintenance',\n",
    "                'optimum', 'restorati\\\\S{2}']\n",
    "\n",
    "    combo1_2 = ['flood', 'flow', ['water', 'level'], 'discharge']\n",
    "    \n",
    "    combo2_1 = ['compensat[a-z]{1,3}', 'conservation', 'cultural', ['cut', 'off'], \n",
    "                'design', 'fish', 'functional', 'indigenous', 'limit', 'maintenance',\n",
    "                'management', 'maximum', 'natural', 'preference', \n",
    "                'protection', 'rating', 'regime[a-z]{0,1}', 'residual',\n",
    "                'right', 'sanita(ry|tion)', 'scenario', 'standard', \n",
    "                'suitable', 'surplus', 'sustainable', 'threshold',\n",
    "                'use', 'vital']\n",
    "    combo2_2 = ['flow']\n",
    "    \n",
    "    combo3_1 = ['downstream', 'dam', 'reservoir']\n",
    "    combo3_2 = [['water', 'release'], ['flow', 'release'], 'reoperation']\n",
    "    \n",
    "    combo4_1 = ['controlled', 'artificial']\n",
    "    combo4_2 = ['flood']\n",
    "    \n",
    "    combo5_1 = ['hydrologic(al)*']\n",
    "    combo5_2 = ['requirement', 'manipulation']\n",
    "    \n",
    "    combo6_1 = ['flow', ['stream', 'flow'], 'freshwater', 'water', ['water', 'level']]\n",
    "    combo6_2 = ['abstraction', 'allocation', 'criteri\\\\S{1,2}', 'delivery*', \n",
    "                'demand', 'guideline',\n",
    "                'need', 'prescription', 'recommendation', 'recovery', 'requirement', \n",
    "                'reserve', 'restoration', 'restriction', 'withdrawal']\n",
    "    \n",
    "    search_dict = {\n",
    "         'search1': ['with', [combo1_1, combo1_2]],\n",
    "         'search2':  ['with', [combo2_1, combo2_2]],\n",
    "         'search3': ['pre', [combo3_1, combo3_2]],\n",
    "         'search4':  ['pre', [combo4_1, combo4_2]],\n",
    "         'search5':  ['pre', [combo5_1, combo5_2]],\n",
    "         'search6':  ['with', [combo6_1, combo6_2]]\n",
    "    }\n",
    "    return(search_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf56b94b-eb6f-49e9-b574-b6b9dda3bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_string(in_search_duo, inflect, \n",
    "                         or_chars = ' OR ', and_chars = ' AND ',\n",
    "                         inner_separators = [\" \", \"-\", \"\"], use_quotes = True\n",
    "                        ):\n",
    "    search_list_formatted = []\n",
    "\n",
    "    if inflect:\n",
    "        nltk_lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "    #For each combo item~~~~~~~~~~~~~~~~~~~~\n",
    "    combo_list_formatted = []\n",
    "    for combo_list in in_search_duo[1]:\n",
    "        word_group_formatted = []\n",
    "        for repattern_group in combo_list:\n",
    "            #When multiple words in a group\n",
    "            if isinstance(repattern_group, list):\n",
    "                #print(repattern_group)\n",
    "                #Apply find_matching_words to each\n",
    "                repattern_group_inflected = []\n",
    "                for repattern in repattern_group:\n",
    "                    k = find_matching_words(repattern)\n",
    "                    if (len(k) == 0):\n",
    "                        k = repattern\n",
    "                    if not isinstance(k, list):\n",
    "                        k = [k]\n",
    "\n",
    "                    if not inflect:\n",
    "                        #Lemmatize with spacy\n",
    "                        token_list = spacy_nlp(\" \".join(k))\n",
    "                        k = [token.lemma_ \n",
    "                             for token in token_list]\n",
    "\n",
    "                    repattern_group_inflected.append(k)\n",
    "\n",
    "                #Then join them in the order with space, no space, and hyphen\n",
    "                if (in_search_duo[0] == 'with') and use_quotes:\n",
    "                    use_quotes = True\n",
    "                else:\n",
    "                    use_quotes = False\n",
    "                repattern_group_inflected = combinate_concats(\n",
    "                    prefixes=repattern_group_inflected[0],\n",
    "                    suffixes=repattern_group_inflected[1],\n",
    "                    separators=inner_separators,\n",
    "                    add_quotes=use_quotes\n",
    "                )\n",
    "            else:\n",
    "                repattern_group_inflected = find_matching_words(repattern_group)\n",
    "                if (len(repattern_group_inflected) == 0):\n",
    "                    repattern_group_inflected = repattern_group\n",
    "                if not isinstance(repattern_group_inflected, list):\n",
    "                    repattern_group_inflected = [repattern_group_inflected]\n",
    "                if not inflect:\n",
    "                    #Lemmatize with spacy\n",
    "                    token_list = spacy_nlp(\" \".join(repattern_group_inflected))\n",
    "                    repattern_group_inflected = [token.lemma_ \n",
    "                                                   for token in token_list]\n",
    "\n",
    "            #Remove duplicates\n",
    "            word_group_formatted += list(set(repattern_group_inflected))\n",
    "            \n",
    "        # Create a new list to store all variations\n",
    "        new_word_group = []\n",
    "        for word in word_group_formatted:\n",
    "            #if inflect, add all original matching words, plural forms, \n",
    "            #present participle, and alternative spelling\n",
    "            if inflect:\n",
    "                new_word_group.append(word)  # Add original word\n",
    "                \n",
    "                # Add plural form\n",
    "                if plural_form_exists(word, nltk_lemmatizer):\n",
    "                    plural_word = textblob_pluralize(word)\n",
    "                    new_word_group.append(plural_word)\n",
    "\n",
    "                # Add present participle\n",
    "                pre_participle = getInflection(word, 'VBG')\n",
    "                if pre_participle is not None:  # Avoid adding if it's the same\n",
    "                    if isinstance(pre_participle, list):\n",
    "                        for w in pre_particile:\n",
    "                            new_word_group.append(pre_participle)\n",
    "                    else:\n",
    "                        new_word_group.append(pre_participle[0])\n",
    "\n",
    "                # Add spelling variants (british vs americna or vice-versa)\n",
    "                spelling_variants = get_spelling_variants(word)\n",
    "                for variant in spelling_variants:\n",
    "                    if variant != word: # Avoid adding if its the same\n",
    "                      new_word_group.append(variant)\n",
    "            else:\n",
    "              new_word_group.append(word)  # Keep original word\n",
    "\n",
    "        # Remove duplicates (again, after adding variants)\n",
    "        word_group_formatted = list(set(new_word_group))\n",
    "\n",
    "        #Combine word within word_group/combo_list\n",
    "        combo_list_formatted.append(word_group_formatted)\n",
    "        \n",
    "    #Then create actual combinations for each combo duo\n",
    "    if in_search_duo[0] == 'pre':\n",
    "        #Create combinations of all terms separated by a space\n",
    "        search_duo_formatted = combinate_concats(\n",
    "            combo_list_formatted[0], \n",
    "            combo_list_formatted[1],\n",
    "            separators=\" \",\n",
    "            add_quotes = use_quotes\n",
    "        )\n",
    "        search_duo_formatted = recomb(search_duo_formatted, \n",
    "                               recomb_sep = or_chars)\n",
    "        \n",
    "    elif in_search_duo[0] == 'with':\n",
    "        #Create two blocks, OR within each block, AND between the two\n",
    "        search_duo_formatted = [\n",
    "            f'{recomb(combo_list_formatted[0], recomb_sep=or_chars)}'\n",
    "            f'{and_chars}'\n",
    "            f'{recomb(combo_list_formatted[1], recomb_sep=or_chars)}'\n",
    "        ][0]\n",
    "    return(search_duo_formatted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}