{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f125ee-dc7c-4306-ad92-3ad1cca09627",
   "metadata": {},
   "source": [
    "# Utility functions used throughout analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507c6c-28d8-4efd-8435-bd2ca38718ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Import necessary libraries\n",
    "import collections\n",
    "from datetime import datetime, UTC\n",
    "from inspect import getsourcefile\n",
    "import itertools\n",
    "import litstudy  # Use pip install git+https://github.com/NLeSC/litstudy to download dev version\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pyzotero import zotero\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "from typing import List, Union, Dict, Any, Optional\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15eb05c-daaf-43a0-9ea7-2046e1aab61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_list_files(in_dir: Union[str, Path], in_pattern: str, full_path: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Lists files in a directory matching a regular expression.\n",
    "\n",
    "    Args:\n",
    "        in_dir: The directory to search. Can be a string path or a pathlib.Path object.\n",
    "        in_pattern: The regular expression pattern to match filenames against.\n",
    "        full_path: Whether to return full paths (True) or just filenames (False).\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, either full paths or filenames, of files matching the pattern.\n",
    "        Returns an empty list if no matches are found or if an error occurs.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: if input arguments are of incorrect type\n",
    "        ValueError: if input directory does not exist\n",
    "        re.error:  If the regular expression pattern is invalid.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Type Validation ---\n",
    "    if not isinstance(in_dir, (str, Path)):\n",
    "        raise TypeError(\"in_dir must be a string or pathlib.Path object.\")\n",
    "    if not isinstance(in_pattern, str):\n",
    "        raise TypeError(\"in_pattern must be a string.\")\n",
    "    if not isinstance(full_path, bool):\n",
    "        raise TypeError(\"full_path must be a boolean.\")\n",
    "\n",
    "    # --- Convert to Path object for consistency ---\n",
    "    if isinstance(in_dir, str):\n",
    "        in_dir = Path(in_dir)\n",
    "\n",
    "    # --- Input Value Validation ---\n",
    "    if not in_dir.is_dir():\n",
    "        raise ValueError(f\"The directory '{in_dir}' does not exist.\")\n",
    "\n",
    "    # --- Regex Compilation (with error handling) ---\n",
    "    try:\n",
    "        regex = re.compile(in_pattern)\n",
    "    except re.error as e:\n",
    "        raise re.error(f\"Invalid regular expression pattern: {e}\")\n",
    "\n",
    "    file_list = []\n",
    "    for root, _, files in os.walk(in_dir):  # os.walk works with Path objects\n",
    "        for file in files:\n",
    "            if regex.match(file):\n",
    "                if full_path:\n",
    "                    # Use .joinpath for consistent path construction with Path objects\n",
    "                    file_list.append(str(Path(root).joinpath(file)))  # Convert to string for consistent return type\n",
    "                else:\n",
    "                    file_list.append(file)\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652c7330-518c-4905-8e02-1ebeb02583dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and compile references from the WoS search into a single document set (lit_study format)\n",
    "def rpickle_bibdocset(in_dirpath, in_pattern, out_pickle):\n",
    "    if not out_pickle.exists():\n",
    "        # Get list of every bib file\n",
    "        bib_initlist = [p for p in list(in_dirpath.glob('*')) \n",
    "                        if re.compile(in_pattern).match(str(p))]\n",
    "        # Read bib files from first scoping and join them (takes ~15-20 sec/1000 refs)\n",
    "        reflist = []\n",
    "        for bib in bib_initlist:\n",
    "            reflist += litstudy.load_bibtex(bib)\n",
    "\n",
    "        # Pickle them (save the full document set as a binary file on disk that can be easily retrieved)\n",
    "        with open(out_pickle, 'wb') as f:\n",
    "            pickle.dump(reflist, f)\n",
    "    else:\n",
    "        # Read pre-saved document set\n",
    "        with open(out_pickle, 'rb') as f:\n",
    "            reflist = pickle.load(f)\n",
    "    return reflist\n",
    "\n",
    "# Get titles and DOIs from Zotero test list\n",
    "def get_testlist(library_id, api_key_path, test_list_name):\n",
    "    api_key = api_key_path.read_text().strip()\n",
    "    zot = zotero.Zotero(library_id=library_id, \n",
    "                        library_type='group', \n",
    "                        api_key=api_key)\n",
    "    testlist_colID = str([col['key'] for col in zot.collections_top() \n",
    "                          if col['data']['name'] == test_list_name][0])\n",
    "    testlist_items = zot.everything(zot.collection_items_top(testlist_colID))\n",
    "\n",
    "    testlist_title_dois = collections.defaultdict(list)\n",
    "    for ref in testlist_items:\n",
    "        testlist_title_dois[ref['key']].append(ref['data']['title'])\n",
    "        if 'DOI' in ref['data']:\n",
    "            testlist_title_dois[ref['key']].append(ref['data']['DOI'])\n",
    "        else:\n",
    "            testlist_title_dois[ref['key']].append(np.nan)\n",
    "    return testlist_title_dois\n",
    "\n",
    "# Get all DOIs and titles in references returned from search\n",
    "def tabulate_searchlist(in_reflist, out_csvpath):\n",
    "    if not out_csvpath.exists():\n",
    "        reflist_dict = {}\n",
    "        for i, ref in enumerate(in_reflist):\n",
    "            reflist_dict[i] = [re.sub(r\"[^a-zA-Z\\d\\s]\", \"\", \n",
    "                                      ref.title.replace('\\n', ' ').lower()),\n",
    "                               ref.publication_source,\n",
    "                               ref.publication_year, ref.abstract]\n",
    "            if 'doi' in ref.entry:\n",
    "                reflist_dict[i].append(ref.entry['doi'])\n",
    "            else:\n",
    "                reflist_dict[i].append(np.nan)\n",
    "\n",
    "        reflist_pd = pd.DataFrame.from_dict(reflist_dict, orient='index')\n",
    "        reflist_pd.columns = ['title', 'source', 'year', 'abstract', 'doi']\n",
    "        reflist_pd.to_csv(out_csvpath)\n",
    "    else:\n",
    "        reflist_pd = pd.read_csv(out_csvpath)\n",
    "    return reflist_pd\n",
    "\n",
    "# Erite string y to file x\n",
    "def write(x, y):\n",
    "    with open(x, 'a') as f:\n",
    "        f.write(y)\n",
    "        f.write('\\n')\n",
    "    return _\n",
    "\n",
    "def combine_2w_regex(pattern1, pattern2, precede=False):\n",
    "    \"\"\"\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    \"\"\"\n",
    "    regexp = f\"{pattern1}\\\\W{pattern2}\\\\b\"\n",
    "    if precede == False:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W{pattern1}\\\\b)\"\n",
    "    return regexp\n",
    "\n",
    "# Count number a times a simple 2-pattern group occurs in text\n",
    "def find_2w_regex(text, pattern1, pattern2, precede=False):\n",
    "    \"\"\"\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    \"\"\"\n",
    "    regexp = f\"{pattern1}\\\\W{pattern2}\\\\b\"\n",
    "    if precede == False:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W{pattern1}\\\\b)\"\n",
    "    wa = re.findall(regexp, text)\n",
    "    wal = len(wa)\n",
    "    return wal\n",
    "\n",
    "# Count number a times a simple pattern occurs in text\n",
    "def find_regex(text, pattern):\n",
    "    return len(re.findall(f\"{pattern}\", text))\n",
    "\n",
    "# Join all strings in a list with | signs and parentheses\n",
    "def recomb(in_str, recomb_sep):\n",
    "    if isinstance(in_str, list):\n",
    "        return f\"({recomb_sep.join(f'({w})' for w in in_str)})\"\n",
    "    else:\n",
    "        return in_str\n",
    "\n",
    "# Find patterns in text based on search dictionary\n",
    "def combo_refind(in_searchdict, text):\n",
    "    for regexp_combo in in_searchdict.values():\n",
    "        if regexp_combo[0] == 'with':\n",
    "            k = find_2w_regex(text, \n",
    "                              regexp_combo[1][0], \n",
    "                              regexp_combo[1][1], \n",
    "                              precede=False)\n",
    "        elif regexp_combo[0] == 'pre':\n",
    "            k = find_2w_regex(text,\n",
    "                              regexp_combo[1][0], \n",
    "                              regexp_combo[1][1], \n",
    "                              precede=True)\n",
    "        elif regexp_combo[0] is None:\n",
    "            k = find_regex(text, regexp_combo)\n",
    "        else:\n",
    "            break\n",
    "        return k\n",
    "\n",
    "# Generate n-grams from DOI\n",
    "#CHECK OUT: from pattern.en import ngrams\n",
    "#print(ngrams(\"He goes to hospital\", n=2))\n",
    "def DOI_ngram(A):\n",
    "    count0 = collections.Counter()\n",
    "    s1 = A[0].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count0 = count0 + (collections.Counter(every))\n",
    "    count0 = count0.most_common()\n",
    "\n",
    "    count1 = collections.Counter()\n",
    "    s1 = A[1].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count1 = count1 + (collections.Counter(every))\n",
    "    count1 = count1.most_common()\n",
    "\n",
    "    count2 = collections.Counter()\n",
    "    for idx, i in enumerate(A[2]):\n",
    "        x = collections.Counter([l.lower() for l in i])\n",
    "        count2 += x\n",
    "    count2 = count2.most_common()\n",
    "\n",
    "    count3 = count0 + count1 + count2\n",
    "    return count3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6256bd67-dd76-4a87-9d5b-d05ce5edb638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dataframe(\n",
    "    list_of_dicts: List[Dict[str, Any]], \n",
    "    keys_to_keep: Optional[List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a list of dictionaries to a Pandas DataFrame, keeping a specified\n",
    "    subset of keys as columns.  Handles missing keys gracefully.\n",
    "\n",
    "    Args:\n",
    "        list_of_dicts: The list of dictionaries.\n",
    "        keys_to_keep: An optional list of keys to keep as columns.\n",
    "            If None, attempts to use all keys present in *any* of the\n",
    "            dictionaries, but prioritizes keys from the *first* dictionary\n",
    "            if there are inconsistencies.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: if input is not a list or contains non-dict elements.\n",
    "        ValueError: if keys_to_keep is provided but is empty.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(list_of_dicts, list):\n",
    "        raise TypeError(\"Input 'list_of_dicts' must be a list of dictionaries.\")\n",
    "    if not all(isinstance(item, dict) for item in list_of_dicts):\n",
    "        raise TypeError(\"All elements in 'list_of_dicts' must be dictionaries.\")\n",
    "    if keys_to_keep is not None and not isinstance(keys_to_keep, list):\n",
    "        raise TypeError(\"'keys_to_keep' must be a list of strings or None.\")\n",
    "    if keys_to_keep is not None and len(keys_to_keep) == 0:\n",
    "        raise ValueError(\"'keys_to_keep' cannot be an empty list.\")\n",
    "\n",
    "\n",
    "    if keys_to_keep is None:\n",
    "        # Attempt to use all keys, prioritizing the first dictionary\n",
    "        if not list_of_dicts:  # Handle empty input list\n",
    "            return pd.DataFrame()\n",
    "        keys_to_keep = list(list_of_dicts[0].keys())  # Start with keys from first dict\n",
    "        # Add any keys present in *other* dicts but missing from the first.\n",
    "        for item in list_of_dicts:\n",
    "            for key in item:\n",
    "                if key not in keys_to_keep:\n",
    "                    keys_to_keep.append(key)\n",
    "    \n",
    "    # Create the DataFrame, handling missing keys\n",
    "    df_data = []\n",
    "    for item in list_of_dicts:\n",
    "        row = {key: item.get(key, None) for key in keys_to_keep}\n",
    "        df_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(df_data, columns=keys_to_keep)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146646f1-ca5f-496c-a665-62758f23e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to export OpenAlex works to RIS\n",
    "%run lib/openalex_formatter_ris.py\n",
    "\n",
    "def export_oalex_works_to_ris(\n",
    "    works: List[Dict[str, Any]], \n",
    "    filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Export a list of OpenAlex works to an RIS file, including the abstract.\n",
    "\n",
    "    Args:\n",
    "        works: List of OpenAlex work dictionaries.\n",
    "        filename: The name of the RIS file to be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(works, list):\n",
    "        raise TypeError(\"works must be a list of dictionaries.\")\n",
    "    if not isinstance(filename, str):\n",
    "        raise TypeError(\"filename must be a string.\")\n",
    "    if not filename.endswith(\".ris\"):\n",
    "        logging.warning(\"Filename does not end with '.ris'.  This may cause problems with some RIS readers.\")\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as ris_file:\n",
    "            for work in works:\n",
    "                if not isinstance(work, dict):\n",
    "                    print(f\"Skipping invalid work entry (not a dictionary): {work}\")\n",
    "                    continue\n",
    "\n",
    "                ris_entry = build_ris_entry(work)\n",
    "                ris_file.write(ris_entry)\n",
    "\n",
    "                # # --- Article type ----\n",
    "                # ris_file.write(\"TY  - JOUR\\n\")  # Type of reference (Journal Article)\n",
    "                \n",
    "                # # --- Title ---\n",
    "                # title = work.get('title', '') or ''  # Handle None or missing title\n",
    "                # ris_file.write(f\"TI  - {title}\\n\")\n",
    "\n",
    "                # # --- Authors ---\n",
    "                # # Handle authors correctly.  OpenAlex stores authors as a list of dictionaries.\n",
    "                # authors = work.get('authorships', []) or []\n",
    "                # for author_data in authors:\n",
    "                #     author_name = author_data.get('author', {}).get('display_name', '') or ''\n",
    "                #     if author_name:\n",
    "                #         ris_file.write(f\"AU  - {author_name}\\n\")\n",
    "\n",
    "                # # --- Publication Year ---\n",
    "                # year = work.get('publication_year', '') or ''\n",
    "                # ris_file.write(f\"PY  - {year}\\n\")\n",
    "\n",
    "                # # --- Journal (Source) ---\n",
    "                # #  OpenAlex stores source information in 'primary_location' and 'locations'.\n",
    "                # source_title = ''\n",
    "                # if work.get('primary_location') and work.get('primary_location').get('source'):\n",
    "                #     source_title = work['primary_location']['source'].get('display_name', '') or ''\n",
    "                # elif work.get('locations'):\n",
    "                #     for location in work['locations']:\n",
    "                #         if location.get('source'):\n",
    "                #             source_title = location['source'].get('display_name', '') or ''\n",
    "                #             break # Use the first available location.\n",
    "\n",
    "                # ris_file.write(f\"JO  - {source_title}\\n\")\n",
    "\n",
    "                # # --- Volume, Issue, Pages ---\n",
    "                # volume = work.get('volume', '') or ''\n",
    "                # issue = work.get('issue', '') or ''\n",
    "                # #  Pages can be in 'biblio' or as separate 'page_start', 'page_end'\n",
    "                # start_page = work.get('page_start', '') or ''\n",
    "                # end_page = work.get('page_end', '') or ''\n",
    "                # if not start_page and work.get('biblio'):\n",
    "                #     start_page = work['biblio'].get('first_page', '') or ''\n",
    "                #     end_page = work['biblio'].get('last_page', '') or ''\n",
    "                \n",
    "                # ris_file.write(f\"VL  - {volume}\\n\")\n",
    "                # ris_file.write(f\"IS  - {issue}\\n\")\n",
    "                # ris_file.write(f\"SP  - {start_page}\\n\")\n",
    "                # ris_file.write(f\"EP  - {end_page}\\n\")\n",
    "\n",
    "                # # --- DOI ---\n",
    "                # doi = work.get('doi', '') or ''  # Get DOI directly.  It's a top-level field.\n",
    "                # ris_file.write(f\"DO  - {doi}\\n\")\n",
    "\n",
    "                # # --- Abstract ---\n",
    "                # abstract = work['abstract']\n",
    "                # if abstract:  # Only write abstract if it exists\n",
    "                #     ris_file.write(f\"AB  - {abstract}\\n\")\n",
    "\n",
    "\n",
    "                # ris_file.write(\"ER  - \\n\\n\")  # End of reference\n",
    "\n",
    "    except (IOError, OSError) as e:\n",
    "        raise OSError(f\"Error writing to RIS file: {e}\")\n",
    "    except Exception as e: #Catch remaining exceptions\n",
    "        raise Exception(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08506a3a-2e56-4b2f-b196-42eec3753afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching(query_string, broad='no', has_ngrams='true', \n",
    "              does_date_matter='no', from_date='1976-01-01'):\n",
    "    if broad == 'no':\n",
    "        if does_date_matter == 'yes':\n",
    "            institution = requests.get(\n",
    "                f'https://api.openalex.org/works?filter=abstract.search:{query_string},\\\n",
    "                has_ngrams:{has_ngrams},from_publication_date:{from_date},to_publication_date:{to_date}&page=1&per-page=20'\n",
    "            ).json()\n",
    "        else:\n",
    "            institution = requests.get(\n",
    "                f'https://api.openalex.org/works?filter=title.search:{query_string},\\\n",
    "                has_ngrams:{has_ngrams},&page=1&per-page=20'\n",
    "            ).json()\n",
    "    elif broad == 'yes':\n",
    "        institution = requests.get(\n",
    "            f'https://api.openalex.org/works?search={query_string}&page=1&per-page=20'\n",
    "        ).json()\n",
    "    return institution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
