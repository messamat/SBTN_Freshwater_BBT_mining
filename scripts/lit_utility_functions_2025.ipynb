{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3847256-99ee-47bd-ad5f-4b5b3c4ea58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from pyzotero import zotero\n",
    "import numpy as np\n",
    "import collections\n",
    "import requests\n",
    "import collections\n",
    "import litstudy #Use pip install git+https://github.com/NLeSC/litstudy to download dev version. Other encoding problem when loading ris files (load_ris_file needs to use robust_open instead of open)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "from pyzotero import zotero\n",
    "#import cProfile\n",
    "import re\n",
    "#import itertools as it\n",
    "from inspect import getsourcefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479d173-3b9a-45d3-9acc-a97275eb58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read and compile references from the WoS search\n",
    "#into a single document set (lit_study format)\n",
    "def rpickle_bibdocset(in_dirpath, in_pattern, out_pickle):\n",
    "    if not out_pickle.exists():\n",
    "        #Get list of every bib file\n",
    "        bib_initlist = [p for p in list(in_dirpath.glob('*')) \n",
    "                        if re.compile(in_pattern).match(str(p))]\n",
    "        #Read bib files from first scoping and join them (takes ~15-20 sec/1000 refs)\n",
    "        reflist = []\n",
    "        for bib in bib_initlist:\n",
    "            reflist += litstudy.load_bibtex(bib)\n",
    "\n",
    "        #Pickle them (save the full document set as a binary file on disk that can be easily retrieved)\n",
    "        with open(out_pickle, 'wb') as f:\n",
    "            pickle.dump(reflist, f)\n",
    "    else:\n",
    "        #Read pre-saved document set\n",
    "        with open(out_pickle, 'rb') as f:\n",
    "            reflist =  pickle.load(f)\n",
    "    return(reflist)\n",
    "            \n",
    "#Get titles and dois from zotero test list\n",
    "def get_testlist(library_id, api_key_path):\n",
    "    api_key = api_key_path.read_text().strip() \n",
    "    zot = zotero.Zotero(library_id = library_id, library_type = 'group', api_key = api_key) #Get \n",
    "    testlist_colID = str([col['key'] for col in zot.collections_top()\n",
    "                          if col['data']['name'] == 'test list'][0])\n",
    "    testlist_items = zot.everything(zot.collection_items_top(testlist_colID))\n",
    "\n",
    "    testlist_title_dois = collections.defaultdict(list)\n",
    "    for ref in testlist_items:\n",
    "        testlist_title_dois[ref['key']].append(ref['data']['title'])\n",
    "        if 'DOI' in ref['data']:\n",
    "            testlist_title_dois[ref['key']].append(ref['data']['DOI'])\n",
    "        else:\n",
    "            testlist_title_dois[ref['key']].append(np.nan)\n",
    "        #testlist_title_dois[ref['key']].append(ref['data']['itemType'])\n",
    "    return(testlist_title_dois)\n",
    "\n",
    "#Get all dois and titles in references returned from search\n",
    "def tabulate_searchlist(in_reflist, out_csvpath):\n",
    "    if not out_csvpath.exists():\n",
    "        reflist_dict = {}\n",
    "        for i, ref in enumerate(in_reflist):\n",
    "            reflist_dict[i] = [re.sub(\"[^a-zA-Z\\d\\s]\", \"\", ref.title.replace('\\n', ' ').lower()),\n",
    "                               ref.publication_source, ref.publication_year, ref.abstract]\n",
    "            if 'doi' in ref.entry:\n",
    "                reflist_dict[i].append(ref.entry['doi'])\n",
    "            else:\n",
    "                reflist_dict[i].append(np.nan)\n",
    "\n",
    "        reflist_pd = pd.DataFrame.from_dict(reflist_dict, orient='index')\n",
    "        reflist_pd.columns = ['title', 'source', 'year', 'abstract', 'doi']\n",
    "\n",
    "        reflist_pd.to_csv(out_csvpath)\n",
    "    else:\n",
    "        reflist_pd = pd.read_csv(out_csvpath)\n",
    "    return(reflist_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f104b301-eb33-469d-b95b-52a90a4999ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(x,y):\n",
    "    with open(x,'a') as f:\n",
    "        f.write(y)\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d4f8b-c55e-45ed-93c8-bd9bab039bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def findNear(text,word1,word2,precede=False):\n",
    "#     wa = re.findall(rf\"{word1}\" +r\"\\W+(?:\\w+\\W+){0,0}?\" + rf\"{word2}\" + r\"\\b\",prepped_text)\n",
    "#     if precede==False:\n",
    "#         wa1 = re.findall(rf\"{word2}\" +r\"\\W+(?:\\w+\\W+){0,0}?\" + rf\"{word1}\" + r\"\\b\",prepped_text)\n",
    "#         return wa, wa1\n",
    "#     else:\n",
    "#         return wa\n",
    "\n",
    "def findSimp(text,word1,word2,precede=False):\n",
    "    '''\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    '''\n",
    "\n",
    "    regexp = f\"{word1}\\\\W{word2}\\\\b\"\n",
    "    if precede==False:\n",
    "        regexp = f\"({regexp})|({word2}\\\\W{word1}\\\\b)\"\n",
    "\n",
    "    wa = re.findall(regexp, text)\n",
    "    wal = len(wa)\n",
    "\n",
    "    return(wal)\n",
    "\n",
    "\n",
    "def findSuperSimp(text,word):\n",
    "    return len(re.findall(f\"{word}\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9df97c-4130-41da-89e0-5ab14abe4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all strings in a list with | signs and parentheses\n",
    "def recomb(in_str):\n",
    "    if isinstance(in_str, list):\n",
    "        return(f\"({'|'.join(f'({w})' for w in in_str)})\")\n",
    "    else:\n",
    "        return(in_str)\n",
    "\n",
    "#Profile whether faster to use product upstream or to use | in regex\n",
    "# with cProfile.Profile() as prof:\n",
    "#     for x in range(100000):\n",
    "#         x1 = list(it.product(combo1_1, combo1_2))\n",
    "#         for j in x1:\n",
    "#             findSimp(prepped_text, j[0], j[1],precede=False)\n",
    "# prof.print_stats()\n",
    "#\n",
    "# with cProfile.Profile() as prof:\n",
    "#     x1 = [recomb(combo1_1), recomb(combo1_2)]\n",
    "#     for x in range(100000):\n",
    "#         findSimp(prepped_text,x1[0],x1[1],precede=False)\n",
    "# prof.print_stats()\n",
    "###### Using | in regex exp is 6.5 faster\n",
    "\n",
    "def combo_refind(in_searchdict, text):\n",
    "    for regexp_combo in in_searchdict.values():\n",
    "        if regexp_combo[0] == 'with':\n",
    "            k = findSimp(text, regexp_combo[1][0], regexp_combo[1][1], precede=False)\n",
    "        elif regexp_combo[0] == 'pre':\n",
    "            k = findSimp(text, regexp_combo[1][0], regexp_combo[1][1], precede=True)\n",
    "        elif regexp_combo[0] is None:\n",
    "            k = findSuperSimp(text, regexp_combo)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        return (k)\n",
    "        # if k > 0:\n",
    "        #\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43eaec8-00fb-45fd-84b8-32167984b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "\n",
    "with open('archive2022/zotero_keys.txt') as f:\n",
    "    for idx,x in enumerate(f):\n",
    "        y = x.split(' ')\n",
    "        z.append(y[-1].strip())\n",
    "        # print(idx,y)\n",
    "        if idx == 2:\n",
    "            print('\\n')\n",
    "            break\n",
    "\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907c6ad-6a71-4f1e-98a1-d88bd2497441",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "### use this to configure with your own api key and groupnum.\n",
    "\n",
    "###  https://pyzotero.readthedocs.io/en/latest/#getting-started-short-version\n",
    "\n",
    "group_id = z[1]\n",
    "api_key = z[2][1:-1]\n",
    "\n",
    "zot = zotero.Zotero(group_id,'group',api_key)\n",
    "cols = zot.collections()\n",
    "\n",
    "# itz = zot.everything(zot.collection_items(cols[-1]['key'])) \n",
    "#not helpful here, includes multiple entries of same\n",
    "\n",
    "itz1 = zot.everything(zot.collection_items_top(cols[-1]['key']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae800d7-45d0-4e39-b5d3-9234368856ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get title, abstract, and tags\n",
    "A = []\n",
    "for idx,_ in enumerate(range(len(itz1))):\n",
    "    x = itz1[idx]['data']['title']\n",
    "    y = itz1[idx]['data']['abstractNote']\n",
    "    z = [i['tag'] for i in itz1[idx]['data']['tags']]\n",
    "    A.append([x,y,z])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f88f8-c969-4aae-8c3e-2dd047b7b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DOI_ngram(A):\n",
    "    count0 = collections.Counter()\n",
    "    s1 = A[0].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\",'')\n",
    "    s4 = s3.replace(\",\",'')\n",
    "    s5 = s4.replace(\":\",'')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens,2,4)\n",
    "    count0 = count0 + (collections.Counter(every))\n",
    "    count0 = count0.most_common()\n",
    "\n",
    "    count1 = collections.Counter()\n",
    "    s1 = A[1].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\",'')\n",
    "    s4 = s3.replace(\",\",'')\n",
    "    s5 = s4.replace(\":\",'')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens,2,4)\n",
    "    count1 = count1 + (collections.Counter(every))\n",
    "    count1 = count1.most_common()\n",
    "\n",
    "    count2 = collections.Counter()\n",
    "    for idx, i in enumerate(A[2]):\n",
    "        x = collections.Counter([l.lower() for l in i])\n",
    "        count2 += x\n",
    "    count2 = count2.most_common()\n",
    "\n",
    "\n",
    "\n",
    "    count3 = count0 + count1 + count2\n",
    "    return count3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dae5e0-2fc9-4d09-9918-3c3039f33d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count4 = []\n",
    "badwords = ['the','in','of','for','and','an','as','no','to','(','we','by','from','was','are','than','have','this','has','is','that','these','on','be','or','at']\n",
    "for c in A:\n",
    "    d = DOI_ngram(c)\n",
    "    for idx,i in enumerate(d):\n",
    "        coun = 0\n",
    "        for idj,j in enumerate(i[0]):\n",
    "            if j in badwords:\n",
    "                break\n",
    "            else:\n",
    "                coun+=1\n",
    "        if coun == len(i[0]):\n",
    "            count4.append(i)   \n",
    "\n",
    "# y = DOI_ngram(A[0])\n",
    "count5 = []\n",
    "for idx,i in enumerate(count4):\n",
    "    coun = 0\n",
    "    for idj,j in enumerate(i[0]):\n",
    "        if j in badwords:\n",
    "            break\n",
    "        elif len(j) == 1:\n",
    "            break\n",
    "        else:\n",
    "            coun+=1\n",
    "    if coun == len(i[0]):\n",
    "        count5.append(i)   \n",
    "count6 = dict(count5)\n",
    "\n",
    "count7 = collections.Counter(count6)\n",
    "count7.most_common()\n",
    "# for w in sorted(count6,key=count6.get,reverse=True):\n",
    "#     print(w,count6[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c113e1b-f9a6-4230-9fd3-76832a66855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching(\n",
    "    query_string,\n",
    "    broad='no',\n",
    "    has_ngrams='true',\n",
    "    does_date_matter = 'no',\n",
    "    from_date='2010-01-01',\n",
    "    to_date='2022-12-12'):\n",
    "\n",
    "    if broad == 'no':\n",
    "        if does_date_matter == 'yes':\n",
    "            institution = requests.get(\n",
    "                f'https://api.openalex.org/works?filter=abstract.search:{query_string},has_ngrams:{has_ngrams},from_publication_date:{from_date},to_publication_date:{to_date}&page=1&per-page=20'\n",
    "            ).json()\n",
    "        else:\n",
    "            institution = requests.get(\n",
    "                f'https://api.openalex.org/works?filter=title.search:{query_string},has_ngrams:{has_ngrams},&page=1&per-page=20'\n",
    "            ).json()\n",
    "\n",
    "    elif broad == 'yes':\n",
    "        institution = requests.get(\n",
    "            f'https://api.openalex.org/works?search={query_string}&page=1&per-page=20'\n",
    "        ).json()\n",
    "    return institution\n",
    "    \n",
    "    # a = []\n",
    "    # for x in range(30):\n",
    "    #     try:\n",
    "    #         y = institution['results'][x]['ngrams_url']\n",
    "    #         if x < 5:\n",
    "    #             print(institution['results'][x]['title'])\n",
    "    #             print(institution['results'][x]['authorships'][0]['author']['display_name'])\n",
    "    #             print(institution['results'][x]['doi'])\n",
    "    #             print(institution['results'][x]['relevance_score'])\n",
    "    #             print(institution['results'][x]['publication_date'])\n",
    "    #             print(y,'\\n')  \n",
    "    #         z = requests.get(y).json()\n",
    "    #         a.append(z)\n",
    "    #     except:\n",
    "    #         pass\n",
    "\n",
    "    # return a\n",
    "\n",
    "\n",
    "count = 0\n",
    "print(f\"count of identical / position in list / total size of list\")\n",
    "listofngrams = []\n",
    "for i in range(len(B)):\n",
    "    _ngrams = searching(B[i][0],broad='yes')\n",
    "    # print(_ngrams['meta'])\n",
    "    if _ngrams['meta']['count'] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        y = _ngrams['results'][0]['title']\n",
    "        if y == B[i][0]:\n",
    "            count+=1\n",
    "            print(f\"{count} / {i} / {len(B)}\")\n",
    "            urlll = _ngrams['results'][0]['ngrams_url']\n",
    "            z = requests.get(urlll).json()\n",
    "            listofngrams.append(z)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# for idx,i in enumerate(_ngrams):\n",
    "#     if idx == 0:\n",
    "#         j = i['ngrams']\n",
    "#         k = pd.DataFrame.from_dict(j)\n",
    "#     else:\n",
    "#         j = i['ngrams']\n",
    "#         l = pd.DataFrame.from_dict(j)\n",
    "#         k = pd.concat([k,l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f4548-9c9f-4b4e-bce4-8c0f75570fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "ngram = []\n",
    "\n",
    "for i in listofngrams:\n",
    "    for j in i['ngrams']:\n",
    "        ngram.append((j['ngram'].lower(),j['ngram_count']))\n",
    "\n",
    "print(len(ngram))\n",
    "\n",
    "b = collections.Counter()\n",
    "for ng,co in ngram:\n",
    "    b[ng] += co\n",
    "\n",
    "c = [[x,y] for x,y in b.items()]\n",
    "\n",
    "badwords = ['add words to filter as necessary',' .$', 'et al','et$', '3 /s', 'm 3 /s']\n",
    "\n",
    "df = pd.DataFrame.from_records(c,columns=['ngram','ngram_count'])\n",
    "df['ngram_tokens'] = df['ngram'].str.split().str.len() #https://stackoverflow.com/questions/37483470/how-to-calculate-number-of-words-in-a-string-in-dataframe\n",
    "df1 = df.dropna(axis=0)\n",
    "df2 = df1.loc[(df1['ngram_tokens'] > 1) & (df1['ngram_tokens'] < 5)]\n",
    "df3 = df2.sort_values(by=['ngram_count','ngram_tokens'],ascending=False)\n",
    "\n",
    "for i in badwords:\n",
    "    df3 = df3[~df3['ngram'].str.contains(i)]\n",
    "\n",
    "df4 = df3.to_numpy()\n",
    "df4.shape\n",
    "\n",
    "df3.head(n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
