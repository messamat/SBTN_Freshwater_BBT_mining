{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3b8f253-74c5-4a91-bfc1-b7f0b71633be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\messa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import collections\n",
    "import inflect\n",
    "from inspect import getsourcefile\n",
    "import itertools\n",
    "import litstudy  # Use pip install git+https://github.com/NLeSC/litstudy to download dev version\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from pyzotero import zotero\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# Download nltk 'punkt' tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652c7330-518c-4905-8e02-1ebeb02583dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and compile references from the WoS search into a single document set (lit_study format)\n",
    "def rpickle_bibdocset(in_dirpath, in_pattern, out_pickle):\n",
    "    if not out_pickle.exists():\n",
    "        # Get list of every bib file\n",
    "        bib_initlist = [p for p in list(in_dirpath.glob('*')) if re.compile(in_pattern).match(str(p))]\n",
    "        # Read bib files from first scoping and join them (takes ~15-20 sec/1000 refs)\n",
    "        reflist = []\n",
    "        for bib in bib_initlist:\n",
    "            reflist += litstudy.load_bibtex(bib)\n",
    "\n",
    "        # Pickle them (save the full document set as a binary file on disk that can be easily retrieved)\n",
    "        with open(out_pickle, 'wb') as f:\n",
    "            pickle.dump(reflist, f)\n",
    "    else:\n",
    "        # Read pre-saved document set\n",
    "        with open(out_pickle, 'rb') as f:\n",
    "            reflist = pickle.load(f)\n",
    "    return reflist\n",
    "\n",
    "# Get titles and DOIs from Zotero test list\n",
    "def get_testlist(library_id, api_key_path):\n",
    "    api_key = api_key_path.read_text().strip()\n",
    "    zot = zotero.Zotero(library_id=library_id, library_type='group', api_key=api_key)\n",
    "    testlist_colID = str([col['key'] for col in zot.collections_top() if col['data']['name'] == 'test list'][0])\n",
    "    testlist_items = zot.everything(zot.collection_items_top(testlist_colID))\n",
    "\n",
    "    testlist_title_dois = collections.defaultdict(list)\n",
    "    for ref in testlist_items:\n",
    "        testlist_title_dois[ref['key']].append(ref['data']['title'])\n",
    "        if 'DOI' in ref['data']:\n",
    "            testlist_title_dois[ref['key']].append(ref['data']['DOI'])\n",
    "        else:\n",
    "            testlist_title_dois[ref['key']].append(np.nan)\n",
    "    return testlist_title_dois\n",
    "\n",
    "# Get all DOIs and titles in references returned from search\n",
    "def tabulate_searchlist(in_reflist, out_csvpath):\n",
    "    if not out_csvpath.exists():\n",
    "        reflist_dict = {}\n",
    "        for i, ref in enumerate(in_reflist):\n",
    "            reflist_dict[i] = [re.sub(r\"[^a-zA-Z\\d\\s]\", \"\", ref.title.replace('\\n', ' ').lower()),\n",
    "                               ref.publication_source, ref.publication_year, ref.abstract]\n",
    "            if 'doi' in ref.entry:\n",
    "                reflist_dict[i].append(ref.entry['doi'])\n",
    "            else:\n",
    "                reflist_dict[i].append(np.nan)\n",
    "\n",
    "        reflist_pd = pd.DataFrame.from_dict(reflist_dict, orient='index')\n",
    "        reflist_pd.columns = ['title', 'source', 'year', 'abstract', 'doi']\n",
    "        reflist_pd.to_csv(out_csvpath)\n",
    "    else:\n",
    "        reflist_pd = pd.read_csv(out_csvpath)\n",
    "    return reflist_pd\n",
    "\n",
    "# Erite string y to file x\n",
    "def write(x, y):\n",
    "    with open(x, 'a') as f:\n",
    "        f.write(y)\n",
    "        f.write('\\n')\n",
    "    return _\n",
    "\n",
    "def combine_2w_regex(pattern1, pattern2, precede=False):\n",
    "    \"\"\"\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    \"\"\"\n",
    "    regexp = f\"{pattern1}\\\\W{pattern2}\\\\b\"\n",
    "    if precede == False:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W{pattern1}\\\\b)\"\n",
    "    return regexp\n",
    "\n",
    "# Count number a times a simple 2-pattern group occurs in text\n",
    "def find_2w_regex(text, pattern1, pattern2, precede=False):\n",
    "    \"\"\"\n",
    "    precede = False means word1 and word2 are looked at with either being first word\n",
    "    precede = True means word1 must be first, word2 must be second\n",
    "    \"\"\"\n",
    "    regexp = f\"{pattern1}\\\\W{pattern2}\\\\b\"\n",
    "    if precede == False:\n",
    "        regexp = f\"({regexp})|({pattern2}\\\\W{pattern1}\\\\b)\"\n",
    "    wa = re.findall(regexp, text)\n",
    "    wal = len(wa)\n",
    "    return wal\n",
    "\n",
    "# Count number a times a simple pattern occurs in text\n",
    "def find_regex(text, pattern):\n",
    "    return len(re.findall(f\"{word}\", text))\n",
    "\n",
    "# Join all strings in a list with | signs and parentheses\n",
    "def recomb(in_str):\n",
    "    if isinstance(in_str, list):\n",
    "        return f\"({'|'.join(f'({w})' for w in in_str)})\"\n",
    "    else:\n",
    "        return in_str\n",
    "\n",
    "# Find patterns in text based on search dictionary\n",
    "def combo_refind(in_searchdict, text):\n",
    "    for regexp_combo in in_searchdict.values():\n",
    "        if regexp_combo[0] == 'with':\n",
    "            k = find_2w_regex(text, regexp_combo[1][0], regexp_combo[1][1], precede=False)\n",
    "        elif regexp_combo[0] == 'pre':\n",
    "            k = find_2w_regex(text, regexp_combo[1][0], regexp_combo[1][1], precede=True)\n",
    "        elif regexp_combo[0] is None:\n",
    "            k = find_regex(text, regexp_combo)\n",
    "        else:\n",
    "            break\n",
    "        return k\n",
    "\n",
    "# Generate n-grams from DOI\n",
    "def DOI_ngram(A):\n",
    "    count0 = collections.Counter()\n",
    "    s1 = A[0].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count0 = count0 + (collections.Counter(every))\n",
    "    count0 = count0.most_common()\n",
    "\n",
    "    count1 = collections.Counter()\n",
    "    s1 = A[1].replace(\"'\", '')\n",
    "    s2 = s1.replace(\"?\", '')\n",
    "    s3 = s2.replace(\".\", '')\n",
    "    s4 = s3.replace(\",\", '')\n",
    "    s5 = s4.replace(\":\", '')\n",
    "    s6 = s5.lower()\n",
    "    tokens = nltk.word_tokenize(s6)\n",
    "    every = nltk.everygrams(tokens, 2, 4)\n",
    "    count1 = count1 + (collections.Counter(every))\n",
    "    count1 = count1.most_common()\n",
    "\n",
    "    count2 = collections.Counter()\n",
    "    for idx, i in enumerate(A[2]):\n",
    "        x = collections.Counter([l.lower() for l in i])\n",
    "        count2 += x\n",
    "    count2 = count2.most_common()\n",
    "\n",
    "    count3 = count0 + count1 + count2\n",
    "    return count3\n",
    "\n",
    "# Search for works in OpenAlex based on search string\n",
    "def searching(query_string, broad='no', has_ngrams='true', \n",
    "              does_date_matter='no', from_date='2010-01-01', to_date='2022-12-12'):\n",
    "    if broad == 'no':\n",
    "        if does_date_matter == 'yes':\n",
    "            institution = requests.get(\n",
    "                f'https://api.openalex.org/works?filter=abstract.search:{query_string},\\\n",
    "                has_ngrams:{has_ngrams},from_publication_date:{from_date},to_publication_date:{to_date}&page=1&per-page=20'\n",
    "            ).json()\n",
    "        else:\n",
    "            institution = requests.get(\n",
    "                f'https://api.openalex.org/works?filter=title.search:{query_string},\\\n",
    "                has_ngrams:{has_ngrams},&page=1&per-page=20'\n",
    "            ).json()\n",
    "    elif broad == 'yes':\n",
    "        institution = requests.get(\n",
    "            f'https://api.openalex.org/works?search={query_string}&page=1&per-page=20'\n",
    "        ).json()\n",
    "    return institution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
